Liner 
Alger 


Springer Undergraduate Mathematics S eries 
Jorg Liesen 
olker Mehrmann 

Springer Undergraduate Mathematics Series 

Advisory Board 

M.A.J. Chaplain, University of St. Andrews, St. Andrews, Scotland, UK 
K. Erdmann, University of Oxford, Oxford, England, UK 
A. MacIntyre, Queen Mary, University of London, London, England, UK 
E.Suli, University of Oxford, Oxford, England, UK 
M.R. Tehranchi, University of Cambridge, Cambridge, England, UK 
J.F. Toland, University of Cambridge, Cambridge, England, UK 

More information about this series at http://www.springer.com/series/3423 


Jorg Liesen • Volker Mehrmann 
Linear Algebra 
123

Jorg Liesen 
Institute of Mathematics 
Technical University of Berlin 
Berlin 
Germany 
Volker Mehrmann 
Institute of Mathematics 
Technical University of Berlin 
Berlin 
Germany 
ISSN 1615-2085 ISSN 2197-4144 (electronic) 
Springer Undergraduate Mathematics Series 
ISBN 978-3-319-24344-3 ISBN 978-3-319-24346-7 (eBook) 
DOI 10.1007/978-3-319-24346-7 
Library of Congress Control Number: 2015950442 
Mathematics Subject Classification (2010): 15-01 
Springer Cham Heidelberg New York Dordrecht London 
© Springer International Publishing Switzerland 2015 
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part 
of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, 
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission 
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar 
methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this 
publication does not imply, even in the absence of a specific statement, that such names are exempt from 
the relevant protective laws and regulations and therefore free for general use. 
The publisher, the authors and the editors are safe to assume that the advice and information in this 
book are believed to be true and accurate at the date of publication. Neither the publisher nor the 
authors or the editors give a warranty, express or implied, with respect to the material contained herein or 
for any errors or omissions that may have been made. 
Printed on acid-free paper 
Springer International Publishing AG Switzerland is part of Springer Science+Business Media 
(www.springer.com)

Preface 
This is a translation of the (slightly revised) second German edition of our book 
“Lineare Algebra”, published by Springer Spektrum in 2015. Our general view 
of the field of Linear Algebra and the approach to it that we have chosen in this 
book were already described in our Preface to the First German Edition, published 
by Vieweg+Teubner in 2012. In a nutshell, our exposition is matrix-oriented, and 
we aim at presenting a rather complete theory (including all details and proofs), 
while keeping an eye on the applicability of the results. Many of them, though 
appearing very theoretical at first sight, are of an immediate practical relevance. In 
our experience, the matrix-oriented approach to Linear Algebra leads to a better 
intuition and a deeper understanding of the abstract concepts, and therefore simplifies 
their use in real-world applications. 
Starting from basic mathematical concepts and algebraic structures we develop 
the classical theory of matrices, vectors spaces, and linear maps, culminating in the 
proof of the Jordan canonical form. In addition to the characterization of important 
special classes of matrices or endomorphisms, the last chapters of the book are 
devoted to special topics: Matrix functions and systems of differential equations, the 
singular value decomposition, the Kronecker product, and linear matrix equations. 
These chapters can be used as starting points of more advanced courses or seminars 
in Applied Linear Algebra. 
Many people helped us with the first two German editions and this English edition 
of the book. In addition to those mentioned in the Preface to the First German 
Edition, we would like to particularly thank Olivier Sete, who carefully worked 
through the entire draft of the second edition and gave numerous comments, as well 
as Leonhard Batzke, Carl De Boor, Sadegh Jokar, Robert Luce, Christian Mehl, 
Helia Niroomand Rad, Jan Peter Schafermeier, Daniel Wachsmuth, and Gisbert 
v

vi Preface 

Wustholz. Thanks also to the staff of Springer Spektrum, Heidelberg, and 
Springer-Verlag, London, for their support and assistance with editorial aspects of 
this English edition. 

Berlin Jorg Liesen 
July 2015 Volker Mehrmann 


Preface to the First German Edition 
Mathematics is the instrument that links theory and practice, thinking and observing; 
it establishes the connecting bridge and builds it stronger and stronger. This is why our 
entire culture these days, as long as it is concerned with understanding and harnessing 
nature, has Mathematics as its foundation.1 
This assessment of the famous mathematician David Hilbert (1862–1943) is even 
more true today. Mathematics is found not only throughout the classical natural 
sciences, Biology, Chemistry and Physics, its methods have become indispensable 
in Engineering, Economics, Medicine, and many other areas of life. This continuing 
mathematization of the world is possible because of the transversal strength of 
Mathematics. The abstract objects and operations developed in Mathematics can be 
used for the description and solution of problems in numerous different situations. 
While the high level of abstraction of modern Mathematics continuously 
increases its potential for applications, it represents a challenge for students. This is 
particularly true in the first years, when they have to become familiar with a lot of 
new and complicated terminology. In order to get students excited about mathematics 
and capture their imagination, it is important for us teachers of basic courses 
such as Linear Algebra to present Mathematics as a living science in its global 
context. The short historical notes in the text and the list of some historical papers at 
the end of this book show that Linear Algebra is the result of a human endeavor. 
An important guideline of the book is to demonstrate the immediate practical 
relevance of the developed theory. Right in the beginning we illustrate several 
concepts of Linear Algebra in everyday life situations. We discuss mathematical 
basics of the search engine Google and of the premium rate calculations of car 
1“Das Instrument, welches die Vermittlung bewirkt zwischen Theorie und Praxis, zwischen 
Denken und Beobachten, ist die Mathematik; sie baut die verbindende Brucke und gestaltet sie 
immer tragfahiger. Daher kommt es, dass unsere ganze gegenwartige Kultur, soweit sie auf der 
geistigen Durchdringung und Dienstbarmachung der Natur beruht, ihre Grundlage in der 
Mathematik findet.” 
vii

insurances. These and other applications will be investigated in later chapters using 
theoretical results. Here the goal is not to study the concrete examples or their 
solutions, but the presentation of the transversal strength of mathematical methods 
in the Linear Algebra context. 
The central object for our approach to Linear Algebra is the matrix, which we 
introduce early on, immediately after discussing some of the basic mathematical 
foundations. Several chapters deal with some of their most important properties, 
before we finally make the big step to abstract vector spaces and homomorphisms. 
In our experience the matrix-oriented approach to Linear Algebra leads to a better 
intuition and a deeper understanding of the abstract concepts. 
The same goal should be reached by the MATLAB-Minutes2 that are scattered 
throughout the text and that allow readers to comprehend the concepts and results 
via computer experiments. The required basics for these short exercises are introduced 
in the Appendix. Besides the MATLAB-Minutes there are a large number of 
classical exercises, which just require a pencil and paper. 
Another advantage of the matrix-oriented approach to Linear Algebra is given 
by the simplifications when transferring theoretical results into practical algorithms. 
Matrices show up wherever data are systematically ordered and processed, which 
happens in almost all future job areas of bachelor students in the mathematical 
sciences. This has also motivated the topics in the last chapters of this book: matrix 
functions, the singular value decomposition, and the Kronecker product. 
Despite many comments on algorithmic and numerical aspects, the focus in this 
book is on the theory of Linear Algebra. The German physicist Gustav Robert 
Kirchhoff (1824–1887) is attributed to have said: 
A good theory is the most practical thing there is.3 
This is exactly how we view our approach to the field. 
This book is based on our lectures at TU Chemnitz and TU Berlin. We would 
like to thank all students, co-workers, and colleagues who helped in preparing and 
proofreading the manuscript, in the formulation of exercises, and with the content 
of lectures. Our special thanks go to Andre Gaul, Florian Go.ler, Daniel Kre.ner, 
Robert Luce, Christian Mehl, Matthias Pester, Robert Polzin, Timo Reis, Olivier 
Sete, Tatjana Stykel, Elif Topcu, Wolfgang Wulling, and Andreas Zeiser. 
We also thank the staff of the Vieweg+Teubner Verlag and, in particular, Ulrike 
Schmickler-Hirzebruch, who strongly supported this endeavor. 
Berlin Jorg Liesen 
July 2011 Volker Mehrmann 
2MATLAB® trademark of The MathWorks Inc. 
3“Eine gute Theorie ist das Praktischste, was es gibt.” 
viii Preface to the First German Edition

Contents 


1 Linear Algebra in Every Day Life .... ..... ..... ..... ..... 1 

1.1 The PageRankAlgorithm........................... 1 


1.2 No Claim Discountingin Car Insurances ................ 3 


1.3 Production Planningina Plant ....................... 4 


1.4 Predicting Future Profits ........................... 5 


1.5 Circuit Simulation................................ 6 
2 Basic Mathematical Concepts ....... ..... ..... ..... ..... 9 


2.1 Setsand MathematicalLogic........................ 9 


2.2 Maps......................................... 14 


2.3 Relations...................................... 17 
3 Algebraic Structures .. ..... ....... ..... ..... ..... ..... 23 


3.1 Groups ....................................... 23 


3.2 Rings and Fields................................. 26 
4 Matrices . ..... ..... ..... ....... ..... ..... ..... ..... 37 


4.1 Basic Definitionsand Operations ..................... 37 


4.2 Matrix GroupsandRings........................... 44 
5 The Echelon Form and the Rank of Matrices. ..... ..... ..... 55 

5.1 Elementary Matrices .............................. 55 


5.2 The Echelon Form and Gaussian Elimination . . . . . . . . . . . . . 57 

5.3 Rankand Equivalenceof Matrices .................... 66 
6 Linear Systems of Equations . ....... ..... ..... ..... ..... 73 
7 Determinants of Matrices ... ....... ..... ..... ..... ..... 81 


7.1 Definitionofthe Determinant........................ 81 


7.2 Propertiesofthe Determinant ........................ 85 


7.3 Minorsandthe Laplace Expansion.................... 91 



8 The Characteristic Polynomial and Eigenvalues of Matrices . . . . . 101 
8.1 The Characteristic Polynomial and the Cayley-Hamilton 
Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 
8.2 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . 106 
8.3 Eigenvectors of Stochastic Matrices. . . . . . . . . . . . . . . . . . . . 109 
9 Vector Spaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 
9.1 Basic Definitions and Properties of Vector Spaces. . . . . . . . . . 115 
9.2 Bases and Dimension of Vector Spaces . . . . . . . . . . . . . . . . . 118 
9.3 Coordinates and Changes of the Basis . . . . . . . . . . . . . . . . . . 124 
9.4 Relations Between Vector Spaces and Their Dimensions . . . . . 128 
10 Linear Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 
10.1 Basic Definitions and Properties of Linear Maps. . . . . . . . . . . 135 
10.2 Linear Maps and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 143 
11 Linear Forms and Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . 155 
11.1 Linear Forms and Dual Spaces . . . . . . . . . . . . . . . . . . . . . . . 155 
11.2 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 
11.3 Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 
12 Euclidean and Unitary Vector Spaces . . . . . . . . . . . . . . . . . . . . . 167 
12.1 Scalar Products and Norms . . . . . . . . . . . . . . . . . . . . . . . . . 167 
12.2 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 
12.3 The Vector Product in R3;1 . . . . . . . . . . . . . . . . . . . . . . . . . 182 
13 Adjoints of Linear Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 
13.1 Basic Definitions and Properties . . . . . . . . . . . . . . . . . . . . . . 187 
13.2 Adjoint Endomorphisms and Matrices . . . . . . . . . . . . . . . . . . 195 
14 Eigenvalues of Endomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . 199 
14.1 Basic Definitions and Properties . . . . . . . . . . . . . . . . . . . . . . 199 
14.2 Diagonalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 
14.3 Triangulation and Schur’s Theorem. . . . . . . . . . . . . . . . . . . . 207 
15 Polynomials and the Fundamental Theorem of Algebra . . . . . . . . 213 
15.1 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 
15.2 The Fundamental Theorem of Algebra. . . . . . . . . . . . . . . . . . 218 
16 Cyclic Subspaces, Duality and the Jordan Canonical Form. . . . . . 227 
16.1 Cyclic f -invariant Subspaces and Duality . . . . . . . . . . . . . . . . 227 
16.2 The Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . 233 
16.3 Computation of the Jordan Canonical Form . . . . . . . . . . . . . . 243 
17 Matrix Functions and Systems of Differential Equations. . . . . . . . 253 
17.1 Matrix Functions and the Matrix Exponential Function . . . . . . 253 
17.2 Systems of Linear Ordinary Differential Equations . . . . . . . . . 261 
x Contents

Contents 

18 Special Classes of Endomorphisms ....................... 271 


18.1 NormalEndomorphisms............................ 271 


18.2 OrthogonalandUnitaryEndomorphisms................ 276 


18.3 SelfadjointEndomorphisms......................... 281 
19 The Singular Value Decomposition ....................... 295 
20 The Kronecker Product and Linear Matrix Equations ......... 303 
AppendixA:A Short Introduction to MATLAB ................ 311 
Selected Historical Works on Linear Algebra ................... 315 
Bibliography ........................................... 317 
Index ................................................ 319 



Chapter1 
LinearAlgebrainEveryDay Life 

Onehastofamiliarizethestudentwith actual questionsfrom applications,sothathelearns 

to deal with realworldproblems.1 

Lothar Collatz(1910–1990) 

1.1 ThePageRankAlgorithm 
The PageRank algorithmis a method to assess the “importance” of documents with 
mutual links,such as web pages,onthe basisof the linkstructure.Itwasdeveloped 
by Sergei Brin and LarryPage, thefoundersofGoogleInc.,atStanfordUniversity 
in thelate 1990s.The basic ideaof thealgorithmisthe following: 

Insteadof counting links,PageRank essentially interpretsalinkofpageAtopage 
Basavoteof pageAfor pageB.PageRank then assessesthe importanceofa page 
by the number of received votes. PageRank also considers the importance of the 
pagethat caststhevote, sincevotesofsome pageshaveahighervalue,andthusalso 
assignahighervaluetothe page they point to.Important pages willbe ratedhigher 
and thus leadtoahigher positioninthe search results.2 

Letusdescribe(model)thisidea mathematically.Our presentationusesideas from 
thearticle [BryL06].Foragiven setof web pages,every page k will be assigned 
an importance value xk . 
0.Apagek is more important than a page jif xk > 
xj. 
If a page k has a link to a page j, we say that page j has a backlink from page k. 
Intheabove descriptionthese backlinks arethevotes.Asanexample, considerthe 
following link structure: 

1“Man muss denLernendenmitkonkreten Fragestellungenaus denAnwendungenvertrautmachen, 
dass er lernt,konkreteFragenzubehandeln.” 
2Translationofatext foundin 2010on http://www.google.de/corporate/tech.html. 


©SpringerInternationalPublishing Switzerland 2015 1 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_1 

1 Linear AlgebrainEvery DayLife 


Herethepage1haslinkstothepages2,3and4,anda backlinkfrompage3. 

The easiest approach to define importance of web pages is to count its backlinks; 
themorevotes are castfora page, themoreimportant the page is.In ourexample 
thisgivesthe importancevalues 

x1 = 
1, 
x2 = 
3, 
x3 = 
2, 
x4 = 
3. 


The pages2and4are thus themostimportant pages, and theyare equally important. 

However, theintuition and also the above description fromGooglesuggests that 
backlinks fromimportant pages aremoreimportant forthevalueofapage than those 
fromlessimportant pages.Thisidea canbe modeledby defining xk as thesumof all 
importance values of the backlinks of the page k.In ourexamplethisresultsin four 
equations that have to be satisfied simultaneously, 

x1 = 
x3, 
x2 = 
x1 + 
x3 + 
x4, 
x3 = 
x1 + 
x4, 
x4 = 
x1 + 
x2 + 
x3. 


Adisadvantage of this approach is that it does not consider the number of links 
of the pages.Thus,itwouldbe possibleto (significantly)increasethe importanceof 
apagejustbyaddinglinkstothatpage.Inordertoavoidthis,the importancevalues 
ofthe backlinksinthePageRank algorithmaredividedbythe numberof linksofthe 
corresponding page. This creates akindof “internet democracy”:Every page can 
voteforother pages,whereintotalit can cast onevote.Inourexamplethisgivesthe 
equations 

x3 x1 x3 x4 x1 x4 x1 x3 

x1 = 
, 
x2 =++ 
, 
x3 =+ 
, 
x4 =+ 
x2 + 
. 
(1.1)

3 332 32 33 

These are four equations for the four unknowns, and all equations are linear,3 i.e., 
the unknowns occur only in first power. In Chap. 6 we will see how to write the 
equationsin(1.1)informofa linear system of equations.Analyzing and solving 
such systemsis oneofthemostimportant tasksofLinear Algebra.Theexampleof 
thePageRank algorithmshows that Linear Algebrapresents apowerful modeling 

3The term “linear” originates from the Latin word “linea”, which means “(straight) line”, and 
“linearis”means “consistingof(straight)lines”. 


1.1 ThePageRank Algorithm 3 
tool: We have turned the real world problem of assessing the importance of web 
pages intoaproblemof Linear Algebra. This problem willbeexamined furtherin 
Sect. 8.3. 

For completeness, we mentionthatasolutionfor thefour unknowns(computed 
withMATLABand roundedtothesecond significant digit)isgivenby 

x1 = 
0.14, 
x2 = 
0.54, 
x3 = 
0.41, 
x4 = 
0.72. 


Thus,page4isthemost importantone.Itispossibletomultiplythesolution,i.e.,the 
importancevalues xk,byapositiveconstant. Suchamultiplicationorscalingisoften 
advantageousfor computational methodsorforthe visual displayofthe results.For 
example, thescaling couldbeusedtogivethe most important page thevalue1.00. 
Ascalingisallowed, sinceit does not change therankingof the pages,whichis the 
essentialinformationprovidedby thePageRank algorithm. 

1.2 No ClaimDiscountingin Car Insurances 
Insurance companies computethe premiums fortheir customers on the basis of the 
insuredrisk: thehigher therisk, thehigher thepremium.Itisthereforeimportantto 
identifythefactorsthat leadto higher risk.Inthe caseofacar insurance thesefactors 
includethe numberof milesdrivenper year,the distance between homeandwork, 
themaritalstatus,theenginepower,ortheageofthedriver.Usingsuch information, 
the companycalculates theinitialpremium. 

Usually the bestindicator forfuture accidents, and hence future insurance claims, 
is the number of accidents of the individual customer in the past, i.e., the claims 
history. In order to incorporate this information into the premium rates, insurers 
establishasystemof risk classes,which divide the customers into homogeneous risk 
groupswith respectto theirprevious claims history. Customerswithfewer accidents 
in the past get a discount on their premium. This approach is called a no claims 
discounting scheme. 

For a mathematical model of this scheme we need a set of risk classes and a 
transition rule for moving between the classes. At the end of a policy year, the 
customermay move toadifferent class depending on theclaimsmade duringthe 
year.The discountisgiveninpercentofthepremiumintheinitialclass.Asasimple 
example we consider four risk classes, 

C1 C2 C3 C4 

%discount 

010 20 40 

and thefollowing transitionrules: 

• 
No accident:Stepup one class(or stayin C4). 

1 Linear AlgebrainEvery DayLife 

• 
One accident:Step back one class(or stayin C1). 
• 
More than one accident:Step back to class C1 (orstayin C1). 
Next,the insurance companyhasto estimatethe probabilitythata customerwho 
is in the class Ci in this year will move to the class Cj.Thisprobability is denoted 
by pij.Let us assume,for simplicity,that theprobabilityofexactly one accident for 
every customeris0.1, i.e.,10%,andtheprobabilityoftwoormore accidentsfor 
every customeris0.05, i.e.,5%.(Of course,inpracticethe insurance companies 
determine theseprobabilitiesin dependenceof theclasses.) 

Forexample,a customerinthe class C1 will stay in C1 in caseof atleast one 
accident.This happens with theprobability 0.15, so that p11 = 
0.15.Acustomerin 
C1 has no accident with theprobability0.85, so that p12 = 
0.85. Thereisno chance 
to movefromC1 to C3 or C4in thenextyear,sothat p13 = 
p14 = 
0.00.Inthis way 
we obtain16 values pij, i, 
j= 
1, 
2, 
3, 
4, whichwe can arrangeina4 . 
4matrix as 
follows: 

... 
. 


p11 p12 p13 p14 0.150.850.000.00 

... 
.

p21 p22 p23 p24 0.150.000.850.00 

... 
. 


= 
. 
(1.2) 

... 
.

p31 p32 p33 p34 0.050.100.000.85 

p41 p42 p43 p44 0.050.000.100.85 

All entries of this matrix are nonnegative real numbers, and the sum of all entries in 
each rowis equal to1.00, i.e., 

pi1 + 
pi2 + 
pi3 + 
pi4 = 
1.00 for each i = 
1, 
2, 
3, 
4. 


Suchamatrixis called row-stochastic. 

The analysis of matrix properties is a central topic of Linear Algebra that is 
developed throughout this book.Asin theexamplewith thePageRank algorithm, 
wehave translateda practical problem into thelanguageof Linear Algebra, and we 
cannowstudyit usingLinear Algebratechniques.Thisexampleofpremium rates 
willbe discussedfurtherin Example 4.7. 

1.3 Production PlanninginaPlant 
Theproductionplanninginaplanthasto consider manydifferentfactors,inparticular 
commodity prices,labor costs,andavailable capital,in orderto determinea 
productionplan.We considerasimpleexample: 

A companyproduces theproducts P1 and P2.If xi units of the product Pi are 
produced, where i = 
1, 
2, then the pair(x1, 
x2) 
is calleda productionplan. Suppose 
that the raw materials and labor for the production of one unit of the product Pi 
cost a1i and a2i Euros, respectively. If b1 Eurosareavailablefor the purchaseofraw 
materials and b2 Eurosforthe paymentof labor costs,thenaproductionplanmust 


1.3 Production PlanninginaPlant 5 
satisfy the constraint inequalities 

a11x1 + 
a12x2 . 
b1, 
a21x1 + 
a22x2 . 
b2. 


Ifaproductionplansatisfies these constraints,itis called feasible.Let pi be theprofit 
fromselling one unitofproduct Pi.Thenthegoalisto determineaproductionplan 
that maximizes the profit function 

(x1,x2) 
= 
p1x1 + 
p2x2. 
How can we find this maximum? 
Thetwo equations 

a11x1 + 
a12x2 = 
b1 and a21x1 + 
a22x2 = 
b2 

describestraight linesinthe coordinatesystemthathasthevariables x1 and x2 on its 
axes.Thesetwolinesform boundarylinesofthefeasible productionplans,whichare 
“below”the lines;seethe figurebelow.Notethatwealso musthave xi . 
0, since we 
cannot producenegative unitsofaproduct.For planned profits yi, i = 
1,2,3,..., 
the equations p1x1 + 
p2x2 = 
yi describe parallel straight lines in the coordinate 
system;seethe dashed linesinthe figure.If x1 and x2 satisfy p1x1+ 
p2x2 = 
yi,then 

(x1,x2) 
= 
yi.The profit maximizationproblem can nowbesolvedby movingthe 
dashed lines until one of them reaches the corner with the maximal y: 
In case of more variables we cannot draw such a simple figure and obtain the 
solution “graphically”. But the general idea of finding a corner with the maximum 
profitis stillthesame. Thisisanexampleofa linear optimizationproblem.Asbefore, 
wehave formulatedarealworldprobleminthelanguageof Linear Algebra,andwe 
can use mathematical methods for its solution. 

1.4 Predicting FutureProfits 
Thepredictionofprofitsorlossesof a companyisa centralplanning instrumentof 
economics. Analogous problems arise in many areas of political decision making, 


1 Linear AlgebrainEvery DayLife 

forexampleinbudget planning,taxestimatesortheplanningofnewinfrastructures. 
We consider a specific example: 

In thefour quartersofa yeara companyhas profitsof10, 
8, 
9, 
11 millionEuros. 
The boardnowwantstopredict thefutureprofitsdevelopment on the basisof these 
values. Evidence suggests, that the profits behave linearly. If this was true, then 
the profits would form a straight line y(t) 
= 
.t + 
. 
that connects the points 
(1, 
10), 
(2, 
8), 
(3, 
9), 
(4, 
11) 
in the coordinate system having “time” and “profit” 
asitsaxes.This,however,does neitherholdinthisexamplenorin practice. Thereforeonetriestofindastraightlinethatdeviates“
aslittleas possible”fromthegiven 
points.One possible approachisto choosethe parameters . 
and . 
in order to minimize the sum of the squared 
distances between the given points and the straight line. 
Once the parameters . 
and . 
have been determined, the resulting line y(t) 
can be 
used for estimating or predicting the future profits, as illustrated in the following 
figure: 


The determination ofthe parameters . 
and . 
that minimize a sum of squares is 
called a least squares problem.We will solve least squares problems using methods of 
Linear Algebra in Example 12.16.The approach itself is sometimes called a 
parameter identification.In Statistics, the modeling of given data(here the company 
profits) using a linear predictor function(here y(t) 
= 
.t + 
.)is known as linear 
regression. 

1.5 Circuit Simulation 
The current development of electronic devices is very rapid.In short intervals,nowadays often less than a year,
new models of laptops or mobile phones have to be issued 
to the market.To achieve this, continuously new generations of computer chips have 
tobedeveloped. Thesetypically becomesmallerand morepowerful,and naturally 
shoulduse as little energy as possible.An importantfactorin thisdevelopmentis 
to plan and simulate the chips virtually, i.e., in the computer and without producing 
aphysical prototype. This model-based planning and optimizationofproductsisa 
centralmethodin manyhigh technology areas,anditis basedon modernmathematics. 



1.5 Circuit Simulation 7 
Usually,the switching behaviorofa chipismodeledbyamathematical system 
consistingofdifferential and algebraic equations that describethe relation between 
currents andvoltages.Without going into details, consider thefollowing circuit: 


In this circuit description, VS(t) 
is the given input current at time t, and the 
characteristic values of the components are R for the resistor, Lforthe inductor, and 
C for the capacitor.The functions for the potential differences at the three components 
are denoted by VR(t), VL(t), and VC(t);I(t) 
is the current. 

Applying the Kirchhoff laws4 of electrical engineering leads to the following 
system of linear equations and differential equations that model the dynamic behavior 
of the circuit: 

d 

LI =VL,

dt 
d 

C VC =I,

dt 
RI =VR, 


VL +VC +VR =VS. 


In thisexampleitis easytosolve thelasttwo equations for VL and VR, and hence 
to obtain a system of differential equations 

dR 11 

I =. 
I . 
VC + 
VS,

dt LLL 
d 1 

VC =. 
I,

dt C 

forthe functions I und VC.Wewill discussandsolvethissysteminExample17.13. 

This simple example demonstrates that for the simulation of a circuit a system 
of linear differential equations and algebraic equations has to be solved. Modern 
computer chips in industrial practice require solving such systems with millions 
of differential-algebraic equations. Linear Algebra is one of central tools for the 
theoretical analysisof such systemsaswellasthedevelopmentofefficient solution 
methods. 

4Gustav Robert Kirchhoff(1824–1887). 


Chapter2 
BasicMathematicalConcepts 

In this chapter we introduce the mathematical concepts that form the basis for the 
developmentsinthe following chapters.Webeginwith sets and basic mathematical 
logic. Then we consider maps between sets and their most important properties. 
Finally we discuss relations and in particular equivalence relations on a set. 

2.1 Sets andMathematicalLogic 
Webeginourdevelopmentwiththe conceptofasetandusethefollowing definition 
of Cantor.1 

Definition 2.1 A set is a collection M of well determined and distinguishable objects 
x of our perception or our thinking. The objects are called the elements of M. 

The objects x in this definition are well determined, and therefore we can uniquely 
decide whether x belongs to a set M or not.We write x .M if x is an element of the 
set M, otherwise we write x ./M. Furthermore, the elements are distinguishable, 
which means that all elements of M are(pairwise) distinct. 

If two objects x and y are equal, then we write x = 
y, otherwise x 
= 
y.For 
mathematical objects we usually have to give a formal definition of equality.As an 
example consider the equality of sets; see Definition 2.2 below. 

We describe sets with curly brackets{}that contain either a list of the elements, 
for example 

{red,yellow,green}, {1,2,3,4}, {2,4,6,...}, 

1GeorgCantor(1845–1918), one of thefounders of settheory. Cantor published this definition in 
the journal “MathematischeAnnalen”in 1895. 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_2 

2 Basic Mathematical Concepts 

ora defining property,forexample 

{x |x is a positive even number}, 
{x |x is a person owning a bike}. 

Someofthe well known setsof numbersare denoted as follows: 

N 
={1,2,3,...} 
(the natural numbers), 
N0 ={0,1,2,...} 
(the natural numbersincluding zero), 
Z 
={...,.2,.1,0,1,2,...} 
(the integers), 
Q 
={x |x =a/b with a .Z 
and b.N} 
(the rational numbers), 
R 
={x |x is a real number} 
(the real numbers). 


The construction and characterization of the real numbers R 
is usually done in an 
introductory course in Real Analysis. 

To describe a set via its defining property we formally write {x | 
P(x)}.Here 
P is a predicate whichmay holdfor an object x or not, and P(x)is the assertion 
“P holds for x”. 

In general,anassertion is a statement that can be classified as either “true” or 
“false”.For instance the statement “The set N 
has infinitely many elements” is true. 
The sentence “Tomorrow the weather will be good” is not an assertion, since the 
meaning of the term“good weather”is unclear and the weather prediction in general 
is uncertain. 

The negation of an assertion Ais theassertion “not A”, whichwe denoteby ¬A. 
This assertionistrueifand onlyif Aisfalse, andfalseifand onlyif Ais true.For 
instance, thenegationofthe true assertion “Theset N 
has infinitely manyelements” 
isgivenby“ThesetN 
does not have infinitely manyelements” (or “The set N 
has 
finitely manyelements”), whichisfalse. 

Two assertions A and B can be combined via logical compositions to a new 
assertion. Thefollowingisa listofthe most commonlogical compositions,together 
with their mathematical short hand notation: 

Composition Notation Wording 
conjunction 
disjunction 
implication 
equivalence 
. 
. 
. 
. 
Aand B 
Aor B 
Aimplies B 
If Athen B 
Ais a sufficient condition for B 
Bis a necessary condition for A 
Aand Bare equivalent 
Ais true if and only if B is true 
Ais necessary and sufficient for B 
Bis necessary and sufficient for A 


2.1 
Sets and Mathematical Logic 11 
For example, we can write the assertion “x is a real number and x is negative” as 
x .R 
.x < 0. Whetheran assertionthatis composedoftwoassertions Aand Bis 
true orfalse, depends on thelogicalvaluesof Aand B.Wehavethe following table 
of logical values (“t” and“f” denotetrue andfalse, respectively): 

For example, the assertion A .B is true only when A and B are both true. The 
assertion A .B isfalse onlywhen A is true and B isfalse. In particular,if A is 
false, then A.B is true, independentof thelogicalvalueof B. 

Thus, 3 < 5 . 
2 < 4 is true, since 3 < 5 and 2 < 4 are both true. But 
3 < 5 .2 > 4isfalse, since2 > 4isfalse.Onthe other hand, theassertions 
4< 2.3> 5and4< 2.3< 5are bothtrue, since4 < 2isfalse. 

In thefollowing we oftenhavetoprove that certain implications A.Baretrue. 
As thetableoflogicalvalues showsand theexample illustrates, we then onlyhaveto 
prove that under theassumptionthat Ais true theassertion Bis true as well. Instead 
of “Assume that Ais true” we will often write “Let Ahold”. 

Itis easytosee that 

(A . 
B) . 
(¬B .¬A). 

(Asanexercisecreatethe tableoflogicalvalues for ¬B .¬Aand compareitwith 
thetablefor A.B.) Thetruthof A.B can thereforebeprovedby showing that 
thetruth of ¬B impliesthe truthof ¬A, i.e., that “B isfalse” implies “Aisfalse”. 
The assertion ¬B .¬A is called the contraposition of the assertion A .B and 
the conclusion from A.B to ¬B .¬Ais calledproofby contraposition. 

Together with assertions we also often use so-calledquantifiers: 

Quantifier Notation Wording 
universal . 
For all 
existential . 
There exists 

Now we return to set theory and introduce subsets and the equality of sets. 

Definition 2.2 Let M, N be sets. 

(1) 
M is called a subset of N, denoted by M .N,if every element of M is also an 
element of N.We write M . 
N,if this does not hold. 
(2) 
M and N are called equal, denoted by M =N,if M .N and N .M.We 
write M =N is this does not hold. 

2 Basic Mathematical Concepts 

(3) 
M is called a proper subset of N, denoted by M . 
N,if both M . 
N and 
M 
= 
N hold. 
Using the notation of mathematical logic we can write this definition as follows: 

(1) 
M . 
N . 
(. 
x : 
x . 
M . 
x . 
N). 
(2) 
M = 
N . 
(M . 
N . 
N . 
M). 
(3) 
M . 
N . 
(M . 
N . 
M 
= 
N). 
The assertion on the right side of the equivalence in (1) reads as follows: For all 
objects x the truth of x . 
M implies the truth of x . 
N.Or shorter:For all x,if 
x . 
M holds,then x . 
N holds. 

Averyspecialsetisthesetwithno elements,whichwedefineformallyasfollows. 

Definition 2.3 The setO:= 
{x | 
x 
= 
x} 
is calledthe empty set. 

The notation “:=” means is defined as.We have introduced the empty set by a 
defining property:Every object x with x 
= 
x is any element of O. This cannot hold 
for any object,and hence Odoes not contain any element.A set that contains at least 
one element is called nonempty. 

Theorem 2.4 For every setM the following assertions hold: 

(1) O. 
M. 
(2) 
M . 
O. 
M = 
O. 
Proof 

(1)Wehavetoshowthat theassertion “. 
x : 
x . 
O. 
x . 
M”istrue. Since there 
is no x . 
O, the assertion “x . 
O”isfalse, and therefore “x . 
O. 
x . 
M”is 
true for every x (cp. theremarks on theimplication A. 
B). 
(2) Let M . 
O. From(1)weknowthatO . 
M and hence M = 
Ofollows by (2) 
in Definition 2.2. . 

Theorem 2.5 LetM, N, Lbesets. Then thefollowing assertions holdfor thesubset 
relation “.”: 

(1) 
M . 
M(reflexivity). 
(2) 
IfM . 
N andN . 
L, thenM . 
L(transitivity). 
Proof 

(1) Wehavetoshowthattheassertion“. 
x : 
x . 
M . 
x . 
M”istrue.If “x . 
M” 
is true, then “x . 
M . 
x . 
M”isanimplicationwithtwotrueassertions, and 
hence it is true. 
(2)Wehavetoshowthattheassertion “. 
x : 
x . 
M . 
x . 
L”istrue.If “x . 
M” 
is true, then also “x . 
N”istrue, since M . 
N.The truth of“x . 
N”implies 
that “x . 
L”is true, since N . 
L.Hence the assertion “x . 
M . 
x . 
L”is 
true. . 


2.1 
Sets and Mathematical Logic 13 
Definition 2.6 Let M,N be sets. 

(1) The union2 of M and N is M . 
N := 
{x | 
x . 
M . 
x . 
N}. 
(2) The intersection of M and N is M . 
N := 
{x | 
x . 
M . 
x . 
N}. 
(3) The difference of M and N is M^ 
N := 
{x | 
x . 
M . 
x ./N}. 
If M. 
N = 
O, then thesets Mand Nare called disjoint.The set operations union 
and intersection can be extended to more than two sets: If I 
= 
Ois a set and if for 
all i . 
I thereisa set Mi,then 

Mi := 
{x |. 
i . 
I with x . 
Mi} 
and Mi := 
{x |. 
i . 
I we have x . 
Mi}. 
i.Ii.I 

The set I is called an index set.For I ={1,2,...,n}. 
N 
we write the union and 
intersectionofthe sets M1,M2,..., Mn as 

nn 

Mi and Mi. 
i=1 i=1 

Theorem 2.7 LetM . 
Nfor twosetsM,N. Then thefollowing are equivalent: 

(1) 
M . 
N. 
(2) 
N ^ 
M 
= 
O. 
Proof We show that(1). 
(2)and (2). 
(1)hold. 

(1) 
. 
(2): 
Since M 
= 
N,there exists an x . 
N with x ./M.Thus x . 
N ^ 
M,so 
that N ^ 
M 
= 
Oholds. 
(2) 
. 
(1): 
There exists an x . 
N with x ./M, and hence N 
= 
M.Since M . 
N 
holds, we see that M . 
N holds. . 

Theorem 2.8 LetM,N,Lbesets. Then thefollowing assertions hold: 

(1) 
M. 
N . 
M andM . 
M . 
N. 
(2) 
Commutativity:M . 
N = 
N . 
M andM. 
N = 
N . 
M. 
(3) 
Associativity:M. 
(N. 
L)= 
(M. 
N). 
LandM. 
(N. 
L)= 
(M. 
N). 
L. 
(4) 
Distributivity: M . 
(N . 
L)= 
(M . 
N). 
(M . 
L)and M . 
(N . 
L)= 
(M. 
N). 
(M. 
L). 
(5) 
M^ 
N . 
M. 
(6) 
M^ 
(N . 
L)= 
(M^ 
N). 
(M^ 
L)andM^ 
(N . 
L)= 
(M^ 
N). 
(M^ 
L). 
Proof Exercise. 
. 


2The notations M . 
N and M . 
N for union andintersection ofsets M and N were introduced 
in 1888by Giuseppe Peano(1858–1932), oneof thefoundersof formal logic. The notationofthe 
“smallest commonmultiple M(M,N)”and “largest commondivisor D(M,N)”ofthesets M and 
N suggestedby GeorgCantor(1845–1918) did not catch on. 


2 Basic Mathematical Concepts 

Definition 2.9 Let M be a set. 

(1) The cardinality of M, denoted by |M|,isthe numberof elementsof M. 
(1) The power set of M, denoted by P(M), is the set of all subsets of M, i.e., 
P(M):={N |N .M}. 
Theempty setO has cardinality zero and P(O) ={O}, thus |P(O)|= 
1.For 
M ={1, 3}the cardinality is |M|=2and 

P(M) ={O, {1}, {3}, M}, 

and hence |P(M)|=4=2|M|.One can showthat forevery set Mwith finitely many 
elements, i.e., finite cardinality, |P(M)|=2|M| 
holds. 

2.2 Maps 
In this sectionwediscussmaps between sets. 

Definition 2.10 Let X, Y be nonempty sets. 

(1) A map f from X to Y is a rule that assigns to each x . 
X exactly one y = 
f(x). 
Y.We write this as 
f : 
X >Y, x >y = 
f(x). 
Instead of x >y = 
f(x)we also write f(x)=y.The sets X and Y are called 
domain and codomain of f. 

(2) Two maps 
f : 
X >Y and g : 
X >Y are called equal when f(x)=g(x) 
holds for all x .X.Wethen write f =g. 
In Definition 2.10 we have assumed that X and Y are nonempty, since otherwise 
there can be no rule that assigns an element of Y to each element of X.If one of 
these sets is empty, one can define an empty map.However,in the following we will 
always assume (but not always explicitly state) that the sets between which a given 
map acts are nonempty. 

Example2.11 Two maps from X =R 
to Y =R 
aregiven by 

f : 
X >Y, f(x)=x2 , (2.1) 
0, x .0, 

g : 
X >Y, x 
> 
(2.2)
1, x > 0. 

To analyze thepropertiesofmaps we need some further terminology. 


2.2 
Maps 15 
Definition 2.12 Let X, Y be nonempty sets. 

(1) The mapIdX : 
X > 
X, x 
> 
x,is called the identity onX . 
(2) Let f : 
X > 
Y be a map and let M . 
X and N . 
Y.Then 
f(M):= 
{ 
f(x)| 
x . 
M}. 
Y is calledthe imageof M under f, 
f.1(N):= 
{ 
x . 
X | 
f(x). 
N} 
is calledthe pre-image of N under f. 

(3) If 
f : 
X > 
Y, x 
> 
f(x)isamap andO 
= 
M . 
X, then f|M : 
M > 
Y, 
x 
> 
f(x),is calledthe restrictionoff toM. 
Oneshould notethatin this definition f.1(N)isaset, and hence thesymbol f.1 
here does not mean theinverse mapof f.(This map will be introduced below in 
Definition 2.21.) 

Example2.13 For the maps with domain X = 
R 
in(2.1)and(2.2)wehavethe 
following properties: 

f(X)={x . 
R 
| 
x . 
0}, f.1(R.)={0}, f.1({.1})= 
O, 
.1.1

g(X)={0,1}, g (R.)= 
g ({0})= 
R., 

where R. 
:={x . 
R 
| 
x . 
0}. 

Definition 2.14 Let X, Y be nonemptysets.A map f : 
X > 
Y is called 

(1) 
injective,iffor all x1, x2 . 
X the equality f(x1)= 
f(x2)impliesthat x1 = 
x2, 
(2) 
surjective,if f(X)= 
Y, 
(3) 
bijective,if f is injective and surjective. 
For every nonempty set X the simplest example of a bijective map from X to X 
is IdX,the identity on X. 

Example2.15 Let R+ 
:={x . 
R 
| 
x . 
0},then 

f : 
R 
> 
R, f(x)= 
x2,is neither injective nor surjective. 
f : 
R 
> 
R+, f(x)= 
x2,issurjectivebut not injective. 
f : 
R+> 
R, f(x)= 
x2,isinjectivebut not surjective. 
f : 
R+> 
R+, f(x)= 
x2,isbijective. 
Intheseassertionswehaveusedthe continuityofthemap f(x)= 
x2 thatis discussed 
in the basic courseson analysis.In particular,wehaveusedthefact that continuous 
functions map real intervals to real intervals. The assertions also show why it is 
importantto includethe domainand codomaininthe definitionof a map. 
Theorem 2.16 Amap f : 
X > 
Yisbijectiveifand onlyifforeveryy . 
Y there 
exists exactly one x . 
X with f (x)= 
y. 

Proof .:Let f be bijective and let y1 . 
Y.Since f is surjective, there exists an 
x1 . 
X with f(x1) = 
y1.Ifsome x2 . 
X also satisfies f(x2) = 
y1, then x1 = 
x2 


2 Basic Mathematical Concepts 

follows from the injectivity of f. Therefore, there exists a unique x1 . 
X with 

f(x1)=y1. 

.:Since forall y .Y thereexistsa unique x .X with f(x)=y,itfollows that 
f(X)=Y.Thus, f surjective. Let now x1, x2 .X with f(x1)= 
f(x2)=y .Y. 
Then theassumptionimplies x1 =x2, sothat f is also injective. . 


One can show that between two sets X and Y of finite cardinality there exists a 
bijectivemapif and onlyif|X|=|Y|. 

Lemma 2.17 For sets X, Y with |X|=|Y|=m .N, there exist exactly m!:= 
1·2·... ·m pairwisedistinct bijectivemaps betweenX andY. 

Proof Exercise. 
. 


Definition 2.18 Let f :X >Y, x 
> 
f(x), and g :Y >Z, y >g(y)be maps. 
Then the composition of f and g is the map 

g.f : 
X >Z, x >g(f(x)). 

The expression g.f shouldberead “g after f”, whichstressesthe orderofthe 
composition: First f is appliedto x and then g to f(x).One immediately sees that 
f .IdX =f =IdY .f for every map f :X >Y. 

Theorem 2.19 Let f :W >X, g :X >Y,h :Y >Z be maps. Then 

(1) 
h.(g.f)=(h.g).f, i.e., the composition ofmaps is associative. 
(2) 
Iff and g are injective/surjective/bijective, then g . 
f is injective/ 
surjective/bijective. 
Proof Exercise. 
. 


Theorem 2.20 Amap f : 
X >Y is bijective if and only if there exists a map 

g :Y >X with 
g.f =IdX and f .g =IdY. 
Proof .:If f is bijective, then by Theorem 2.16 for every y .Y there exists an 
x =xy .X with f(xy)=y.We define the map g by 

g :Y >X, g(y)=xy. 
Let 

y .Y be given, then 

(f .g)(
y)=f(g(
y)) =f(x 
y)=y, hence f .g =IdY. 

If, on the other hand, 

x .X is given, then 

y = 
f(
x).Y.ByTheorem 2.16,there 
exists a unique x 

y .X with f(x 
y)=
y such that 

x =x 
y.So with 

(g.f)(
x)=(g.f)(x 
y)=g(f(x 
y)) =g(
y)=x 

y =x, 


2.2 Maps 17 
we have g. 
f =IdX. 

.:Byassumptiong.f =IdX,thus g.f is injective and thus also f is injective 
(seeExercise 2.7).Moreover, f .g =IdY,thus f .gis surjective and hence also f 
is surjective(seeExercise2.7). Therefore, f is bijective. . 


The map g :Y >X that was characterized in Theorem 2.20 is unique: If there 
were another map h :Y >X with h. 
f =IdX and f .h =IdY,then 

h =IdX .h =(g. 
f).h =g.(f .h)=g.IdY =g. 

This leads to thefollowing definition. 

Definition 2.21 If f :X >Y isabijectivemap, then the unique map g :Y >X 
fromTheorem 2.20is calledthe inverse (or inverse map)of f.Wedenotetheinverse 
of f by f.1. 

To show thatagiven mapg :Y >X isthe uniqueinverseofthe bijectivemap 

f :X >Y,itissufficienttoshow oneofthe equations g.f =IdX or f .g =IdY. 
Indeed, if f is bijective and g. 
f =IdX,then 
g =g.IdY =g.(f . 
f.1)=(g. 
f). 
f.1 =IdX . 
f.1 = 
f.1 . 

In the same way g = 
f.1 follows from the assumption f .g =IdY. 

Theorem 2.22 If f :X >Y andg :Y >Zare bijectivemaps,then thefollowing 
assertions hold: 

(1) f.1 is bijective with (f.1).1 = 
f. 
.1

(2) g. 
f isbijective with(g. 
f).1 = 
f.1 .g. 
Proof 
(1) Exercise. 
(2)We know fromTheorem 2.19 that g. 
f :X >Z is bijective. Therefore, there 
exists a(unique) inverse ofg. 
f.For the map f.1 .g.1 we have 
(f.1 .g .1= 
f.1 .= 
f.1 .

).(g. 
f)g .1 .(g. 
f)(g .1 .g). 
f 
= 
f.1 .(IdY . 
f)= 
f.1 . 
f =IdX. 

Hence, f.1 .g.1is theinverseof g. 
f. . 


2.3 Relations 
We first introduce the cartesian product3 of two sets. 

3Namedafter Rene Descartes(1596–1650),thefounderofAnalytic Geometry.GeorgCantor (1845– 
1918) used in 1895 the name “connection set of M and N” and the notation (M.N)={(m,n)}. 


2 Basic Mathematical Concepts 

Definition 2.23 If M,N are nonempty sets, then the set 
M.N :={(x,y)|x .M . 
y .N} 


is the cartesian product of M and N. An element (x,y). 
M .N is called an 
(ordered) pair. 

We can easily generalizethis definitionton .N 
nonempty sets M1,..., Mn: 

M1 .... .Mn := 
{(x1,...,xn)|xi .Mi for i =1,...,n}, 

where anelement (x1,...,xn).M1 .···.Mn is calledan (ordered) n-tuple.The 
n-fold cartesian product of a single nonempty set M is 

Mn :=M .... .M ={(x1,..., xn)|xi .M for i =1,...,n}. 

n times 

Ifin these definitionsat least oneofthesetsisempty,thentheresulting cartesian 
product is the empty set as well. 

Definition 2.24 If M,Nare nonemptysetsthenaset R .M.Nis calleda relation 
between Mand N.If M =N,then Ris calledarelationon M.Instead of (x,y).R 
we also write x .Ry or x .y,ifitisclear whichrelationis considered. 

Ifin this definitionatleast oneof thesets M and N is empty, then every relation 
between M and N is also the empty set, since then M .N =O. 

If, forinstance M =N 
and N =Q,then 

R ={(x,y).M .N |xy =1} 


is a relation between M and N that can be expressed as 

R ={(1,1),(2,1/2),(3,1/3),...}={(n,1/n)|n .N}. 

Definition 2.25 Arelation Ron a set M is called 

(1) reflexive,if x .x holds for all x .M, 
(2) symmetric,if (x .y).(y .x)holds for all x,y .M, 
(3) transitive,if (x .y . 
y .z).(x .z)holds for all x,y,z .M. 
If R is reflexive, transitive and symmetric, then it is called an equivalence relation 
on M. 
Example2.26 

(1) Let R ={(x,y).Q2 |x =.y}.Then Ris not reflexive, since x =.x holds 
only for x = 
0.If x =.y, then also y =.x, and hence R is symmetric. 
Finally, R is not transitive.Forexample, (x,y)=(1,.1). 
R and (y,z)= 
(.1,1).R,but (x,z)=(1,1)/.R. 

2.3 
Relations 19 
(2) The relation R ={(x, y) . 
Z2 | 
x . 
y}is reflexive and transitive, but not 
symmetric. 
(3) If 
f : 
R 
> 
R 
is a map, then R ={(x, y) . 
R2 | 
f(x) = 
f(y)}is an 
equivalence relation on R. 
Definition 2.27 let Rbe an equivalence relationonthe set M.Then, for x .M the 
set 

[x]R :={y .M |(x, y).R}={y .M |x .y} 


is calledthe equivalence class ofx with respect to R.The setofequivalence classes 

M/R :={[x]R |x .M} 


is calledthe quotient set of M with respect to R. 

The equivalence class [x]R of elements x .Misneverthe emptyset,sincealways 
x .x (reflexivity) and therefore x .[x]R.Ifitisclear which equivalence relation 
R is meant, we often write [x]instead oft [x]R and also skip the additional “with 
respect to R”. 

Theorem 2.28 IfRisan equivalencerelationonthe setM andifx, y . 
M, then 
thefollowing are equivalent: 

(1) 
[x]=[y]. 
(2) 
[x].[y]=O. 
(3) 
x .y. 
Proof 

(1) 
.(2) :Since x . 
x,itfollows that x .[x].From [x]=[y]it follows that 
x .[y]and thus x .[x].[y]. 
(2) 
.(3):Since [x].[y]=O, thereexistsa z .[x].[y].For this element z we 
have x .z and y .z, and thus x .z and z . 
y (symmetry) and, therefore, 
x .y (transitivity). 
(3) 
.(1):Let x .yand z .[x], i.e., x .z.Using symmetry and transitivity,we 
obtain y .z, and hence z .[y].This means that [x].[y].In an analogous 
wayone showsthat [y].[x], and hence [x]=[y]holds. . 

Theorem 2.28 showsthat fortwo equivalence classes [x]and [y]we have either 
[x]=[y]or [x].[y]=O. Thusevery x .Mis containedinexactly oneequivalence 
class(namely in [x]), so that an equivalence relation R yields a partitioning or 
decomposition of M into mutually disjoint subsets. Every element of [x]is called a 
representative of the equivalence class [x].Avery useful and general approach that 
we will oftenusein this bookisto partitionasetofobjects(e.g. setsof matrices)into 
equivalence classes,andtofindineachsuchclassarepresentativewithaparticularly 
simple structure. Sucharepresentativeis calleda normalform with respect to the 
given equivalence relation. 


2 Basic Mathematical Concepts 

Example2.29 For a given numbern .N 
the set 

Rn :={(a,b).Z2 |a .bis divisibleby n without remainder} 


is an equivalence relation on Z,since thefollowing properties hold: 

•Reflexivity: a .a =0isdivisible byn without remainder. 
•Symmetry: If a .bis divisibleby n without remainder,then also b.a. 
• 
Transitivity:Leta .b and b.c be divisibleby n without remainder and write 
a .c =(a .b)+(b.c).Bothsummandsontheright aredivisibleby n without 
remainder and hence this also holds for a .c. 
For a .Z 
the equivalence class [a]is called residue class of a modulo n, and 
[a]=a +nZ 
:={a +nz |z .Z}.The equivalence relation Rn yields a partitioning 
of Z 
into n mutually disjoint subsets.In particular,wehave 

n.1 

 


[0].[1].···.[n .1]= 
[a]=Z. 

a=0

The set of all residue classes modulo n, i.e., the quotient set with respect to Rn,is 
often denoted by Z/nZ.Thus, Z/nZ 
:={[0],[1],...,[n .1]}.This set plays an 
important rolein themathematical fieldofNumber Theory. 

Exercises 

2.1 Let A,B,C be assertions. Showthat thefollowing assertions aretrue: 
(a) For .and .the associativelaws 
[(A.B).C].[A.(B.C)], [(A.B).C].[A.(B.C)] 


hold. 

(b) For .and .the commutative laws 
(A.B) .(B.A), (A.B) .(B.A) 

hold. 

(c) For .and .thedistributivelaws 
[(A.B).C].[(A.C).(B.C)], [(A.B).C].[(A.C).(B.C)] 


hold. 

2.2 Let A,B,C be assertions. Showthat thefollowing assertions aretrue: 
(a) A.B .A. 
(b) [A.B].[(A.B).(B .A)]. 

2.3 
Relations 21 
(c) 
¬(A.B).[(¬A).(¬B)]. 
(d) 
¬(A.B).[(¬A).(¬B)]. 
(e) 
[(A.B).(B .C)].[A.C]. 
(f) [A.(B.C)].[(A.¬B).C]. 
(The assertions(c)and(d)are calledthe De Morgan laws for .and ..) 
2.3 Prove Theorem 2.8. 
2.4 Showthat fortwo sets M,N thefollowing holds: 
N .M . 
M .N =N . 
M.N =M. 

2.5 Let X,Y be nonempty sets, U,V .Y nonempty subsets and let f :X >Y 
be a map. Show that f.1(U .V)= 
f.1(U). 
f.1(V).Let U,V . 
X be 
nonempty. Check whether f(U .V)= 
f(U). 
f(V)holds. 
2.6Are thefollowing maps injective, surjective, bijective? 
(a) 
f1 :R 
\{0}>R,x 
> 
1. 
x 

(b) 
f2 :R2 >R,(x,y)>x +y. 
(c) 
f3 :R2 >R,(x,y)>x2 +y2 .1. 
nn even,

(d) 
f4 :N 
>Z,n 
> 
2, 
n.1

.2 , n odd. 

2.7 Showthat fortwo maps 
f :X >Y and g :Y >Z thefollowing assertions 
hold: 
(a) 
g. 
f is surjective .g is surjective. 
(b) 
g. 
f is injective . 
f is injective. 
2.8 Let a . 
Z 
be given. Show that the map fa : 
Z 
> 
Z, fa(x)= 
x +a is 
bijective. 
2.9ProveLemma2.17. 
2.10 Prove Theorem 2.19. 
2.11 Prove Theorem 2.22 (1). 
2.12 Find two maps f,g :N 
>N,sothat simultaneously 
(a) 
f is not surjective, 
(b) 
g is not injective, and 
(c) 
g. 
f is bijective. 
2.13 Determine all equivalence relations on the set {1,2}. 
2.14 Determine a symmetric and transitive relation on the set {a,b,c}that is not 
reflexive. 

Chapter3 
Algebraic Structures 

An algebraicstructureisasetwith operations betweenits elementsthatfollowcertain 
rules.Asanexampleofsuchastructure considertheintegersandthe operation ‘+.’ 
What are the properties of this addition? Already in elementary school one learns 
that the sum a +b of twointegers a and b is another integer.Moreover, thereis 
a number0 such that0 +a =a for every integer a, and for every integer a there 
exists an integer .a such that (.a +a =0. The analysisof thepropertiesofsuch 
concreteexamples leadsto definitionsof abstract conceptsthat arebuiltonafew 
simple axioms.Fortheintegersandthe operation addition,thisleadstothealgebraic 
structure of a group. 

This principleofabstractionfrom concreteexamplesis oneofthestrengthsand 
basic working principles of Mathematics. By “extracting and completely exposingthe 
mathematicalkernel”(DavidHilbert) we also simplify our further work: 
Every proved assertion about an abstract concept automatically holds for all concreteexamples.
Moreover,by combining defined conceptswe can move to further 
generalizations andin thisway extend themathematical theorystepby step. Hermann 
Gunther Gra.mann (1809–1877) described this procedureasfollows1:“... the 
mathematical methodmovesforwardfromthesimplest conceptsto combinationsof 
them andgainsvia such combinations new and more general concepts.” 

3.1 Groups 
We begin with a set and an operation with specific properties. 
Definition 3.1 Agroup is a set G with a map, called operation, 

.:G.G >G ,(ab >a .b 

1“... diemathematische Methode hingegenschreitetvonden einfachstenBegriffenzuden zusammengesetzterenfort, 
andgewinnt so durchVerknupfung desBesonderen neueand allgemeinere 
Begriffe.” 

©SpringerInternationalPublishing Switzerland 2015 23 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_3 

3 Algebraic Structures 

that satisfies thefollowing: 

(1) The operation . 
is associative, i.e., (a . 
b . 
c = 
a . 
(b. 
c holds for all 
abc . 
G. 
(2) There exists an element e . 
G, called a neutralelement,for which 
(a) e . 
a = 
a for all a . 
G, and 
(b) for every a . 
G there exists an

a . 
G, called an inverse element of a, with 

a . 
a = 
e. 
If a . 
b = 
b. 
a holds for all ab . 
G,then thegroup is called commutative or 
2

Abelian. 

As short hand notation for a group we use (G . 
or just G,ifisclear which 
operationis used. 

Theorem 3.2 For everygroup(G . 
thefollowing assertions hold: 

(1) Ife . 
Gisaneutralelementandifa 

a . 
Gwith
a.a = 
e,then alsoa.
a = 
e. 
(2) Ife . 
Gis a neutralelement and if a . 
G, then also a. 
e = 
a. 
(3) G contains exactly one neutralelement. 
(4) For every a . 
G there exists a unique inverse element. 
Proof 

(1) Let e . 
G be a neutral element and let a 

a . 
G satisfy 

a . 
a = 
e.Then by 
Definition 3.1 there exists an element a1 . 
G with a1 . 

a = 
e.Thus, 
a . 

a = 
e . 
(a . 

a = 
(a1 . 

a . 
(a . 
aa1 . 
((

a . 
a . 

a 
= 
a1 . 
(e . 

a = 
a1 . 

a = 
e 

(2) Let e . 
G be a neutral element and let a . 
G.Then there exists 

a . 
G with 

a . 
a = 
e.By(1) then also a . 

a = 
e and it follows that 
a . 
e = 
a . 
(

a . 
a = 
(a . 

a . 
a = 
e . 
a = 
a 

(3) Let ee1 . 
G be two neutral elements. Then e1 . 
e = 
e, since e1 is a neutral 
element.Since e is alsoa neutralelement,itfollows that e1 = 
e . 
e1 = 
e1 . 
e, 
wherefor thesecond identity wehave used assertion(2).Hence, e = 
e1. 
(4) Let 
aa1 . 
G betwoinverse elementsofa . 
G and let e . 
G be the(unique) 
neutralelement.Then with(1)and(2)itfollows that 

a = 
e . 

a = 
(a1 . 
a . 

a = 
a1 . 
(a . 

a = 
a1 . 
e = 
a1 . 


2Namedafter NielsHenrikAbel(1802–1829),the founderofgroup theory. 


3.1 
Groups 25 
Example3.3 

(1) 
(Z +, (Q + 
and (R + 
are commutative groups.Inall thesegroups the neutral 
elementisthe number0(zero)andtheinverseof a is the number .a.Instead 
of a +(.b we usually write a .b.Since the operationisthe addition, these 
groups are also called additivegroups. 
The natural numbers N with the additiondo not formagroup, since thereisno 
neutral element in N.Ifwe consider theset N0,which includes also the number 
0(zero),then0+a =a +0=a for all a .N0,but only a =0has an inverse 
element in N.Hence also N0 with the addition does not formagroup. 

(2) The sets Q \{0}and R \{0}with theusual multiplicationform commutative 
groups.Inthese multiplicativegroups,the neutralelementisthe number1(one) 
and theinverse elementof a is the number 1 (or a.1). Instead of a ·b.1 we also 
a 

a

write b or a/b. 
Theintegers Z with themultiplicationdo not formagroup. Theset Z includes 
the number1, forwhich1·a =a ·1=a for all a .Z,but no a .Z\{.11}
has an inverse element in Z. 


Definition 3.4 Let (G . 
beagroup and H .G.If(H . 
isagroup, thenitis 
called a subgroup of (G .. 

Thenexttheoremgivesanalternative characterizationofa subgroup. 

Theorem 3.5 (H . 
isasubgroupof thegroup (G . 
if and onlyifthe following 
properties hold: 

(1) 
O	=H .G. 
(2) 
a .b.Hfor allab.H. 
(3) For every a .Halsotheinverse element satisfies

a .H. 
Proof Exercise. 
. 


Thefollowing definition characterizes maps between twogroups whichare compatible 
with therespectivegroup operations. 

Definition 3.6 Let (G1 . 
and (G2 . 
be groups.Amap 

. 
: 
G1 >G2 g >.(g 

is called a group homomorphism,if 

.(a .b = 
.(a . 
.(b for all ab.G1 

Abijectivegroup homomorphismis calledagroup isomorphism. 


3 Algebraic Structures 

3.2 Rings and Fields 
Inthis sectionweextendthe conceptofagroupand discussmathematical structures 
that are characterizedbytwooperations.Asmotivatingexample considertheintegers 
with the addition, i.e., thegroup (Z + 
.We can multiply theelementsof Z and this 
multiplicationisassociative, i.e., (a·b ·c = 
a·(b·c forall abc . 
Z. Furthermore 
the addition and multiplicationsatisfy thedistributivelaws a · 
(b+ 
c = 
a · 
b+ 
a · 
c 
and (a + 
b · 
c = 
a · 
c + 
b· 
c for all integers abc.Theseproperties make Z with 
addition and multiplicationintoa ring. 

Definition 3.7 Aring is a set Rwith two operations 

+: 
R. 
R . 
R ,(ab 
. 
a + 
b (addition) 

.: 
R. 
R . 
R ,(ab 
. 
a . 
b (multiplication) 

that satisfy thefollowing: 

(1) 
(R + 
is a commutative group. 
Wecallthe neutralelementin this groupzero,and write0.Wedenotetheinverse 
element of a . 
Rby .a, and write a . 
binstead of a + 
(.b . 
(2) The multiplicationisassociative, i.e., (a. 
b . 
c = 
a. 
(b. 
c forall abc . 
R. 
(3) The distributive laws hold, i.e., for all abc . 
Rwe have 
a . 
(b+ 
c = 
a . 
b+ 
a . 
c 

(a + 
b . 
c = 
a . 
c + 
b. 
c 

Aringis called commutative if a . 
b= 
b. 
a for all ab . 
R. 
An element1 . 
Ris called unitif1 . 
a = 
a . 
1= 
a for all a . 
R.Inthis case Ris 
called a ring with unit. 


Ontherighthandsideofthetwo distributivelawswehave omittedthe parentheses, 
since multiplicationissupposed to bind stronger than addition, i.e., a + 
(b. 
c = 
a + 
b. 
c.Ifitisusefulfor illustration purposes we neverthelessuse parentheses, 
e.g., we sometimes write (a . 
b + 
(c . 
d instead of a . 
b+ 
c . 
d. 


Analogous to the notationfor groups we denotea ring with (R +. 
or just with 
R,ifthe operations areclear from the context. 
Inaringwithunit,theunitelementis unique:If1 e . 
Rsatisfy1 .a = 
a. 
1= 
a 
and e . 
a = 
a . 
e = 
a for all a . 
R,then in particular e = 
e . 
1= 
1. 
For a1 a2 an . 
Rwe usethe following abbreviationsforthe sum and product 
of these elements: 

n 
n 

aj := 
a1 + 
a2 ++ 
an and aj := 
a1 . 
a2 .. 
an 

j=1 
j=1 


3.2 Rings andFields 27 
n

Moreover, an := 
1 a for all a . 
R and n . 
N.If > 
k, then we define the

j=

empty sum as 

k 

aj :=0 
j=. 


In a ring with unit we also define for >kthe emptyproduct as 

k 

aj :=1 
j=. 


Theorem 3.8 ForeveryringRthefollowing assertions hold: 

(1) 0.a =a .0=0for all a .R. 
(2) a .(.b =.(a .b =(.a .band(.a .(.b =a .bfor allab.R. 
Proof 

(1) For every a . 
R we have0.a = 
(0+0 .a = 
(0.a +(0.a .Adding 
.(0.a on theleft and right hand sidesof this equality we obtain0=0.a.In 
the same way we can show that a .0=0for alla .R. 
(2) Since (a.b +(a.(.b =a.(b+(.b =a.0=0, it follows thata.(.b 
isthe(unique) additiveinverseof a .b, i.e., a .(.b =.(a .b .Inthe same 
waywe can show that (.a .b=.(a .b . Furthermore,wehave 
0=0.(.b =(a +(.a .(.b =a .(.b +(.a .(.b 
=.(a .b +(.a .(.b 

and thus (.a .(.b =a .b. . 


It is immediately clear that (Z +. 
is a commutative ring with unit. This is the 
standardexample,by whichthe conceptofaringwas modeled. 

Example3.9 Let M be a nonempty set and let R be the set of maps f : 
M >R. 
Then (R +. 
with the operations 

+:R.R >R ,(fg 
. 
f +g ,(f +g )(x := 
f(x +g(x 
.:R.R >R ,(fg 
. 
f .g ,(f .g )(x := 
f(x ·g(x 

is a commutative ring with unit. Here f(x +g(x and f(x ·g(x arethe sum and 
productoftworeal numbers.The zerointhisringisthemap0R :M >R, x >0, 
and the unitisthe map1R : 
M >R, x >1, where0and1are thereal numbers 
zero and one. 

In the definition of a ring only additive inverse elements occur. We will now 
formally define the conceptofamultiplicativeinverse. 


3 Algebraic Structures 

Definition 3.10 Let (R +. 
be a ring with unit. An element b . 
R is called an 
inverse of a .R(with respect to .), if a .b=b.a =1. An element of Rthat has 
aninverseis called invertible. 

It is clear from the definition that b . 
R is an inverse of a . 
R if and onlyif 
a .Ris an inverse ofb.R.In general,however, noteveryelementinaringmust 
be(oris)invertible.Butifan elementisinvertible,thenithasa uniqueinverse,as 
showninthe following theorem. 

Theorem 3.11 Let (R +. 
be a ring with unit. 

(1) Ifa .Risinvertible,then theinverseis unique and we denoteitbya.1 . 
.1 .1

(2) Ifa b.Rareinvertible thena .b.Risinvertible and(a .b =b.1.a. 
Proof 

(1) Ifbb.Rareinverses ofa .R,then b=b.1=b.(a .b =(b.a .b= 
1.b=b. 

(2) Since a and bareinvertible, b.1 .a.1 .Ris well defined and 
(b.1.a.1 .(a .b =((b.1.a.1 .a .b =(b.1.(a.1.a .b=b.1.b =1 

.1

In the same way we can show that (a . 
b . 
(b.1 . 
a= 
1, and thus 

.1 .1

(a . 
b =b.1 .a. . 


From an algebraic pointof view thedifference between theintegersonthe one 
hand, and therational or real numbersonthe other,isthatin thesets Q and R every 
element(exceptforthe numberzero)isinvertible.This “additional structure”makes 
Q and R into fields. 

Definition 3.12 Acommutative ringRwith unitis calledafield,if0 	=1andevery 
a .R\{0}is invertible. 

By definition,every fieldisa commutative ring with unit,butthe converse does 
not hold. One can also introduce the concept of a field based on the concept of a 
group (cp. Exercise 3.15). 

Definition 3.13 Afield is a set K with two operations 

+:K .K >K ,(ab >a +b ,(addition 
.:K .K >K ,(ab >a .b ,(multiplication 


3.2 
Rings andFields 29 
that satisfy thefollowing: 

(1) 
(K + 
is a commutative group. 
Wecallthe neutralelementin this groupzero,and write0.Wedenotetheinverse 
element of a . 
K by .a, and write a .binstead of a +(.b . 
(2) 
(K \{0}. 
is a commutative group. 
Wecallthe neutralelementin this groupunit,and write1.Wedenotetheinverse 
.1

element of a . 
K \{0}by a. 

(3) The distributive laws hold, i.e., for all abc . 
K we have 
a .(b+c =a .b+a .c 
(a +b .c =a .c +b.c 

We nowshowafewusefulpropertiesoffields. 

Lemma 3.14 Forevery fieldK thefollowing assertions hold: 

(1) 
K has at leasttwo elements. 
(2) 
0.a =a .0=0for all a . 
K. 
(3) 
a .b=a .c and a 	=0implythatb=cfor allab c . 
K. 
(4) 
a .b=0implythat a =0orb=0foralla b. 
K. 
Proof 

(1)Thisfollows fromthe definition, since01. 
K with0	=1. 
(2)This has already been shownfor rings (cp. Theorem 3.8). 
(3) Since a 	=0, we knowthata.1exists.Multiplying bothsidesof a .b =a .c 
fromthe left with a.1 yields b=c. 
(4) Supposethat a .b=0.Ifa =0, then we are finished. Ifa 	=0, thena.1 exists 
and multiplying both sides of a .b=0fromthe left witha.1 yields b=0. . 

For a ring R an element a . 
R is called a zero divisor,3 ifa b . 
R\{0}exists 
with a .b =0. The elementa =0is calledthe trivial zerodivisor.Property (4)in 
Lemma 3.14 means that fields contain onlythe trivial zerodivisor.Thereare also 
ringsin whichproperty(4) holds,for instancetheringofintegers Z.Inlater chapters 
wewill encounterringsof matricesthat contain non-trivial zerodivisors(seee.g.the 
proof of Theorem 4.9 below). 

Thefollowing definitionis analogoustothe conceptsofa subgroup(cp. Definition 
3.4)andasubring(cp. Excercise 3.14). 

Definition 3.15 Let (K +. 
be a field and L . 
K.If(L +. 
is a field, then it 
is called a subfield of (K +. 
. 

Astwoveryimportantexamplesforalgebraic conceptsdiscussedabovewenow 
discussthe field ofcomplex numbers and the ring of polynomials. 

3Theconceptofzerodivisorswas introducedin 1883by Karl TheodorWilhelmWeierstra.(1815– 
1897). 


3 Algebraic Structures 

Example3.16 The set of complex numbers is defined as 

C :={(xy |xy .R}=R.R 

On this setwe define thefollowing operations as addition and multiplication: 

+:C.C >C ,(x1 y1 +(x2 y2 := 
(x1 +x2 y1 +y2 

·:C.C >C ,(x1 y1 ·(x2 y2 := 
(x1 ·x2 .y1 ·y2 x1 ·y2 +x2 ·y1 

On theright hand sides we hereuse the addition and themultiplicationinthe field 

R.Then (C +· 
is a field with the neutral elements with respect to addition and 
multiplicationgiven by 
0C =(00 
1C =(10 

andtheinverse elementswith respectto additionand multiplicationgivenby 

.(xy =(.x .y for all (xy .C 

xy

.1

(xy =. 
for all (xy .C\{(00 }

x2 +y2 x2 +y2 

Inthemultiplicativeinverse elementwehave written a instead of a ·b.1, which is 

b 

the common notationin R. 

Consideringthe subset L :={(x 0 |x .R}.C, we can identify every x .R 
with an element of the set L via the (bijective) map x 
. 
(x 0 . In particular, 
0R >(00 =0C and1R >(10 =1C.Thus,we can interpret Ras subfieldof C 
(although R is not really a subset of C), and we do not have to distinguish between 
the zero and unitelementsin R and C. 

Aspecial complex number is the imaginary unit (0 1 , which satisfies 

(01 ·(01 =(0·0.1·10·1+0·1 =(.10 =.1 

Hereagainwehaveidentifiedthereal number .1with the complexnumber(.10 . 
Theimaginary unitis denotedby i, i.e., 

i:=(01 

and hence we can write i2 =.1. Usingthe identificationof x .R with (x 0 .C 
we can write z =(xy .C as 

(xy =(x 0 +(0 y =(x 0 +(01 ·(y 0 =x +iy =Re(z +iIm(z 


3.2 Rings andFields 31 
In the last expression Re(z = 
x and Im(z = 
y are the abbreviations for real 
part and imaginary part of the complex number z = 
(xy .Since (01 · 
(y 0 = 
(y 0 · 
(0 1 , i.e., iy = 
yi,itisjustified to write the complex number x + 
iy as 
x + 
yi. 

For a given complex numberz = 
(xy or z = 
x + 
iy the number z := 
(x .y , 
respectively z := 
x . 
iy,is calledthe associated complex conjugate number.Using 
the(real)squareroot,the modulus or absolute value ofa complex numberis defined 
as 

	1/2 	1/2

1/2 221/2

|z|:= 
(zz = 
(x + 
iy )(x . 
iy = 
x2 . 
ixy + 
iyx . 
i2y= 
(x2 + 
y

(Again,for simplificationwehaveomittedthe multiplicationsign betweentwocomplex 
numbers.)This equationshows that the absolutevalueofa complex numberis 
a nonnegative real number. Further properties ofcomplex numbers are stated in the 
exercises at the end of this chapter. 

Example3.17 Let (R +· 
bea commutative ring with unit.A polynomial overR 
andin theindeterminateorvariable t is anexpressionof theform 

p = 
.0 · 
t0 + 
.1 · 
t1 ++ 
.n · 
tn 

where .0 .1 .n . 
R arethe coefficients of the polynomial. Instead of .0 · 
t0, 
t1 and . 
j · 
tj we oftenjustwrite .0, t and . 
jtj.The set of all polynomials over R 
is denoted by R[t . 

Let 

p = 
.0 + 
.1 · 
t ++ 
.n · 
tn q = 
.0 + 
.1 · 
t ++ 
.m · 
tm 

be two polynomials in R[t with n . 
m.If n > 
m, then we set .j = 
0for j = 
m + 
1 n and call pand q equal,written p = 
q,if .j = 
. 
j for j= 
01 n. 
In particular,wehave 

.0 + 
.1 · 
t ++ 
.n · 
tn = 
.n · 
tn ++ 
.1 · 
t + 
.0 

0+ 
0· 
t ++ 
0· 
tn = 
0 

The degree of the polynomial p = 
.0 + 
.1 · 
t ++ 
.n · 
tn, denoted by deg( 
p , 
is defined as thelargestindex j,for which .j 	
= 
0. If no such index exists, then the 
polynomialisthe zero polynomial p = 
0and we setdeg( 
p := 
... 

Let pq . 
R[t as above have degrees nm, respectively, with n . 
m.If n > 
m, 
then weagainset . 
j = 
0, j= 
m + 
1 n.We define thefollowing operations on 
R[t : 


3 Algebraic Structures 

p+q :=(.0 +.0 +(.1 +.1 ·t ++(.n +.n ·tn 

p.q :=.0 +.1 ·t ++.n+m ·tn+m .k := 
.i.j 

i+j=k 

With these operations(R[t +. 
isa commutativering with unit. The zeroisgiven 
by the polynomial p = 
0and the unitis p = 
1·t0 = 
1. But R[t it is not a field, 
since not every polynomial p . 
R[t ]\{0}is invertible, not even if R isa field.For 
example, for p = 
t and any other polynomial q =.0 +.1t ++.mtm . 
R[t 
we have 

p.q =.0t +.1t2 ++.mtm+1 	=1 

and hence pis not invertible. 

Ina polynomialwe can “substitute”thevariable t by some other object when the 
resultingexpression canbeevaluated algebraically.Forexample,wemaysubstitute 
t by any . 
. 
R and interpret the addition and multiplication as the corresponding 
operations in the ring R.This defines a map from Rto Rby 

. 

. 
p(. 
=.0 ·.0 +.1 ·.1 ++.n ·.n .k :=. 
··. 
k =01 n 

ktimes 

where .0 =1. 
R(thisisanempty product). Here one should not confusethe ring 
element p(. 
with the polynomial pitself,butrather thinkof p(. 
as an evaluation 
of pat ..Wewillstudythepropertiesofpolynomialsin more detaillateron,andwe 
will also evaluate polynomials at other objects such as matrices or endomorphisms. 

Exercises 

3.1Determine forthe following (M . 
whether theyformagroup: 
b

(a) M ={x .R|x > 
0}and .: 
M.M >M, (ab >a. 
(b) M =R\{0}and .:M .M . 
M, (ab 
. 
ab. 
3.2 Let ab .R,the map 
fa b : 
R.R >R.R ,(xy >(ax .by ay 

and the set G ={fa b ~ 
ab . 
R a 	
= 
0}be given. Show that (G . 
is a 
commutative group, when the operation .: 
G .G . 
G is defined as the 
compositionoftwo maps (cp. Definition 2.18). 

3.3 Let X 	
= 
Obe a set and let S(X ={f : 
X . 
X~ 
f is bijective}. Show that 
(S(X . 
is agroup. 
3.4 Let (G . 
be a group.For a . 
G denoteby .a . 
G the(unique) inverse 
element. Showthe following rulesfor elementsof G: 
(a) .(.a =a. 
(b) .(a .b =(.b .(.a . 
(c) a .b1 =a .b2 .b1 =b2. 
(d) a1 .b=a2 .b.a1 =a2. 

3.2 Rings andFields 33 
3.5 Prove Theorem 3.5. 
3.6 Let (G . 
beagroup and fora fixed a .G let ZG(a ={g .G|a .g = 
g.a}. Showthat ZG(a is a subgroup of G. 
(This subgroup of all elements of G that commute with a is called centralizer 
of a.) 
3.7 Let . 
:G >H beagroup homomorphism. Showthe following assertions: 
(a) If U .G is a subgroup, then also .(U . 
H isasubgroup. If,furthermore, 
Gis commutative,then also .(U is commutative(evenif H is not 
commutative). 
(b) If V .H is a subgroup, then also ..1(V .G is a subgroup. 
3.8 Let . 
:G . 
H beagroup homomorphism and let eG and eH be the neutral 
elementsofthe groups G and H, respectively. 
(a) Showthat .(eG =eH. 
(b) Letker(. 
:={g .G|.(g =eH}. Showthat . 
is injectiveifand only 
ifker(. 
={eG}. 
3.9 Showthe propertiesinDefinition 3.7 for (R +. 
from Example 3.9 in order 
to showthat (R +. 
isacommutativeringwithunit.SupposethatinExample 
3.9 we replace the codomain R of the maps by a commutative ring with unit. 
Is (R +. 
then still a commutative ring with unit? 
3.10 Let Rbe a ring and n .N. Showthe following assertions: 
an 

if n is even 

(a) For all a .Rwe have (.an = 
an 
n 

.if n is odd 

(b)Ifthereexistsa unitin Rand if a=0for a .R,then1.a is invertible. 
(An element a .Rwith an =0for some n .N is called nilpotent.) 
3.11 Let Rbearingwith unit. Showthat1 =0ifand onlyif R ={0}. 
3.12 Let (R +. 
be a ring with unit and let R. 
denote the set of all invertible 
elements of R. 
(a) Showthat (R.. 
isagroup (calledthe group of units of R). 
(b) Determine the sets Z. 
, K., and K[t ., when K is a field. 
3.13 For fixed n .N let nZ ={nk |k .Z}and Z/nZ ={[0 [1 [n .1 ]}be 
as in Example 2.29. 
(a) Showthat nZ is a subgroup of Z. 
(b) Define by 
.:Z/nZ.Z/nZ >Z/nZ ,([a [b >[a ].[b ]=[a +b 

:Z/nZ.Z/nZ >Z/nZ ,([a [b >[a ][b ]=[a ·b 

an addition and multiplicationin Z/nZ,(with +and ·being the addition 
and multiplicationin Z). Showthe following assertions: 


3 Algebraic Structures 

(i) .and are well defined. 
(ii) (Z/nZ .. 
is a commutative ring with unit. 
(iii) (Z/nZ .. 
isa fieldifand onlyif n isaprime number. 
3.14 Let (R +. 
be a ring.A subset S . 
R is called a subring of R,if (S +. 
isaring. Showthat S is a subring of R if and onlyifthe following properties 
hold: 
(1) S . 
R. 
(2) 0R .S. 
(3) For all rs .Salso r +s .Sand r .s . 
S. 
(4) For all r . 
Salso .r .S. 
3.15 ShowthattheDefinitions 3.12and3.13ofafield describethesame mathematical 
structure. 
3.16 Let (K +. 
be a field. Show that (L +. 
is a subfield of (K +. 
(cp. 
Definition 3.15),if and onlyifthe following properties hold: 
(1) L . 
K. 
(2) 0K 1K . 
L. 
(3) a +b. 
L and a .b.L for all ab . 
L. 
(4) .a . 
L for all a . 
L. 
(5) a.1 .L for all a .L\{0}. 
3.17 Showthatina field1 +1=0holdsif and onlyif1+1+1+1=0. 
3.18 Let (R +. 
beacommutativeringwith1 	=0that does not contain non-trivial 
zerodivisors. (Sucharingis calledan integral domain.) 
(a) Define on M = 
R.R\{0}a relationby 
(xy . 
(

x 

y . 
x .
y = 
y.
x 

Showthat thisis an equivalence relation. 

(b)Denotethe equivalence class [(xy by x . Showthat thefollowing maps 
y

are well defined: 

x 
xx .y+y.
x 

.: 
(M/ 
.. 
(M/ 
.. 
(M/ 
. 
with . 
:= 


y 
yy.
y 
x 
xx .
x 

: 
(M/ 
.. 
(M/ 
.. 
(M/ 
. 
with . 
:= 


y 
yy.
y 

where M/ 
.denotesthequotientsetwith respectto .(cp. Definition 2.27). 

(c) Show that (M/ 
... 
isa field. (This fieldis calledthe quotient field 
associated with R.) 
(d)Which fieldis (M/ 
... 
for R =Z? 
3.19 In Exercise 3.18 consider R = 
K[t ,the ringof polynomialsoverthe field K, 
and constructin thiswaythe fieldof rational functions. 

3.2 Rings andFields 
35 
3.20 Let a =2+i .C and b =1.3i .C.Determine .a .b, a +ba.b, 
.1 b.1
aa.1ab.1b, ab ba. 

3.21 Showthe following rulesfor the complex numbers: 
(a) 
z1 +z2 =z1 +z2 and z1z2 =z1 z2 for all z1 z2 .C. 
.11
(b) z.1 =(z .1 and Re(z= 
2Re(z for all z .C\{0}. 
|z|

3.22 Showthat the absolutevalueof complex numberssatisfies thefollowing properties: 
(a) |z1z2|=|z1||z2|for all z1 z2 .C. 
(b) |z|.0for allz .C with equalityif and onlyif z =0. 
(c) |z1 +z2|.|z1|+|z2|for all z1 z2 .C. 

Chapter4 
Matrices 

In this chapter we define matrices with theirmostimportant operations and we study 
severalgroupsandringsof matrices.JamesJosephSylvester (1814–1897) coinedthe 
term matrix1 in 1850 and described matrices as “an oblong arrangement of terms”. 
The matrix operations defined in this chapter were introduced by Arthur Cayley 
(1821–1895)in 1858.Hisarticle“Amemoironthetheoryof matrices”wasthefirst 
to consider matrices as independent algebraic objects. In our book matrices form the 
central approachtothetheoryofLinear Algebra. 

4.1 BasicDefinitionsand Operations 
Webeginwithaformal definitionofmatrices. 

Definition 4.1 Let R be a commutative ring with unit and let n, 
m . 
N0.An array 
of theform 
.. 


a11 a12 ··· 
a1m 

..

a21 a22 ··· 
a2m 

.

A=[aij = 


. 


. 


. 


. 


..
. 


..
. 
...

. 


an1 an2 ··· 
anm 

1TheLatinword “matrix” means “womb”. Sylvesterconsidered matrices as objects “outof which 
we may form various systems of determinants” (cp. Chap.5). Interestingly, the English writer 
CharlesLutwidge Dodgson (1832–1898),betterknownbyhispen nameLewisCarroll, objectedto 
Sylvester’stermand wrotein 1867:“Iamawarethattheword ‘Matrix’is alreadyinusetoexpress 
theverymeaningfor whichI usetheword ‘Block’;but surelythe formerword meansratherthe 
mould,or form,intowhich algebraic quantitiesmaybe introduced,thanan actualassemblageof 
such quantities”.Dodgson also objected to the notation aij for the matrix entries: “…most of the 
space is occupied by a number of a’s,which arewholly superfluous,while the onlyimportant part 
ofthe notationisreducedto minutesubscripts,alikedifficulttothe writerandthe reader.” 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_4 

4 Matrices 

with aij . 
R, i = 
1,...,n, j = 
1,...,m,is called a matrix of size n .m over R. 
The aij are calledthe entries or coefficients ofthematrix.Thesetofallsuch matrices 
is denoted by Rn,m . 

Inthefollowingwe usually assume (withoutexplicitly mentioningit)that1 =0 
in R.This excludes the trivial case of the ring that contains only the zero element 
(cp. Exercise 3.11). 

Formally,inDefinition4.1 for n = 
0or m = 
0weobtain “emptymatrices”ofthe 
size0 .m, n .0or0 .0.We denotesuch matricesby[ 
.They will be used for 
technical reasonsinsomeoftheproofsbelow.Whenweanalyze algebraicproperties 
of matrices,however,wealways consider n,m .1. 

The zero matrix in Rn,m, denotedby0n,m or just0,is thematrixthat has all its 
entries equal to0 . 
R. 

Amatrixofsize n .n is calleda square matrix or just square.The entries aii for 
i = 
1,...,n are calledthe diagonal entries of A.The identity matrix in Rn,n is the 
matrix In :=[.ij , where 

1, 
if i = 
j,

.ij := 
(4.1)

0, 
if i 
= 
j. 


is the Kronecker delta-function.2 If it is clear which n is considered, then we just 
write I instead of In.For n =0we set I0 :=[ 
. 

The ith row of A . 
Rn,m is [ai1,ai2,...,aim . 
R1,m , i = 
1,...,n, where we 
use commasfor the optical separationofthe entries.The jth column of Ais 

.. 


a1j 

..

a2j 

.. 


. 
Rn,1 

. 
. 
. 
, 
j=1,...,m. 


. 
. 
.

. 


anj 

Thus,the rows and columnsofamatrixareagainmatrices. 

If1 .m matrices ai := 
[ai1,ai2,...,aim . 
R1,m , i = 
1,...,n, are given, then 
we can combine them to the matrix 

... 
. 


a1 a11 a12 ··· 
a1m 

... 
.

a2 a21 a22 ··· 
a2m 

... 
. 


. 
Rn,m

A= 
. 
. 
. 
= 
. 
.. 
. 
. 
. 


. 
. 
.. 
.. 
. 
.

. 
... 


an an1 an2 ··· 
anm 

2LeopoldKronecker (1823–1891). 


4.1 Basic Definitions andOperations 39 
We thendo not write squarebracketsaround therowsof A.Inthe samewaywe can 
combine the n .1matrices 

.. 


a1j 

..

a2j 

.. 


. 
Rn,1

aj :=. 
. 
. 
, 
j=1,...,m, 


. 
. 
.

. 


anj 

to the matrix 

.. 


a11 a12 ··· 
a1m 

..

a21 a22 ··· 
a2m 

.. 


. 
Rn,m

A=[a1,a2,...,am = 
. 
.. 
. 
. 
. 


. 
.. 
. 
.

.. 
. 


an1 an2 ··· 
anm 

If n1,n2,m1,m2 . 
N0 and Aij . 
Rni,mj, i, 
j = 
1,2, then we can combine these 
four matrices to the matrix 

A11 A12 . 
Rn1+n2,m1+m2

A= 
.

A21 A22 

The matrices Aij arethen called blocks of the block matrix A. 
We nowintroduce four operations formatrices andbeginwith theaddition: 

+: 
Rn,m . 
Rn,m . 
Rn,m ,(A,B) 

. 
A+ 
B :=[aij +bij . 


The additionin Rn,m operates entrywise, based on the additionin R.Note that the 
additionis only defined formatricesof equal size. 

The multiplication of two matrices is defined as follows: 

m 

.: 
Rn,m . 
Rm,s . 
Rn,s,(A,B) 

. 
A. 
B =[cij , 
cij := 
aikbkj. 


k=1 

Thus,the entry cij of theproduct A. 
B is constructed by successive multiplication 
and summing up the entries in the ith row of Aand the jth column of B.Clearly,in 
order to define theproduct A. 
B,the number of columns of Amust be equal to the 
number of rows in B. 

In the definition of the entries cij of the matrix A. 
B we have not written the 
multiplicationsymbol forthe elementsin R.Thisfollows theusual conventionof 
omittingthe multiplicationsignwhenitis clear whichmultiplicationis considered. 
Eventually we will also omit the multiplication sign between matrices. 


4 Matrices 

Wecan illustrate themultiplicationrule “cij equals ith rowof Atimes jth column 
of B” asfollows: 

. 
.. 
. 


b11 ··· 
b1j ··· 
b1s 

. 


. 


..


. 


. 


..


. 


. 


..


. 


. 


bm1 ··· 
bmj ··· 
bms 
. 
. 
a11 ··· 
a1m . 
. 


. 


. 


..
..


. 


.. 


. 


. 


v 


. 


.. 


.. 


.. 


.
[ 
ai1 ··· 
aim 
.. 
.. 
cij . 


.. 


..
..


. 


. 


.. 


. 


. 


an1 ··· 
anm 

Itis importantto notethatthematrixmultiplicationin generalisnot commutative. 

Example 4.2 For the matrices 
. 
. 
A= 
. 
123 . 
.Z2,3 B = 
. 
.11 
00 
. 
.Z3,2 

,

456 

1.1 

we have 
	. 
2.2 

2,2

A. 
B =.Z

2.2

. 


On the other hand, B . 
A . 
Z3,3.Although A. 
B and B. 
Aare both defined, we 
obviously have A. 
B 
= 
B. 
A.Inthis case one recognizes the non-commutativity 
of thematrixmultiplication fromthefact that A.B and B. 
Ahave different sizes. 
But evenif A. 
B and B . 
A are both defined and have the same size, in general 

A. 
B 
= 
B. 
A. For example, 
. 
. 
. 
. 
A= 
12 
03 .Z2,2 ,B = 
40 
56 .Z2,2 

yield the two products 
. 
. 
. 
. 
A. 
B = 
14 12 
15 18 
and B. 
A= 
48 
528 

. 


Thematrixmultiplicationis,however, associativeand distributivewith respectto 
the matrix addition. 


4.1 Basic Definitions andOperations 41 
Lemma 4.3 For A, 
B . 
Rm,. 
andC . 
R,k thefollowing assertions

A . 
Rn,m,B, 
hold: 

(1) A. 
(B. 
C) 
= 
(A. 
B) 
. 
C. 
(2) (A+ 
A) 
. 
B = 
A. 
B+ 
A. 
B. 
(3) A. 
(B+ 
B) 
= 
A. 
B+ 
A. 
B. 
(4) In . 
A= 
A. 
Im = 
A. 
, B . 
Rm,

Proof Weonlyshowproperty(1);the othersareexercises.LetA. 
Rn,m , 
C . 
R,k as well as (A. 
B) 
. 
C =[dij and A. 
(Bdij .Bythe definition

. 
C) 
=[ofthematrixmultiplicationandusingthe associativeand distributivelawin R,we 
get 
. 
m . 
m . 
m 

dij = 
aitbts csj = 
(aitbts ) 
csj = 
ait btscsj 
s=1 t=1 s=1 t=1 s=1 t=1 

m . 


= 
ait btscsj = 
dij, 
t=1 s=1 


for1. 
i . 
n and1. 
j. 
k,which impliesthat ( 
A. 
B) 
. 
C = 
A. 
(B. 
C). . 



Ontherighthand sidesof(2)and(3)inLemma4.3wehavenot written parentheses,
since we will usethe common conventionthat themultiplicationofmatrices 
binds stronger than the addition. 

For A. 
Rn,n we define 

Ak := 
A. 
... 
. 
A for k . 
N, 


k times 

A0 := 
In. 


Another multiplicative operationfor matricesisthemultiplicationwitha scalar,3 
whichis defined as follows: 

·: 
R. 
Rn,m . 
Rn,m,(., 
A) 

. 
. 
· 
A:= 
[.aij . 
(4.2) 

We easily seethat0· 
A = 
0n,m and1· 
A = 
A for all A . 
Rn,m.In addition, the 
scalar multiplication has thefollowing properties. 

Lemma 4.4 For A, 
B . 
Rn,m,C . 
Rm,. 
and ., 
. 
. 
R the following assertions 
hold: 

(1) (..) 
· 
A= 
. 
· 
(. 
· 
A). 
(2) (. 
+ 
.) 
· 
A= 
. 
· 
A+ 
. 
· 
A. 
3Theterm “scalar”was introducedin 1845by SirWilliamRowan Hamilton (1805–1865).Itoriginates 
fromthe Latinword “scale” whichmeans “ladder”. 


4 Matrices 

(3) . 
·(A+B) 
=. 
·A+. 
·B. 
(4) (. 
·A) 
.C =. 
·( 
A.C) 
=A.(. 
·C). 
Proof Exercise. . 



Thefourth matrix operationthat we introduceis the transposition: 

T : 
Rn,m >Rm,n 
, 
A=[aij >AT =[bij , 
bij :=aji. 


For example, 

.. 


14

123 

2,33,2

A=.Z, 
AT = 
.
25. 
.Z.

456 

36 

The matrix AT is calledthe transpose of A. 

Definition 4.5 If A . 
Rn,n satisfies A = 
AT, then A is called symmetric.If A = 
.AT,then Ais called skew-symmetric. 

Forthe transpositionwehavethe following properties. 

Lemma 4.6 For A, 
A.Rn,m,B .Rm,. 
and . 
.Rthe following assertions hold: 

(1) (AT)T =A. 
(2) (A+A
)T =AT +A

T. 
(3) (. 
·A)T =. 
·AT . 
(4) (A.B)T =BT .AT . 
Proof Properties (1)–(3) 
areexercises.For theproofof (4) 
let A.B =[cij with 

m 

cij = 
1aikbkj, AT =[aij , BT =[bij and (A.B)T =[cij .Then

k=

mmm 

cij =cji = 
ajkbki = 
akjbik = 
bik
akj, 


k=1 k=1 k=1 

fromwhich we seethat (A.B)T =BT .AT . . 



MATLAB-Minute. 

Carryoutthefollowing commandsinordertogetusedtothematrix operations 
of this chapter in MATLAB notation: A=ones(5,2), A+A, A-3.A, A’, A’.A, 
A.A’. 
(InordertoseeMATLAB’s output,donotputasemicolonattheendofthe 
command.) 



4.1 Basic Definitions andOperations 43 
Example4.7 Consideragaintheexampleof car insurance premiums from Chap. 1. 
Recallthat pij denotestheprobabilitythatacustomerinclass Ci inthis year will move 
to the class Cj.Ourexample consistsof four such classes,andthe16probabilities 
canbe associated witharow-stochastic4 . 
4matrix(cp.(1.2)), whichwe denoteby 

P. Supposethat theinsurance companyhas thefollowing distributionofcustomers 
in thefour classes:40%inclass C1,30%in class C2,20%in class C3, and10%in 
class C4.Then the1 . 
4matrix 
p0 := 
[0.4, 
0.3, 
0.2, 
0.1 

describes theinitial customerdistribution. Usingthe matrix multiplicationwenow 
compute 

.. 


0.150.850.000.00 
..

0.150.000.850.00 
..

p1 := 
p0 . 
P =[0.4, 
0.3, 
0.2, 
0.1 . 


..

0.050.100.000.85 
0.050.000.100.85 
=[0.12, 
0.36, 
0.265, 
0.255 . 


Then p1 containsthedistributionofthe customersinthenext year.Asanexample, 
consider the entry of p0 . 
P in position (1,4), which is computed by 

0.4· 
0.00 + 
0.3· 
0.00 + 
0.2· 
0.85 + 
0.1· 
0.85 = 
0.255. 


Acustomerinthe classesC1 or C2 in this year cannot move to the class C4.Thus, 
the respective initial percentages are multiplied by the probabilities p14 = 
0.00 
and p24 = 
0.00.A customerinthe class C3 or C4 will be in the class C4 with the 
probabilities p34 = 
0.85 or p44 = 
0.85, respectively. This yields the two products 
0.2· 
0.85 and0.1· 
0.85. 

Continuing in the same way we obtain after k yearsthe distribution 

pk := 
p0.Pk , 
k = 
0,1,2,.... 


(Thisformula also holds for k = 
0, since P0 = 
I4.) Theinsurance company can 
usethisformulato computetherevenuefromthe paymentsofpremium ratesinthe 
coming years. Assume that thefullpremium rate (class C1)is 500 Euros per year. 
Then the rates in classes C2, C3, and C4 are 450, 400 and 300 Euros(10,20 and 
40%discount).Ifthereare1000 customers initially,thentherevenueinthefirstyear 
(inEuros)is 

1000 · 
p0 .[500, 
450, 
400, 
300 T= 
445000. 



4 Matrices 

If no customer cancels the contract, then this model yields the revenue in year 
k .0as 

1000 · 
pk .[500, 
450, 
400, 
300 T = 
1000 · 
p0 .(Pk .[500, 
450, 
400, 
300 T). 


Forexample,therevenueinthenext4yearsis 404500, 372025, 347340and 341819 
(roundedtofull Euros).These numbers decrease annually,buttherateofthe decrease 
seems to slow down. Does there exists a “stationary state”, i.e., a state when the 
revenue is not changing (significantly) any more? Which properties of the model 
guaranteetheexistenceofsuchastate?Theseare important practical questionsfor 
theinsurance company.Onlytheexistenceofastationarystate guarantees significant 
revenuesinthelong-timefuture. Sincetheformula depends essentiallyonthe entries 
of the matrix Pk,wehavereached an interestingproblemof Linear Algebra: the 
analysisofthepropertiesofrow-stochastic matrices.Wewill analyze theseproperties 
in Sect.8.3. 

4.2 MatrixGroups andRings 
In this sectionwestudy algebraicstructures that areformedbycertain setsof matrices 
and thematrix operations introduced above.Webegin with the additionin Rn,m . 

Theorem 4.8 (Rn,m , 
+) 
isa commutativegroup. The neutralelementis 0 . 
Rn,m 
(the zero matrix) and for A =[aij . 
Rn,m theinverse element is .A :=[.aij . 
Rn,m.(We writeA.Binstead of A+(.B).) 

Proof Usingthe associativityof the additionin R,for arbitrary A, 
B, 
C .Rn,m,we 
obtain 

(A+B) 
+C =[aij +bij ]+[cij ]=[(aij +bij) 
+cij ]=[aij +(bij +cij) 


=[aij ]+[bij +cij ]=A+(B+C). 


Thus,the additionin Rn,m is associative. 

The zeromatrix0.Rn,m satisfies0+A=[0 ]+[aij ]=[0+aij ]=[aij ]=A. 
For a given A =[aij . 
Rn,m and .A := 
[.aij . 
Rn,m we have .A+ 
A = 
[.aij ]+[aij ]=[.aij +aij ]=[0 ]=0. 

Finally,the commutativityofthe additionin Rimpliesthat A+B =[aij ]+[bij = 
[aij +bij ]=[bij +aij ]=B+A. . 



Note that (2) 
inLemma4.6 impliesthatthetranspositionisahomomorphism(even 
an isomorphism) between thegroups (Rn,m , 
+) 
and (Rm,n , 
+) 
(cp. Definition 3.6). 


4.2 MatrixGroups andRings 45 
Theorem 4.9 (Rn,n , 
+, 
.) 
isaringwith unitgivenby theidentity matrixIn. This 
ringis commutative onlyforn = 
1. 

Proof We have already shown that (Rn,n , 
+) 
is a commutative group (cp. Theorem4.8).
Theother propertiesofaring (associativity,distributivityandtheexistence 
of a unit element) follow from Lemma4.3. The commutativity for n = 
1 holds 
becauseofthe commutativityof themultiplicationinthe ring R.The example 

0110 00 01 1001

.= 

= 
=. 


0000 00 00 0000 

showsthat thering Rn,n is not commutative for n . 
2. . 



Theexampleintheproofof Theorem 4.9showsthatfor n . 
2the ring Rn,n has 
non-trivial zero-divisors, i.e., thereexist matrices A, 
B . 
Rn,n \{0}with A.B = 
0. 
These exist even when Ris a field. 

Letusnowconsiderthe invertibility of matricesinthering Rn,n (with respect to the 
matrix multiplication).Foragiven matrix A. 
Rn,n,aninverse A. 
Rn,n must satisfy 

thetwo equations A. 
A= 
In and A. 
A= 
In (cp. Definition 3.10). If an inverse of 
A. 
Rn,n exists, i.e., if Ais invertible,thentheinverseis uniqueand denotedby A.1 
(cp. Theorem 3.11). An invertible matrix is sometimes called non-singular, while 
a non-invertible matrix is called singular.We will show in Corollary7.20 that the 
existence of theinverse already is impliedbyone of thetwo equations A. 
A = 
In 

and A. 
A = 
In, i.e., if one of them holds,then Ais invertible and A.1 A.Until 

then, to be correct, we will have to check the validity of both equations. 
Not all matrices A . 
Rn,n areinvertible.Simpleexamples arethe non-invertible 
matrices 
	. 
10 

. 
R2,2

A=[0 . 
R1,1 and A= 
.

00 

Another non-invertible matrix is 

11 

2,2

A=. 
Z.

02 

However, considered as an element of Q2,2,the (unique)inverseof Ais givenby 

1.1 

A.1 22,2

=. 
Q.

1

0 

2 

Lemma 4.10 If A, 
B . 
Rn,n areinvertible,then thefollowing assertions hold: 

(1) AT is invertible with (AT).1 = 
(A.1)T.(We also write this matrix asA.T.) 
(2) A. 
Bisinvertible with(A. 
B).1 = 
B.1 . 
A.1 . 

4 Matrices 

Proof 

(1) Using (4)in Lemma4.6 we have 
T 
TT

(A.1)T .AT =(A.A.1)=IT =In =IT =(A.1 .A)=AT .(A.1),

nn 

and thus (A.1)T is theinverseof AT . 

(2)Thiswas alreadyshowninTheorem3.11for generalringswithunitandthusit 
holds,in particular,for thering (Rn,n ,+,.). . 


Ournextresultshows that theinvertible matrices formamultiplicativegroup. 

Theorem 4.11 The setofinvertiblen.nmatricesoverRformsagroup withrespect 
to thematrixmultiplication.We denotethisgroupbyGLn(R) 
(“GL” abbreviates 
“general linear (group)”). 

Proof Theassociativityof themultiplicationin GLn(R) 
is clear.Asshownin (2) 
in Lemma4.10,the productoftwoinvertible matricesisaninvertible matrix.The 
neutral element in GLn(R) 
is theidentity matrix In, and since every A .GLn(R) 
is assumedtobeinvertible, A.1 exists with (A.1).1 =A.GLn(R). . 



We nowintroduce some important classesofmatrices. 

].Rn,n

Definition 4.12 Let A=[aij . 

(1) 
Ais called upper triangular,ifaij =0for alli > 
j. 
Ais called lowertriangular,ifaij =0for all j>i(i.e., AT is upper triangular). 
(2) 
Ais called diagonal,ifaij =0for alli 
= 
j(i.e., Ais upperandlowertriangular). 
We writeadiagonal matrix as A=diag(a11,...,ann ). 
Wenextinvestigatethesesetsofmatrices with respectto theirgroup properties, 
beginning with theinvertible upper andlowertriangular matrices. 

Theorem 4.13 The setsoftheinvertible upper triangularn .n matrices and of the 
invertible lowertriangularn .n matrices overRform subgroups ofGLn(R). 

Proof Wewillonlyshowtheresultfortheupper triangular matrices;theproofforthe 
lowertriangular matricesis analogous.Inorderto establishthe subgroup property 
we will prove thethree propertiesfromTheorem 3.5. 

Since In is an invertible upper triangular matrix, the set of the invertible upper 
triangular matrices is a nonempty subset of GLn(R). 

Next we show that fortwo invertible upper triangular matrices A,B . 
Rn,n the 
product C = 
A.B isagainaninvertible upper triangular matrix.Theinvertibility 
of C =[cij follows from (2)in Lemma4.10.For i > 
jwe have 


4.2 MatrixGroups andRings 47 
n 

cij = 
aikbkj (here bkj = 
0for k> 
j) 
k=1 

j 

= 
aikbkj (here aik =0for k=1,..., 
j, since i> 
j) 
k=1 

=0. 


Therefore, Cis upper triangular. 

It remainstoprove that theinverse A.1 of an invertible upper triangular matrix A 
is an upper triangular matrix.For n = 
1the assertion holds trivially, so we assume 
that n . 
2.Let A.1 =[cij , then the equation A. 
A.1 = 
In can be written as a 
system of nequations 

. 
a11 ··· 
··· 
a1n . 
. 
c1j . 
. 
.1j . 
. 
. 
. 
. 
. 
0 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
= 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
, 
j=1,...,n. 
(4.3) 
0 ··· 
0 ann cnj .nj 

Here, .ij is theKronecker delta-function definedin(4.1). 
We will nowprove inductivelyfori=n,n.1,...,1that thediagonal entryaii 

.1

of Ais invertible with aii =cii, and that 

n 

.1

cij = 
aii .ij . 
aicj , 
j=1,...,n. 
(4.4) 
=i+1 

This formulaimplies,in particular,that cij =0for i> 
j. 
For i=nthelastrowof(4.3)isgivenby 

anncnj =.nj, 
j=1,...,n. 


For j= 
n we have anncnn = 
1 = 
cnnann,whereinthe second equationweuse the 
commutativityof themultiplicationin R.Therefore, ann is invertible with a.1 =

nn cnn, 

and thus 

.1

cnj =ann .nj, 
j=1,...,n. 


Thisis equivalentto(4.4)for i= 
n.(Note that for i= 
n in(4.4)thesumisempty 
and thus equal to zero.)In particular, cnj =0for j=1,...,n.1. 

Nowassume that our assertion holds for i=n,...,k+1, where1 .k. 
n.1. 
Then, in particular, cij = 
0for k+ 
1 . 
i . 
n and i > 
j. In words, the rows 
i = 
n,...,k+ 
1of A.1 are in “upper triangular from”. In order to prove the 
assertionfor i=k, we consider the kth rowin(4.3), whichisgivenby 


4 Matrices 

akkckj +ak,k+1ck+1,j +...+akncnj = 
.kj, 
j=1,...,n. 
(4.5) 

For j=k(< 
n)we obtain 

akkckk +ak,k+1ck+1,k +...+akncnk =1. 


Bytheinductionhypothesis,wehaveck+1,k =···= 
cn,k =0. This impliesakkckk = 
1=ckkakk,wherewehaveusedthe commutativityofthemultiplicationin R.Hence 

.1

akk is invertible with akk =ckk .From(4.5)we get 

.1

ckj =akk .kj .ak,k+1ck+1,j .....akncnj , 
j=1,...,n, 


and hence(4.4)holds fori = 
k.Ifk > 
j,then .kj = 
0andck+1,j = 
··· 
= 
cnj =0, 
whichgives ckj =0. . 



Wepoint out that(4.4)representsarecursive formulafor computingthe entriesof 
theinverseofaninvertibleupper triangular matrix.Usingthis formulathe entriesare 
computed “from bottomtotop”and “fromrightto left”.This processissometimes 
called backwardsubstitution. 

In thefollowing we will frequently partitionmatrices into blocks and make use 
of the blockmultiplication:For everyk .{1,...,n .1}, we can write A. 
Rn,n as 

A11 

A12

A= 


with A11 . 
Rk,k and A22 . 
Rn.k,n.k .

A21 

A22 

If A,B . 
Rn,n are both partitioned likethis, then theproduct A.Bcan beevaluated 
blockwise, i.e., 

A11 

A12 B11 

B12 A11 . 
B11 + 
A12 . 
B21 

A11 . 
B12 + 
A12 . 
B22

.= 
.
A21 

A22 B21 

B22 A21 . 
B11 + 
A22 . 
B21 

A21 . 
B12 + 
A22 . 
B22 

(4.6) 
In particular,if 

A11 

A12

A= 


0 

A22 

with A11 . 
GLk(R)and A22 . 
GLn.k(R),then A . 
GLn(R)andadirect computation 
shows that 

A.1 

.A.1 

. 
A12 . 
A.1 
A.1 11 

11 22

= 
. 
(4.7)

A.1

0 

22 


4.2 MatrixGroups andRings 
49 
MATLAB-Minute. 

Createblock matricesinMATLABbycarrying out thefollowing commands: 

k=5; 
A11=gallery(’tridiag’,-ones(k-1,1),2.ones(k,1),-ones(k-1,1)); 
A12=zeros(k,2); A12(1,1)=1; A12(2,2)=1; 
A22=-eye(2); 
A=full([A11 A12; A12’ A22]) 
B=full([A11 A12; zeros(2,k) -A22]) 
Investigate the meaning of the command full. Compute the products A.B 
and B.Aas well as theinverses inv(A) and inv(B).Computethe inverse of 
BinMATLABwith theformula(4.7). 


Corollary 4.14 The set of the invertible diagonal n . 
n matrices over R forms a 
commutativesubgroup (withrespecttothematrixmultiplication)oftheinvertible 
upper (orlower)triangularn . 
n matrices overR. 

Proof Since In isaninvertible diagonal matrix,theinvertible diagonal n.n matrices 
forma nonemptysubsetoftheinvertible upper(orlower)triangular n . 
n matrices. 
If A = 
diag(a11,...,ann )and B = 
diag(b11,...,bnn )areinvertible,then A. 
B is 
invertible(cp.(2)inLemma4.10)and diagonal,since 

A. 
B = 
diag(a11,...,ann ). 
diag(b11,...,bnn ) 
= 
diag(a11b11,...,annbnn ). 


Moreover, if A = 
diag(a11,...,ann ) 
is invertible, then aii . 
R is invertible for 
all i = 
1,...,n (cp. theproofof Theorem 4.13). Theinverse A.1 isgivenby the 

.1 .1

invertible diagonal matrix diag(a11 ,...,a).Finally, the commutativity property

nn 

A. 
B = 
B. 
Afollows directly fromthe commutativityin R. 
. 



Definition 4.15 Amatrix P . 
Rn,n is called a permutation matrix,ifinevery row 
and every column of P thereisexactly one unit and allother entries are zero. 

The term “permutation” means “exchange”. If a matrix A . 
Rn,n is multiplied 
witha permutationmatrixfromthe left or fromthe right,then its rows or columns, 
respectively, areexchanged(or permuted).Forexample,if 

. 
. 
.. 


001 123 

3,3 

. 
...

P = 
010 , 
A= 
456 . 
Z, 
100 789 


50 4 Matrices 
then 
P.A= 
. 
. 
789 
456 
123 
. 
. 
and A.P = 
. 
. 
32 1 
654 
987 
. 
. 
. 


Theorem 4.16 The setofthen .n permutationmatricesoverRformsasubgroup 
ofGLn(R).In particular,ifP . 
Rn,n isa permutationmatrix, thenPisinvertible 
withP.1 =PT . 

Proof Exercise. . 



From nowonwewill omit themultiplication signinthe matrix multiplication 
and write AB instead of A.B. 

Exercises 

(In thefollowingexercises Ris a commutative ring with unit.) 

4.1Consider thefollowing matricesover Z: 
.. 


24

1.24 .10 

..

A= 
, 
B = 
36 , 
C = 
.

.23 .5 11

1.2 

Determine, if possible, the matrices CA, BC, BTA, ATC, (.A)TC, BTAT , 
AC and CB. 

4.2 Consider the matrices 
.. 


x1 

. 
. 
.

A= 
aij .Rn,m , 
x = 
. 
. 
. 
.Rn,1 , 
y =[y1,...,ym ].R1,m . 


. 


xn 

Whichofthe followingexpressions arewell defined for m =n or m =n? 

T

(a) xy, (b)xy, (c)yx,(d)yxT,(e)xAy, (f) xT Ay, 
(g) xAyT,(h)xT AyT,(i)xyA,(j)xyAT,(k)Axy,(l)ATxy. 
4.3 Showthe following computational rules: 
.1

.1x1 +.2x2 =[x1,x2 and A[x1,x2 ]=[Ax1,Ax2

.2 

for A.Rn,m , x1,x2 .Rm,1 and .1,.2 .R. 


4.2 MatrixGroups andRings 
51 
4.4ProveLemma4.3 (2)–(4). 
4.5 ProveLemma4.4. 
4.6ProveLemma4.6 (1)–(3). 
.. 


011 

..

4.7 Let A= 
001 .Z3,3.Determine An for all n .N.{0}. 
000 

4.8 Let p =.ntn +...+.1t +.0t0 . 
R[t bea polynomial(cp. Example3.17) 
and A. 
Rm,m.Wedefine p(A) 
. 
Rm,m as p(A) 
:=.nAn +...+.1A+.0Im. 
10 

.Z2,2

(a) Determine p(A)for p =t2 .2t +1.Z[t and A= 
.
31 

(b) For a fixed matrix A . 
Rm,m consider the map fA : 
R[t . 
Rm,m , p 
. 
p(A). Showthat fA(p+q) 
= 
fA(p)+ 
fA(q)and fA(pq) 
= 
fA(p)fA(q) 
for all p,q . 
R[t . 
(The map fA is a ring homomorphism between the rings R[t and Rm,m.) 
(c) Showthat fA(R[t ) 
={p(A)|p . 
R[t ]}isacommutativesubringof Rm,m , 
i.e., that fA(R[t ) 
is a subring of Rm,m (cp. Exercise 3.14) and that the 
multiplicationinthissubringis commutative. 
(d)Isthe map fA surjective? 
4.9 Let K bea fieldwith1 +1 
= 
0. Show that every matrix A . 
Kn,n can be 
written as A = 
M +S with a symmetric matrix M . 
Kn,n (i.e., MT = 
M) 
and a skew-symmetric matrix S . 
Kn,n (i.e., ST =.S). 
Does this also holdinafieldwith1 +1=0?Giveaprooforacounterexample. 
4.10 Show the binomial formula for commuting matrices: If A,B . 
Rn,n with 
k 

k 
kk!

AB = 
BA,then (A+B)k = 
AjBk.j, where :=

j=0 
j jj!(k.j)!
. 

4.11 Let A . 
Rn,n be a matrix for which In . 
A is invertible. Show that (In . 
m

A).1(In .Am+1) 
= 
j=0 Aj holds for every m .N. 

4.12 Let 
A . 
Rn,n beamatrixfor whichan m .N with Am = 
In exists and let m 
be smallest natural number with this property. 
(a) Investigate whether A is invertible, and if so, give a particularly simple 
representation ofthe inverse. 
(b) Determine the cardinality of the set {Ak |k .N}. 
. 
Rn,n

4.13 Let A =[aij . 
anj =0for j=1,...,n . 
(a) Showthat Ais a subring of Rn,n . 
(b) Showthat AM .Afor all M . 
Rn,n and A.A. 
(A subring with this property is called a left ideal of Rn,n.) 
(c) Determine an analogous subring B of Rn,n, such that MB . 
B for all 
M . 
Rn,n and B .B. 
(A subring with this property is called a left ideal of Rn,n.) 
4.14 Examine whether (G,.)with 

4 Matrices 

cos(.) 
.sin(.)

G = 
. 
.R

sin(.) 
cos(.) 


is a subgroup of GL2(R). 

4.15 Generalizethe block multiplication(4.6)tomatricesA. 
Rn,m and B . 
Rm,. 
4.16 Determine all invertible upper triangular matrices A. 
Rn,n with A.1 = 
AT . 
4.17 Let A11 . 
Rn1,n1, A12 . 
Rn1,n2, A21 . 
Rn2,n1, A22 . 
Rn2,n2 and 
A11 A12 . 
Rn1+n2,n1+n2

A= 
.

A21 A22 

(a) Let 
A11 . 
GLn1(R). Show that A is invertible if and only if A22 . 
A21 A.1.
11 A12isinvertibleandderiveinthis casea formulafor A.1 

(b) Let 
A22 . 
GLn2(R). Show that A is invertible if and only if A11 . 
A12 A.1 
22 A21isinvertibleandderiveinthis casea formulafor A.1. 

4.18 Let A. 
GLn(R), U . 
Rn,m and V . 
Rm,n. Showthe following assertions: 
(a) 
A+UV . 
GLn(R)holdsif and onlyif Im +VA.1U . 
GLm(R). 
(b) If Im +VA.1U . 
GLm(R),then 
.1

(A+UV)= 
A.1 . 
A.1U(Im +VA.1U).1VA.1 . 


(This last equation is called the Sherman-Morrison-Woodbury formula; 
named afterJack Sherman,WinifredJ. Morrison and MaxA.Woodbury.) 

4.19 Show that the set of block upper triangular matrices with invertible 2 . 
2 
diagonal blocks, i.e., the set of matrices 
.. 


A11 A12 ··· 
A1m 

..

0 A22 ··· 
A2m 

. 
. 
. 
. 
. 
. 
. 
.
, 
Aii . 
GL2(R), 
i =1,...,m, 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
0 ··· 
0 Amm 

isagroupwith respecttothematrixmultiplication. 

4.20 Prove Theorem 4.16.Isthe groupof permutationmatrices commutative? 
4.21 Showthat thefollowingis an equivalence relationon Rn,n: 
A. 
B . 
There exists a permutation matrix P with A= 
PT BP. 

4.22Acompanyproducesfromfourrawmaterials R1, R2, R3, R4 fiveintermediate 
products Z1, Z2, Z3, Z4, Z5,andfromthesethreefinal products E1, E2, E3.The 
following tablesshowhowmanyunitsof Ri and Zj arerequiredfor producing 
one unit of Zk and E, respectively: 

4.2 MatrixGroups andRings 53 
R1 
R2 
R3 
R4 

Z1 Z2 Z3 Z4 Z5 

Z1

01112 

Z2

50 121 

Z3

11110 

Z4

02010 

Z5 

E1 E2 E3 

111 
120 
011 
411 
311 

Forinstance,five unitsofR2 and one unitof R3 arerequiredfor producing one 
unit of Z1. 

(a)Determine,withthehelpofmatrix operations,acorrespondingtablewhich 
showshow many units of Ri arerequiredfor producing one unitof E. 
(b)Determinehowmanyunitsof thefour rawmaterials arerequiredfor producing 
100 units of E1, 200 units of E2 and 300 units of E3. 

Chapter5 
The EchelonFormand the Rankof Matrices 

In this chapter we develop a systematic method for transforming a matrix A with 
entries froma fieldintoaspecial form whichis calledthe echelon formof A.The 
transformation consistsofasequenceof multiplicationsof Afrom theleftbycertain 
“elementarymatrices”. If Aisinvertible,thenits echelonformistheidentity matrix, 
and theinverse A.1istheproductoftheinversesofthe elementarymatrices.Fora 
non-invertible matrixits echelonformis,insomesense,the “closest possible” matrix 
totheidentity matrix.Thisformmotivatesthe conceptoftherankofamatrix, which 
we introducein this chapter and will usefrequently later on. 

5.1 Elementary Matrices 
Let R be a commutative ring with unit, n .N and ij .{1 n}.Let In . 
Rn n 
be theidentity matrix and let ei be its ith column, i.e., In =[e1 en]. 

We define 

Eij :=eieTj =[00 ei 00].Rn n 

column j 

i.e., the entry (ij of Eij is1, allother entries are0. 
For n .2andi < 
jwe define 

Pij :=[e1 ei.1 ej ei+1 ej.1 ei ej+1 en].Rn n (5.1) 

Thus, Pij isa permutationmatrix(cp. Definition 4.12)obtainedbyexchanging the 
columns i and jof In.Amultiplication ofA.Rn m from theleftwith Pij means an 
exchange ofthe rows i and jof A.For example, 

©SpringerInternationalPublishing Switzerland 2015 55 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_5 

5 The EchelonFormand theRankofMatrices 

.. 
.... 


123 
001 789 

.. 
....

A= 
456 P13 =[e3 e2 e1]= 
010 P13 A= 
456 
789 100 123 

For . 
.Rwe define 

Mi(. 
:=[e1 ei.1 .ei ei+1 en].Rn n (5.2) 

Thus, Mi(. 
isadiagonal matrix obtainedby replacing the ith column of In by .ei. 
Amultiplicationof A.Rn m from theleftwith Mi(. 
meansamultiplicationofthe 
ith row of Aby ..For example, 

. 
. 
.... 


123 100 123 
A= 
456 M2(.1 =[e1 .e2 e3]= 
0 .10 M2(.1 A=.4 .5 .6

. 
. 
.... 


789 
001 789 

For n .2, i < 
jand . 
.Rwe define 

Gij(. 
:=In +.Eji =[e1 ei.1 ei +.ej ei+1 en].Rn n (5.3) 

Thus,thelowertriangular matrix Gij(. 
is obtained by replacing the ith column of 
In by ei +.ej.Amultiplication of A . 
Rn m fromthe left with Gij(. 
means that 
. 
times the ith row of Ais added to the jth row of A.Similarly, a multiplication of 
A . 
Rn m from the left by the upper triangular matrix Gij(. 
T means that . 
times 
the jth row of Ais added to the ith row of A.For example, 

. 
. 
.. 


123 
100 

. 
. 
..

A= 
456 G23(.1 =[e1 e2 .e3 e3]= 
010 
789 0.11 

.. 
.. 


123 123 
G23(.1 A= 
.
456. 
G23(.1 TA= 
. 
.3.3.3


. 


333 
789 

Lemma 5.1 The elementarymatricesPij,Mi(. 
forinvertible . 
. 
R, andGij(. 
definedin(5.1),(5.2),and(5.3),respectively,areinvertibleand havethefollowing 
inverses: 

P.1 PT

(1) =ij =Pij.
ij 

(2) Mi(. 
.1 =Mi(..1 . 
(3) Gij(. 
.1 =Gij(.. 
. 
Proof 

(1) The invertibility of Pij with P.1 = 
PijT wasalready showninTheorem 4.16;
ij 
the symmetry of Pij is easily seen. 



5.1 ElementaryMatrices 57 
(2) Since . 
. 
Risinvertible,the matrix Mi(..1 is well defined.Astraightforward 
computationnow showsthat Mi(..1 Mi(. 
= 
Mi(. 
Mi(..1 = 
In. 
T TT

(3) Since ejei =0for i < 
j,wehave E2 
ji =(eiej )(eiej =0, and therefore 
Gij(. 
Gij(.. 
=(In +.Eji )(In +(.. 
Eji 

= 
In +.Eji +(.. 
Eji +(..2 E2 
ji = 
In 

Asimilar computation shows that Gij(.. 
Gij(. 
= 
In. 

5.2 TheEchelonForm andGaussian Elimination 
The constructiveproofof thefollowing theorem reliesonthe Gaussian elimination 
algorithm.1Foragivenmatrix A. 
Kn m,where Kisafield, this algorithm constructs 
amatrix S . 
GLn(K such that SA = 
C is quasi-upper triangular.We obtainthis 
special form by left-multiplication of A with elementary matrices Pij, Mij(. 
and 
Gij(. 
.Eachof theseleft-multiplications correspondstothe applicationof oneof 
theso-called “elementaryrow operations”tothe matrix A: 

• 
Pij:exchange two rows of A. 
• 
Mi(. 
:multiply a row of Awith an invertible scalar. 
• 
Gij(. 
:add a multiple of one row of Ato another row of A. 
Weassumethatthe entriesofAareinafield (rather thanaring) becauseintheproof 
ofthetheoremwerequirethat nonzero entriesof Aareinvertible.Ageneralizationof 
theresultwhich holdsover certain rings (e.g.the integers Z)isgivenbythe Hermite 
normalform,2 whichplays an important rolein Number Theory. 

Theorem 5.2 LetKbeafield and letA. 
Knm.Thenthereexistinvertible matrices 
S1 St . 
Kn n (these are products of elementary matrices) such that C := 
St ···S1Aisin echelon form, i.e., eitherC =0or 

1Namedafter Carl FriedrichGau.(1777–1855).Asimilarmethodwasalreadydescribedin Chap. 8, 
“Rectangular Arrays”, of the “Nine Chapters on the Mathematical Art”. This text developed in 
ancientChinaoverseveral decadesBC stated problemsofeveryday lifeandgave practical mathematical 
solutionmethods.Adetailed commentaryand analysiswaswrittenby LiuHui (approx. 
220–280 AD) around 260 AD. 

2CharlesHermite (1822–1901). 


5 The EchelonFormand theRankofMatrices 

C = 


. 
. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


1 . 
0 
1 
. 
0 
0 
1 
. 
0 
0 
0 

. 


0 

. 


. 


. 


. 


. 


.

0 

.

0 

. 


.

1 

. 


. 


0 

Here . 
denotes an arbitrary (zero or nonzero) entryofC. 
More precisely, C =[cij] 
is either the zero matrix, or there exists a sequence of 
natural numbersj1 jr (theseare calledthe “steps”ofthe echelon form), where 
1. 
j1 < 
···< 
jr .m and 1.r .min{nm}, suchthat 

(1) cij =0for 1.i .r and 1. 
j< 
ji, 
(2) cij =0for r < 
i .n and 1. 
j.m, 
(3) ci ji =1for 1.i .r and allother entriesin columnji arezero. 
If n = 
m, then A . 
Kn n is invertible if and onlyifC = 
In.Inthis case A.1 = 
St ···S1. 

Proof If A=0, then we sett =1, S1 = 
In, C =0and we are done. 
Nowlet A=0and let j1be theindexofthe first columnof 

(1

A(1 

= 
a:= 
A

ij 

(1

that does not consistof all zeros. Let abe the first entry in this column that is 

i1 j1 

nonzero, i.e., A(1 has theform 

. 
. 
. 
0 . 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
0 
. 
. 
A(1 = 
. 
. 
. 
0 . 
. 
. 
a(1 
i1 j1 . 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
j1 

We then proceed as follows:First wepermute therows i1 and1(if i1 > 
1).Then we 

.1 

(1

normalize thenew firstrow, i.e.,we multiplyit with a.Finally we eliminate

i1 j1 

the nonzeroelements below thefirst entryin column j1. Permuting and normalizing 
leads to 


5.2 The EchelonFormand Gaussian Elimination 59 
.. 


. 
1 
. 


. 
(1 

. 
.a
. 


(1 (1 .1 
.. 
2 j1 
.

A

(1 = 
a.:= 
M1 aA(1 = 
. 
0 
. 
. 
.

ij i1 j1 
P1 i1 

.. 


. 
(1 
. 


a

nj1 

j1 

If i1 =1, then we set P11 := 
In.Inorderto eliminatebelowthe1in column j1,we 
multiply A(1 fromthe left with thematrices 

(1G1 n .an j1 

Then we have 

. 


0 

. 


.

S1A(1 = 
. 


. 
0 

where 

1 
0 

0 

j1 

n nj12 j1 i1 j1 

(2

and A(2 =[aij ]with i=2 n, j= 
j1 +1 m, i.e., wekeep theindicesof 
thelarger matrix A(1 in the smaller matrix A(2. 

If A(2 =[] 
or A(2 = 
0, then we are finished, since then C := 
S1A(1 is in 
echelon form.Inthis case r =1. 

If at least one of the entries of A(2 is nonzero, then we applythe steps described 
above to the matrix A(2 .For k= 
2 3 we define the matrices Sk recursively as 

(1 (1

S1 := 
G1 ..a···G12 ..a

(1G12 .a2 j1 

. 


. 


. 


.

A(2 

. 


. 
.1 

M1 a(1 P1 i1 

. 
0 
Sk = 
. 
Ik.1 0 
0 
Sk 
. 
where 
SkA(k = 
. 
. 
. 
. 
0 

1 
0 

0 

jk 

. 


. 


. 


.

A(k+1 

. 


Each matrix S.k is constructed analogous to S1:First we identifythe first column jk 
of A(k that is not completely zero, as well as the first nonzero entry a(k in that 

ik jk 

column. Then permuting and normalizingyieldsthe matrix 

.1 

(k (k 


A(k

A(k =[aij ]:= 
Mk aikjk 
Pkik 


5 The EchelonFormand theRankofMatrices 

If k=ik,then we set Pkk := 
In.k+1.Now 

. 
.1 

(k (k (k

Sk = 
Gk.a
···Gkk+1 .aMka

n njk k+1 jk ikjk 
Pkik 

so that Sk is indeedaproductof elementarymatricesof theform 

Ik.10 
0 T 

where Tis an elementary matrix of size (n.k+1 .(n.k+1. 

If we continue this procedureinductively, it will end after r . 
min{nm} 
steps 
with either A(r+1 =0or A(r+1 =[]. 

After rsteps we have 

Sr ···S1A(1 = 


. 
. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


1 . 
. 
1 
. 
. 
. 
1 
. 
0 
0 
0 



0 

1 
0 

. 
. 


. 


. 


. 


. 


. 


(5.4) 
. 


. 


. 


. 


. 


By construction, the entries1in(5.4)arein the positions 

(1 j1 (2 j2 (r jr 

If r = 
1, then S1A(1 is in echelon form (see thediscussion at thebeginning of 
theproof). If r > 
1, then we still have to eliminatethe nonzero entries above the1 

(1

in columns j2 jr.Todothis, we denotethe matrixin(5.4)by R(1 =[rij ]and 
form for k=2 rrecursively 

(kr+k.1R(k.1

R(k =[rij ]:= 
S

where 

TT 

(k.1 (k.1

Sr+k.1 := 
G1 k .r···Gk.1 k .r

1 jk k.1 jk 

For t:= 
2r.1wehave C:= 
StSt.1 ···S1Ain echelon form. 

Suppose now that n=mand that C= 
StSt.1 ···S1Aisin echelon form.If Ais 
invertible,then Cisaproductofinvertible matricesandthusinvertible.Aninvertible 
matrix cannot have a row containing only zeros, so that r = 
nand hence C= 
In. 
If, on the other hand, C = 
In, then the invertibility of the elementary matrices 


5.2 The EchelonFormand Gaussian Elimination 61 
impliesthat S. 
1 ··· 
S. 
1 = 
A.Asaproductofinvertible matrices, Ais invertible and
1 t 

A. 
1 

= 
St ··· 
S1. . 

Inthe literature,the echelonformis sometimes called reducedrow echelon form. 
Example5.3 Transformation ofa matrix from Q35 to echelon form vialeftmulti


plication with elementary matrices: 

.. 


02133 

..

02011 

02011 

. 


j1 = 
2 i1 = 
10 
.> 
. 
0 

1

M12 0 

. 


0 

1 

.> 


..

133

10

222 

.> 


..

2011 0

G13(. 
2

2011 0 

.

133 

222 j2 = 
3 i2 = 
2 

. 


. 


.>

0 

0 

. 
1. 
2 . 
2

G12(. 
2 

0 

0 

. 
1. 
2 . 
2 M2(. 
1 

.

133

1 

222 

.

2011 
0 . 
1. 
2. 
2 

. 


0 

. 


0 
0 

1 
0 
0 

.

133 
222 

.

122 
. 
1. 
2. 
2 

. 


1 1 
2 
1 
3 3 
2 2 
220 
0 0 00 

.. 


1 0 
1 
1 1 
2 2 
220 
0 0 00 

.

0 

0 

.> 
. 


. 
.> 
. 


. 



0

. 
0 

. 
T 
. 


.

. 
1

G23(1 

G12 2 

0

0 

MATLAB-Minute. 

The echelon form is computed in MATLAB with the command 
rref (“reduced row echelon form”). Apply rref to [A eye(n+1)] in 
order to compute the inverse of the matrix A=full(gallery(’tridiag’, 
-ones(n,1),2. 
ones(n+1,1),-ones(n,1))) for n=1,2,3,4,5 (cp. Exercise 
5.5). 
Formulate a conjecture about the general form of A. 
1.(Can you prove your 
conjecture?) 

Theproofof Theorem 5.2 leadstotheso-called LU-decompositionof a square 
matrix. 

Theorem 5.4 For every matrix A . 
Kn n, there exists a permutation matrix P . 
Kn n, a lower triangular matrix L . 
GLn(K with ones on the diagonal and an 
upper triangular matrixU . 
Kn n,suchthatA = 
PLU. The matrixUisinvertible 
if and onlyifAisinvertible. 


5 The EchelonFormand theRankofMatrices 

Proof For A . 
Kn n theEq.(5.4)has theformSn ···S1A = 
U, where U is upper 
triangular.If r <n,then we set Sn = 
Sn.1 = 
··· 
= 
Sr+1 = 
In.Since the matrices 
S1 Sn areinvertible,itfollows that U isinvertibleif and onlyif Ais invertible. 

For i =1 n every matrix Si has theform 

.. 


1 

.. 


.. 


.. 


..

1 

.. 


..

Si = 
sii Piji 

.. 


..

si+1 i 1 

.. 


.. 


.. 


sn i 1 

where ji . 
i for i = 
1 n and Pi i := 
In (if ji = 
i, then no permutation was 
necessary). Therefore, 

... 
. 


11 

... 
. 


... 
. 


... 
. 


... 
.

Sn ···S1 = 


. 
1 
.. 
1 
. 
Pn.1 jn.1 

... 
. 


. 
1 
.. 
.

sn.1 n.1 
snn snn.1 


1 

.. 


. 
.. 
.

1 

1 

.. 
s11 

. 
. 
.... 


. 
.. 
s22 
.. 
s21 1 
. 


. 
. 
.... 


. 
1 
.. 
s32 
.. 
s31 
. 


··· 


.. 
Pn.2 jn.2 
. 
1 
. 
P2 j2 
. 
1 
. 
P1 j1 

. 
. 
....

sn.2 n.2 

.. 


. 
.. 
. 


..

sn.1 n.21 

sn 21 sn 11 
sn n.2 

01 

Theformof the permutationmatrices for k = 
2 n .1and= 
1 k .1 
impliesthat 

. 
... 


11 

. 
... 


. 
... 


. 
... 


. 
...

11 

. 
... 


. 
...

Pkjk s. 
= 
s. 
Pk jk 

. 
... 


. 
...

s+1 . 
1 
s
+1 . 
1 

. 
... 


.

.. 
. 


. 
... 


sn . 
1 
sn . 
1 


5.2 The EchelonFormand Gaussian Elimination 63 
holds for certain 
sj. 
. 
K, j=. 
+1 n.Hence, 

Sn ···S1 = 


.. 


.. 
1

1 

.. 


. 
... 


. 
... 


. 
...

1 

. 
... 
···

1 

. 
... 


. 
sn.2 n.2 
. 


..

sn.1 n.1 

.. 


sn.1 n.21 
sn nsn n.1 sn n 
sn n.21 
... 
. 


1 s11 

... 
.

s22 
s21 1 

... 
. 


... 
. 


s32 1 
s31 1 

... 
. 
Pn.1 jn.1 ···P1 j1 

... 
. 


... 
. 


sn21 
sn 11 

Theinvertiblelowertriangular matrices and the permutationmatrices form groups 
with respect to the matrix multiplication (cp. Theorems 4.13 and 4.16). Thus, 

Sn ···S1 = 
LP, where L is invertible and lower triangular, and P is a permuta


tion matrix. Since L =[lij]is invertible, also D := 
diag(l11 lnn is invertible, 
and we obtain A= 
PLU with P := 
P

.1 = 
P

T , L := 
L.1Dand U := 
D.1U

.By 
construction, alldiagonal entriesof L are equal to one. . 


Example5.5 Computation ofan LU-decomposition ofa matrix from Q33: 

224 . 
. 
.
221. 
201 
. 
. 
. 
. 
j1 =2 i1 =1 
.> 
M1 . 
1 
2 . 
. 
112 
221 
201 . 
. 
. 
.> 
G13(.2 . 
11 2 
221 
0.2 .3 . 
. 
. 
.> 
G12(.2 . 
11 2 
00 .3 
0.2.3 . 
.> 
P23 . 
11 2 
0.2 .3 
00 .3 . 
= 

U 

Hence, P = 
P23, 

..

1 

00

12 1 

..

L = 
G12(.2 G13(.2 M1 =.210 D =diag 11 

22

.211 

and thus, P = 
P

T = 
PT 

23 = 
P23, 


5 The EchelonFormand theRankofMatrices 

.. 
.. 


100 224 
L = 
L.1D = 
.
110. 
U =D.1U
= 
. 
0.2.3. 
101 00 .3 

1 U.1L.

If A .GLn(K , then the LU-decompositionyields A.=1PT.Hence 
after computingthe LU-decomposition, one obtains theinverseof Aessentially by 
inverting the two triangular matrices. Since this can be achieved by the efficient 
recursive formula(4.4), the LU-decomposition is a popular method in scientific 
computing applicationsthatrequiretheinversionof matricesorthesolutionoflinear 
systems of equations (cp. Chap. 6). In this context, however, alternative strategies 
forthe choiceof the permutationmatrices areused.Forexample, insteadof thefirst 
nonzeroentryinacolumnone choosesanentrywithlarge(orlargest) absolutevalue 
forthe rowexchange and thesubsequent elimination.By this strategy theinfluence 
of rounding errorsin the computationisreduced. 

MATLAB-Minute. 

The Hilbert matrix3 A =[aij].Qnn has the entries aij =1/(i +j.1 
for ij = 
1 n. It can be generated in MATLAB with the command 
hilb(n).Carry out the command [L,U,P]=lu(hilb(4)) in order to compute 
an LU-decompositionofthe matrix hilb(4).How do the matrices P, L 
and Ulook like? 
Compute alsothe LU-decompositionofthe matrix 

full(gallery(’tridiag’,-ones(3,1),2.ones(4,1),-ones(3,1))) 
and study the corresponding matrices P, Land U. 

We will nowshowthat,foragiven matrix A, the matrix C in Theorem 5.2 is 
uniquely determinedina certain sense.Forthiswe need thefollowing definition. 

Definition 5.6 IfC .Kn m isin echelonform(asinTheorem5.2),thenthe positions 
of (1 j1 ,...,(r jr are calledthe pivot positions of C. 

We also need thefollowing results. 

Lemma 5.7 IfZ .GLn(K and x .Kn 1,thenZx =0if and onlyifx =0. 

Proof Exercise. 

Theorem 5.8 LetA B .Knm beinechelon form.IfA =ZB for a matrix Z . 
GLn(K ,thenA=B. 

3DavidHilbert(1862–1943). 


5.2 The EchelonFormand Gaussian Elimination 65 
Proof If B is the zero matrix, then A= 
ZB = 
0, and hence A= 
B. 

Let now B 
= 
0 and let AB have the respective columns ai bi,1 . 
i . 
m. 
Furthermore,let (1 j1 ,...,(r jr be the r . 
1pivot positions of B.We will show 
that every matrix Z . 
GLn(K with A= 
ZB has theform 

Ir

Z = 


0 

Zn.r 

where Zn.r . 
GLn.r(K .Since B is in echelon form and all entries of B below its 
row r are zero, it then follows that B = 
ZB = 
A. 

Since (1 j1 isthefirstpivot positionof B,wehave bi = 
0 . 
Kn 1 for1. 
i . 
j1 . 
1andbj1 = 
e1 (the first column of In). Then A = 
ZB implies ai = 
0 . 
Kn 1 
for1. 
i . 
j1 . 
1andaj1 = 
Zbj1 = 
Ze1.Since Z isinvertible,Lemma 5.7 implies 
that aj1 
= 
0. 
Kn 1.Since Ais in echelon form, aj1 = 
e1 = 
bj1. Furthermore, 

1

Z = 
Zn := 


0 

Zn.1 

where Zn.1 . 
GLn.1(K (cp.Exercise 5.3).Ifr = 
1, then we are done. 

If r > 
1, then we proceed with the other pivot positions in an analogous way: 
Since B is in echelon form,the kthpivot positiongives bjk = 
ek.From ajk = 
Zbjk 
and theinvertibilityof Zn.k+1 we obtain ajk = 
bjk and 

. 


. 


. 


.

Z = 


Ik.1 0 . 
0 1 . 
0 0 Zn.k 

where Zn.k . 
GLn.k(K . . 


This resultyieldsthe uniquenessofthe echelonformofamatrixanditsinvariance 
under left-multiplication with invertible matrices. 

Corollary 5.9 For A. 
Kn m thefollowing assertions hold: 

(1) Thereisa unique matrixC . 
Kn m in echelon form to which A can be transformedbyelementaryrow 
operations,i.e.,byleft-multiplicationwith elementary 
matrices. ThismatrixCis calledthe echelon form of A. 
(2) IfM . 
GLn(K,thenthematrixCin(1)isalsotheechelon formofMA,i.e., 
theechelon formofamatrixisinvariant under left-multiplicationwithinvertible 
matrices. 
Proof 

(1) If S1A = 
C1 and S2A = 
C2, where C1 C2 arein echelon form and S1 S2 are 
invertible,then C1 = 
S1S2 .1 C2.Theorem 5.8 nowgives C1 = 
C2. 

(2) If M . 
GLn(K and S3(MA = 
C3 is in echelon form, then with S1A = 
C1 
from(1) we get C3 = 
S3MS.1 C1.Theorem 5.8 nowgives C3 = 
C1. . 


1 


5 The EchelonFormand theRankofMatrices 

5.3 Rankand EquivalenceofMatrices 
As we have seen in Corollary 5.9, the echelon form of A . 
Kn m is unique. In 
particular,foreverymatrix A.Kn m,thereexistsaunique numberofpivotpositions 
(cp. Definition 5.6)in its echelon form.Thisjustifies thefollowing definition. 

Definition 5.10 The number r of pivot positions in the echelon form of A .Kn m 
is calledthe rank4 of Aand denoted by rank( 
A . 

We seeimmediatelythat for A.Kn m always0 .rank(A .min{nm},where 
rank(A =0ifand onlyif A =0. Moreover, Theorem5.2 showsthat A .Kn n is 
invertibleif and onlyifrank(A =n. Further propertiesofthe rank aresummarized 
in thefollowing theorem. 

Theorem 5.11 For A .Kn m thefollowing assertions hold: 

(1) Thereexist matricesQ .GLn(K andZ .GLm(K with 
Ir 0rm.r

QAZ = 


0n.rr 0n.rm.r 

if and onlyifrank(A =r. 

(2) IfQ .GLn(K andZ .GLm(K ,thenrank(A =rank(QAZ . 
(3) IfA =BC withB .Kn . 
andC .K. 
m,then 
(a rank(A .rank(B 
(b rank(A .rank(C 

(4) 
rank(A =rank(AT . 
(5) There exist matrices B . 
Kn . 
and C . 
K. 
m with A = 
BC if and only if 
rank(A .. 
Proof 

(3a) Let Q .GLn(K be such that QB is in echelon form.Then QA = 
QBC. 
In the matrix QBC at most the first rank(B rows contain nonzero entries.By 
Corollary5.9,the echelonformof QA is equal to the echelon form of A.Thus, 
in the normal echelon form of A also at most the first rank(B rows will be 
nonzero, whichimpliesrank (A .rank(B . 

(1) 
.:If rank(A =r =0, i.e., A =0, then Ir =[]and the assertion holds for 
arbitrary matrices Q.GLn(K and Z .GLm(K . 
If r .1, then there exists a matrix Q .GLn(K such that QA is in echelon 
form with r pivotpositions.Then thereexistsa permutationmatrix P .Km m , 
thatisaproductof elementary permutationmatrices Pij, with 
4Theconceptofthe rankwasintroduced(inthe contextofbilinear forms) firstin 1879byFerdinand 
GeorgFrobenius (1849–1917). 


5.3 Rankand EquivalenceofMatrices 67 
Ir 0rn.r

PATQT = 


V 0m.rn.r 

for some matrix V . 
Km.rr.If r = 
m, then V =[].In the following, for 
simplicity,weomitthe sizesof the zeromatrices.The matrix 

Ir 0 
. 
Km m 

Y:= 


.VIm.r 

is invertible with 
. 
Y.1 Ir 0 
. 
Km m 


= 


VIm.r 

Thus, 

Ir 0

YPATQT = 


00 

and with Z:= 
PTYT . 
GLm(K we obtain 

Ir 0

QAZ = 
(5.5)

00 

.: Supposethat(5.5)holds for A . 
Kn m and matrices Q. 
GLn(K and 
Z. 
GLm(K .Then with (3a) we obtain 
rank(A = 
rank(AZZ.1 . 
rank(AZ . 
rank(A 

and thus, in particular, rank(A = 
rank(AZ . Due to the invariance of the 
echelon form (and hence the rank) under left-multiplication with invertible 
matrices (cp. Corollary 5.9), we get 

Ir 0

rank(A = 
rank(AZ = 
rank(QAZ = 
rank = 
r

00 

(2) If A. 
Kn.n , Q. 
GLn(K and Z. 
GLm(K ,then theinvarianceof therank 
under left-multiplication with invertible matrices and (3a canagainbeused 
for showing that 
rank(A = 
rank(QAZZ.1 . 
rank(QAZ = 
rank(AZ . 
rank(A 

and hence, in particular, rank(A = 
rank(QAZ . 

(4) If rank(A = 
r, then by (1 there exist matrices Q . 
GLn(K and Z . 
Ir 0

GLm(K with QAZ = 
.Therefore, 

00 


5 The EchelonFormand theRankofMatrices 

. 
TIr 0 Ir 0

rank(A = 
rank(QAZ = 
rank = 
rank = 
rank((QAZ T 

00 00 
= 
rank(ZTATQT = 
rank(AT 

(3b) Using (3a and (4 , we obtain 
rank(A = 
rank(AT = 
rank(CTBT . 
rank(CT = 
rank(C 

(5) Let A= 
BC with B. 
Kn . 
, C. 
K. 
m.Then by (3a , 
rank(A = 
rank(BC . 
rank(B . 
. 
Let, on the other hand, rank(A = 
r . 
. Then there exist matrices Q . 


Ir 0

GLn(K and Z . 
GLm(K with QAZ = 
.Thus, we obtain

00 

. 
. 
. 
. 


Q.1 Ir 0r ,.r Ir 0rm.rZ.1

A= 
=: 
BC

0n.rr 0n.r ,.r 0.rr 0.rm.r 

where B. 
Kn . 
and C. 
K. 
m . . 

Example5.12 The matrix 

.. 


02133 

35 

..

A= 
02011 . 
Q

02011 
fromExample 5.3 has the echelon form 

. 


1 0 
1 
1 1 
2 2 
220 
0 0 00 

. 


0 

. 


. 


. 
0 

. 


0 

Since therearetwopivot positions,wehaverank( 
A = 
2. Multiplying Afrom the 
right by 

.. 


10000 

..

00000 

.. 


.. 
55

B= 
00000 . 
Q

.. 


..

000 .1.1 
000 .1.1 

yields AB = 
0. 
Q35, and hence rank(AB = 
0< 
rank(A . 
Assertion(1)in Theorem 5.11 motivatesthe following definition. 


5.3 Rankand EquivalenceofMatrices 
69 
Definition 5.13 Two matrices AB . 
Kn m are called equivalent, if there exist 
matrices Q.GLn(K and Z .GLm(K with A=QBZ. 

As the namesuggests,this defines an equivalence relationonthe set Kn m, since 
thefollowing properties hold: 

• 
Reflexivity: A=QAZ with Q=In and Z =Im. 
• 
Symmetry: If A=QBZ,then B =Q.1AZ.1. 
• 
Transitivity:If A=Q1BZ1 and B =Q2CZ2,then A=(Q1Q2 C(Z2Z1. 
The equivalence class of A.Kn m is givenby 

[A]= 
QAZ |Q.GLn(K and Z .GLm(K 

If rank(A =r,thenby(1)inTheorem 5.11wehave 

Ir 0rm.r Ir 0 

= 
.[A]

0n.rr 0n.rm.r 00 

and, therefore, 

Ir 0 
=[A]

00 

Consequently,the rankof Afully determines the equivalence class [A].The matrix 

Ir 0 
.Kn m 

00 

is calledthe equivalence normalform of A.We obtain 

min{nm}. 
. 


 


Knm Ir 0 

= 
where

00 

r=0 

. 
. 


Ir 0 I. 
0 

=O ifr =.

00 00 

Hence thereare1 +min{nm}pairwisedistinct equivalence classes, and 

Ir 0 
.Kn m 


r =0 1 min{nm}

00 

is a complete set of representatives. 

From theproofof Theorem 4.9 we knowthat (Kn n +. 
for n .2is a non-
commutative ring with unitthat contains non-trivial zerodivisors. Usingthe equivalence 
normalformthese canbe characterized as follows: 

• 
If A . 
Kn n is invertible, then A cannotbea zerodivisor,since then AB = 
0 
impliesthat B =0. 

5 The EchelonFormand theRankofMatrices 

• 
If A. 
Kn n \{0} 
isa zerodivisor,then Acannot be invertible, and hence1 . 
rank(A = 
r < 
n,sothat the equivalence normalformof Ais not theidentity 
matrix In.Let QZ. 
GLn(K be given with 
Ir 0

QAZ = 


00 

Then for every matrix 

0rr 0rn.r . 
Kn n 

V := 


V21 V22 

and B:= 
ZV we have 

Q.1 Ir 00rr 0rn.r

AB = 
= 
0

00 V21 V22 

If V 
= 
0, then B
= 
0, since Zis invertible. 

Exercises 

(In thefollowingexercises Kis an arbitrary field.) 

5.1Computethe echelon formsofthe matrices 
1 i .i0 
. 
. 
. 
. 
. 
. 
A= 
12 3 
2448 . 
Q2 3 B= 
1 i 
i 1 . 
C2 2 C= 
. 
. 
. 
00 01 
50 .6i0 . 
. 
. 
. 
C4 4 
01 0 0 
. 
. 
. 
. 
10 1020 

32 
34 

.. 
..

D= 
11 . 
(Z/2Z E= 
2011 . 
(Z/3Z 

01 
1202 

(Herefor simplicity theelementsof Z/nZ are denoted by kinstead of [k].) 
Statethe elementarymatricesthat carryoutthetransformations.If oneofthe 
matricesisinvertible,then computeitsinverseasa productoftheelementary 
matrices. 
. 


..

5.2 Let A=. 
K22 with .. 

= 
...Determine the echelon form of Aand 
.. 


aformulafor A.1. 

1 A12

5.3 Let A=. 
Kn n with A12 . 
K1 n.1 and B . 
Kn.1 n.1. Show that 
0 B 
A. 
GLn(K if and onlyif B. 
GLn.1(K . 

5.4 Consider the matrix 
t+1 t.1 
t.1 t2 . 
(K(t 22 

A= 


t2 t.1 
t+1 t+1 


5.3 
Rankand EquivalenceofMatrices 71 
where K(t is the field of rational functions (cp. Exercise 3.19). Examine 
whether Ais invertible and determine, if possible, A.1.Verify your resultby 
computing A.1Aand AA.1. 

5.5 Showthatif A.GLn(K ,then the echelon form of [A In].Kn 2n is given 
by [In A.1]. 
(Theinverseof aninvertible matrix Acan thus be computed viathe transformation 
of [A In]to its echelon form.) 
5.6 Two matrices AB . 
Kn m are called left equivalent,ifthere exists a matrix 
Q.GLn(K with A=QB. Showthat this defines an equivalence relationon 
Kn m and determineamostsimplerepresentativefor each equivalence class. 
5.7ProveLemma5.7. 
5.8 Determine LU-decompositions (cp. Theorem 5.4)ofthe matrices 
... 
. 


1230 20 .20 

... 
.

4001 .404.1 

... 
. 
44 

A= 
B = 
. 
R

... 
.

5060 0.1.1.2 

0100 0011 

Ifoneofthesematricesisinvertible,then determineitsinverseusingits LU-
decomposition. 

5.9 Let 
A be the4 .4 Hilbert matrix (cp. theMATLAB-Minute above Definition 
5.6). Determine rank(A .Does Ahave an LU-decomposition asinTheorem 
5.4 with P =I4? 
5.10 Determinethe rankof thematrix 
.. 


0 .. 


33 

..

A=.. 
0 . 
.R

.... 
0 

in dependence of ....R. 

5.11 Let AB .Kn n begiven. Showthat 
AC

rank(A +rank(B .rank 

0 B 

for all C .Kn n.Examine when this inequality is strict. 

5.12 Let abc .Rn 1. 
(a) Determine rank(baT . 
(b) Let M(ab :=baT .abT. Showthe following assertions: 
(i) 
M(ab =.M(ba and M(ab c +M(bca+M(c ab =0, 
(ii) 
M(.a +.bc =.M(ac +.M(bc for ...R, 
(iii) rank(M(ab =0ifand onlyifthereexist...R with .=0or 
.=0and.a +.b=0, 
(iv) rank(M(ab .{02}. 

Chapter6 
LinearSystemsof Equations 

Solving linear systems of equations is a central problem of Linear Algebra that 
we discussinanintroductorywayin this chapter. Such systemsarisein numerous 
applications from engineeringtothe natural and social sciences.Major sources of 
linear systemsof equations arethe discretization ofdifferential equations and the 
linearizationofnonlinear equations.Inthis chapter we analyze thesolutionsetsof 
linear systems ofequations and we characterize the number of solutions using the 
echelon form fromChap. 5.Wealsodevelop an algorithmfor the computationofthe 
solutions. 

Definition 6.1 Alinear system (of equations) over a field K with n equations in m 
unknowns x1,...,xm has theform 

a11x1 +...+a1mxm =b1, 
a21x1 +...+a2mxm =b2, 
. 


. 


. 
an1x1 +...+anmxm =bn 

or 

Ax =b, 


where the coefficient matrix A =[aij].Kn,m and the right hand sideb =[bi].Kn,1 aregiven. If b =0, then the linear system is calledhomogeneous, otherwise 
non-homogeneous.Every 

x .Km,1 with A

x =bis called a solution of the linear 
system.All these 

x form the solution set of the linear system,which we denoteby 
L(A,b). 

Thenextresult characterizesthesolutionset L(A,b)of the linear system Ax =b 
usingthe solutionset L(A,0)oftheassociated homogeneous linear system Ax =0. 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_6 

6 Linear SystemsofEquations 

Lemma 6.2 Let A . 
Kn,m and b . 
Kn,1 with L(A, 
b) 

= 
O be given. If 

x . 
L(A, 
b),then 

L(A, 
b) 
= 

x + 
L(A, 
0) 
:= 
{
x +
z |
z . 
L( 
A, 
0)}. 


Proof If

z . 
L(A, 
0), and thus

x +
z . 

x + 
L( 
A, 
0),then 

A(

x +z) 
A

x + 
A

z = 
b+ 
0= 
b. 


Hence 

x +
z . 
L(A, 
b),which showsthat

x + 
L(A, 
0) 
. 
L(A, 
b). 
Let now 
x1 . 
L(A, 
b) 
and let 

z := 
x1 . 
x.Then 

A

z = 
A
x1 . 
A

x = 
b. 
b= 
0, 


i.e., 

z . 
L( 
A, 
0).Hence 
x1 = 

x +
z . 

x + 
L(A, 
0),which showsthat L(A, 
b) 
. 

x + 
L(A, 
0). . 


We will have a closer look at the set L(A, 
0):Clearly,0 . 
L(A, 
0) 

= 
O. If 

z . 
L(A, 
0),then for all . 
. 
K we have A(.
z) 
= 
.(A
z) 
= 
. 
· 
0 = 
0, and hence 

.

z . 
L(A, 
0). Furthermore,for
z1,
z2 . 
L( 
A, 
0) 
we have 

A(
z1 +z2) 
= 
A
z1 + 
A
z2 = 
0+ 
0= 
0, 


and hence 
z1 +z2 . 
L( 
A, 
0).Thus, L(A, 
0) 
is a nonempty subset of Km,1 that is 
closed under scalar multiplication and addition. 

Lemma 6.3 If A . 
Kn,m,b . 
Kn,1 andS . 
Kn,n, then L(A, 
b) 
. 
L(SA, 
Sb). 
Moreover,ifSisinvertible,then L(A, 
b) 
= 
L(SA, 
Sb). 

Proof If 

x . 
L( 
A, 
b), then also SA

x = 
Sb, and thus 

x . 
L(SA, 
Sb), which 
shows that L(A, 
b) 
. 
L(SA, 
Sb).If S is invertible and 

y . 
L(SA, 
Sb), then 
SA

y = 
Sb.Multiplying fromthe left with S.1 yields A

y = 
b.Since 

y . 
L(A, 
b), 
we have L(SA, 
Sb) 
. 
L(A, 
b). . 


Consider the linear system of equations Ax = 
b.ByTheorem5.2 we can find 
amatrix S . 
GLn(K) 
such that SA is in echelon form.Let b =[bi]:= 
Sb, then 

L(A, 
b) 
= 
L(SA, 
b) 
by Lemma6.3, and the linear system SAx = 
b takes the 

form 

. 
. 


. 


. 


. 


. 


. 


0 

1 . 
0 
1 
. 
0 
0 
1 
. 
0 
0 
0 

. 


. 


. 


. 


. 


. 


. 


. 


0 

. 


. 


. 


0 
1 

. 


.. 


b1 

. 


.. 


. 


.. 


. 


.. 


.

. 
.. 


. 


. 
. 
. 


. 


x = 
. 
. 
. 
. 


. 
. 


.. 


. 


.. 


. 


.. 


. 


.. 


. 


0 


bn 


6 Linear SystemsofEquations 

Supposethat rank(A) 
=r, and let j1, 
j2,..., 
jr be thepivot columns.Usingarightmultiplicationwitha 
permutationmatrixwe canmovethe r pivotcolumns of SA to 
the first r columns.Thisis achievedby 

PT :=[ej1,...,ejr ,e1,...,ej1.1,ej1+1,...,ej2.1,ej2+1,...,ejr .1,ejr+1,...,em]. 
Km,m , 


whichyields 

Ir A12

A:= 
SAPT = 
,

0n.r,r 0n.r,m.r 

where A12 . 
Kr,m.r.If r = 
m,then we have A12 =[].This permutationleads to 

asimplificationof the following presentation,butitisusually omittedinpractical 
computations. 

Since PTP = 
Im, we can write SAx = 
b as (SAPT)(Px) 
= 
b,or Ay = 
b, 

which has theform 

. 
. 


Ir 

. 


. 


. 


. 


. 


. 


0n.r,r 

. 
.. 
. 


. 


y1 b1 

. 
. 
.. 
. 
. 


.. 
. 
.. 
. 
.
A12 .. 


.. 
.. 
. 


.. 
.. 
.

yr br 

0n.r,m.r 
. 
. 
. 
. 
. 
. 
. 
. 
. 
yr+1 
. 
. 
. 
. 
. 
. 
. 
. 
= 
. 
. 
. 
. 
. 
br+1 
. 
. 
. 
. 
. 
. 
. 
. 
. 
(6.1) 
. 
. 
ym bn 

=A:=SAPT 

=y:=Px 


=b:=Sb 

Theleft-multiplicationof x with P just meansadifferent orderingofthe unknowns 
x1,...,xm.Thus,the solutionsof Ax =bcanbe easily recoveredfromthe solutions 

of Ay = 
b, and vice versa: We have 

y . 
L(A,b) 
if and only if 

x := 
PT 

y . 
L(SA,b) 
=L(A,b). 

Thesolutionsof(6.1)cannowbedeterminedusingtheextended coefficient matrix 

[A,b]. 
Kn,m+1 , 


which is obtained by attaching b as an extra column to A. Note that rank(A) 
. 


rank([A,b]),with equalityif and onlyifbr+1 = 
···=bn = 
0. 

If rank(A)< 
rank([A,b]), then at least one of br+1,...,bn is nonzero. Then 

(6.1)cannot have a solution, since all entries ofAin the rows r +1,...,n are zero. 

If, on the other hand, rank(A) 
= 
rank([A,b]), then br+1 = 
··· 
= 
bn = 
0 and 

(6.1)can be written as 

.... 
.. 


y1 b1 yr+1 

. 
. 
.. 
. 
.. 
. 
. 


. 
. 
. 
= 
. 
. 
. 
. 
A12 
. 
. 
. 
. 
(6.2)

.. 
. 


yrbr ym 


6 Linear SystemsofEquations 

This representationimplies,in particular,that 

b(m) 
:=[b1,...,br,0,...,0]T .L(A,b) 
=O. 


m.r 

From Lemma 6.2weknowthat L(A
,b) 
b(m)+L(A
,0).Inorderto determine 
L(A,0)we set b1 =···= 
br =0in(6.2), whichyields 

T

L(A,0) 
=[y1,...,
ym]|yr+1,...,
ym arbitrary and (6.3) 

TT

[y1,...,
yr]=0.A12[yr+1,...,
ym]. 


If r =m,then A12 =[], L(A,0) 
={0}and thus |L(A,b)|=1, i.e., the solution 

of Ay
=bis uniquely determined. 

Example6.4 Fortheextended coefficient matrix 

. 


103 
[A,b]= 
014

. 


000 

. 


b1 

3,4 

.

b2 .Q

b3 

we haverank(A) 
=rank([A,b])if andonlyifb3 =0.Ifb3 =0,thenL(A,b) 
=O. 

If b3 =0, then Ay =bcan be written as 

y1 b13 

= 
.[y3]. 


y2 b24 

Hence, b(3) 
=[b1,b2,0]T .L(A,b)and 

T

L(A
,0) 
=[y1,
y2,
y3]|y3 arbitrary and [y1,
y2]T =.[3,4]T[y3] 
. 


Summarizing our considerations wehave thefollowing algorithmfor solvinga 
linear system of equations. 

Algorithm6.5 Let A.Kn,m and b.Kn,1 be given. 

(1) Determine S.GLn(K)such that SA is in echelon form and define b:=Sb. 
(2a) If rank(SA)< 
rank([SA,b]),then L(SA,b) 
=L(A,b) 
=O. 

(2b) If r =rank(A) 
=rank([SA,b]),then define A:=SAPT as in(6.1). 

Wehave 
b(m) 
.L(A
,b)and L(A
,b) 
= 
b(m) 
+L(A
,0), where L(A
,0)is 

determined asin(6.3), as well as L(A,b) 
={PT 
y|y.L(A,b)}. 

Since rank(A) 
= 
rank(SA) 
= 
rank(A) 
and rank([A,b]) 
= 
rank([SA,b]) 
= 


rank([A,b]),the discussion abovealso yields thefollowing result about thedifferent 

cases of the solvability of a linear system of equations. 


6 Linear SystemsofEquations 

Corollary 6.6 For A .Kn,m andb .Kn,1 thefollowing assertions hold: 

(1) If rank(A)<rank([A,b]),then L(A,b)=O. 
(2) If rank(A)=rank([A,b])=m, then |L(A,b)|=1(i.e., there exists a unique 
solution). 
(3) If rank(A)=rank([A,b])<m, then thereexist many solutions.Ifthe fieldK 
has infinitely many elements(e.g.,whenK =Q,K =R orK =C), then there 
existinfinitely many pairwisedistinct solutions. 
Thedifferent casesin Corollary 6.6 willbe studiedagaininExample 10.8. 

Example6.7 Let K =Q and consider the linear system of equations Ax =bwith 

.. 
.. 


1221 1 

.. 
..

0103 0 

.. 
.. 


.. 
..

A= 
1030 , 
b= 
2 . 


.. 
.. 


.. 
..

2354 3 
1133 2 

We form[A,b]and applythe Gaussian eliminationalgorithminorderto transform 
Ainto echelon form: 

. 


122 1 

. 


0103 

. 


. 


0.21 .1

[A,b]. 


. 


. 


0 .11 
0 .11 

. 


1221 

. 


0103 

. 


.

. 
0015 
. 


. 


0000 
0000 

. 


2 
2 

100 .15 

. 


010 3 

. 


.

. 
001 5 
. 


. 


000 0 
000 0 

.. 


1 1221 

..

0 0103 

.. 


..

1 . 
0015 

.. 


..

1 0015 

1 0015 

.. 


1 102 .5 

..

0 010 3 

.. 


..

1 . 
0015 

.. 


..

0 0000 

0 

0000 

. 


.1 

.

0 

. 


.

1 

=[SA|b]. 


. 


.

0 

0 

1 
0 
1 
0 
0 

. 


1 

.

0 

. 


.

1 

. 


.

1 

1 

. 
. 


. 


. 


. 


. 


Here rank(SA) 
= 
rank([SA,b]) 
= 
3, and hence there exist solutions. The pivot 
columns are ji = 
i for i = 
1,2,3, so that P = 
PT = 
I4 and A = 
SA.Now 

SAx =bcan be written as 

..... 
. 


x1 .1 .15 

..... 
.

x2 = 
0 . 
3 [x4]. 
x31 5 


6 Linear SystemsofEquations 

Consequently, b(4) 
=[.1,0,1,0]T . 
L(A,b) 
and L(A,b) 
= 
b(4) 
+L(A,0), 

where 

L(A,0) 
=[x1,...,
x4]T |x4 arbitrary and [x1,
x2,
x3]T =.[.15,3,5]T[x4] 
. 


Exercises 

6.1Findafield Kand matrices A. 
Kn,m , S. 
Kn,n and b. 
Kn,1 with L(A,b) 

= 
L(SA,Sb). 
6.2 Determine L(A,b)forthe following Aand b: 
.. 
.. 


111 
1 

3,3 
3,1 

.. 
..

A= 
12 .1 .R, 
b=.2 .R, 


1.16 
3 

. 
... 


1110 
1 

3,4 
3,1 

. 
...

A= 
12 .1 .1 .R, 
b=.2 .R, 
1 .162 3 

.. 
.. 


111 
1 

.. 
..

12 .1 
.2 

.. 
4,3 
.. 
4,1

A=.R, 
b=.R, 


.. 
..

1 .16 
3 

111 
1 

.. 
.. 


111 
1 

.. 
..

12 .1 
.2

4,3 
.

.. 
. 
4,1

A=.R, 
b=.R. 


.. 
..

1 .16 
3 

111 
0 

6.3 Let . 
.Q, 
.. 
.. 


321 
6 

3,3 
3,1 

.. 
..

A= 
111 .Q, 
b. 
= 
3 .Q. 
210 . 


Determine L(A,0)and L(A,b.)in dependence of .. 

6.4 Let A. 
Kn,m and B. 
Kn,s.For i = 
1,...,s denoteby bi the ith column of 
B. Showthat the linear system of equations AX = 
Bhas at least one solution 
X. 
Km,s if and onlyif 

rank(A) 
=rank([A,b1]) 
= 
rank([A,b2]) 
=···=rank([A,bs]). 


Find conditions under whichthissolutionis unique. 


6 Linear SystemsofEquations 

6.5 Let 
.. 
0 .1 
.. 
. 
. 
. 
b1 

A= 
. 
. 
. 
.2 0 
. 
. 
. 
. 
. 
. 
. 
. 
Kn,n , 
b= 
. 
. 
. 
. 
. 
. 
. 
. 
Kn,1 
. 
. 
. 
. 
. 
.n . 
bn 
.n 0 

be givenwith .i, 
.i 
= 
0for alli.Determinearecursive formulafor the entries 
of thesolutionofthe linear system Ax = 
b. 


Chapter7 
Determinants of Matrices 

The determinant is a map that assigns to every square matrix A .Rn n, where R is 
a commutative ring with unit, an element of R.This map has very interesting and 
important properties.Forinstanceit yieldsa necessary and sufficient conditionfor 
theinvertibility of A . 
Rn n.Moreover, it forms the basis for the definition of the 
characteristic polynomial of a matrix in Chap.8. 

7.1 Definitionof theDeterminant 
Thereareseveraldifferent approachesto definethe determinantofamatrix.Weuse 
the constructive approach via permutations. 

Definition 7.1 Let n .N be given.Abijective map 

. 
:{12 n}>{12 n} 
j>.( 
j 

is called a permutation of the numbers {12 n}.We denotethe setof all these 
maps by Sn. 

Apermutation. 
.Sn canbe writteninthe form 

.(1 .(2 .(n 

For exampleS1 ={[1 ]}, S2 ={[12 [21 ]}, and 

S3 ={[123 [132 [213 [231 [312 [321 } 


From Lemma 2.17 we knowthat |Sn|=n!=1·2··n. 

©SpringerInternationalPublishing Switzerland 2015 81 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_7 

7 DeterminantsofMatrices 

The set Sn with the composition of maps “.”formsagroup (cp.Exercise 3.3), 
whichissometimes calledthe symmetric group.The neutralelementin this groupis 
the permutation [12 n . 

While S1 and S2 are commutative groups, the group Sn for n . 
3 is non-
commutative. As an example consider n = 
3 and the permutations .1 =[231 , 
.2 =[132 .Then 

.1 ..2 =[.1(.2(1 .1(.2(2 .1(.2(3 ]=[.1(1 .1(3 .1(2 ]=[213 

.2 ..1 =[.2(.1(1 .2(.1(2 .2(.1(3 ]=[.2(2 .2(3 .2(1 ]=[321 

Definition 7.2 Let n .2and. 
.Sn.Apair (.(i .( 
j with1 .i < 
j .n and 
.(i > 
.( 
j is called an inversion of ..If k is the number of inversions of .,then 

0

sgn(. 
:=(.1 k is calledthe sign of ..For n =1we define sgn([1 :=1=(.1. 

In short, an inversion of a permutation . 
isa pairthatis “outof order”. Theterm 
inversionshould notbe confused with theinverse map ..1 (which exists, since . 
is 
bijective).The signofa permutationissometimes also calledthe signature. 

Example7.3 The permutation [2314 . 
S4 has the inversions (2 1 and (31 , 
so that sgn([2314 =1. The permutation [4123 ].S4 has theinversions (41 , 
(42 , (43 ,sothat sgn([4123 =.1. 

We can now define the determinant map. 

Definition 7.4 Let Rbe a commutative ring with unit and let n .N.The map 

n 

det :Rn n >R 

A=[aij >det( 
A := 
sgn(. 
ai .(i (7.1) 

..Sn i=1 

is calledthe determinant, and theringelement det(A is calledthe determinant ofA. 

Theformula(7.1)for det(A is calledthe signatureformulaof Leibniz.1 Theterm 
sgn(. 
in this definitionistobeinterpretedasan elementofthering R, i.e., either 
sgn(. 
=1.Ror sgn(. 
=.1.R,where .1.Ris the unique additive inverse 
of the unit1 .R. 

Example7.5 For n =1wehave A =[a11 and thus det( 
A =sgn([1 a11 =a11. 
For n =2we get 

a11 a12

det(A =det =sgn([12 a11a22 +sgn([21 a12a21

a21 a22 

=a11a22 .a12a21 

1GottfriedWilhelmLeibniz(1646–1716). 


7.1 Definitionof theDeterminant 83 
For n =3wehavethe Sarrus rule2: 

det(A =a11a22a33 +a12a23a31 +a13a21a32 

.a11a23a32 .a12a21a33 .a13a22a31 

In order to compute det( 
A usingthe signatureformulaof Leibnizwehaveto 
form n!products with n factors each.Forlarge n this is too costly even on modern 
computers.As we will seeinCorollary 7.16,thereare moreefficientways for 
computing det(A .The signatureformulais mostlyof theoretical relevance, sinceit 
representsthe determinant of Aexplicitlyin termsofthe entriesof A.Considering 
the n2 entries asvariables,wecan interpret det(A asapolynomialinthesevariables. 
If R = 
R or R = 
C, then standard techniques of Analysis show that det(A is a 
continuous functionofthe entriesof A. 

We will now study the group of permutations in more detail. The permutation 
. 
=[321 . 
S3 has theinversions (32 , (3 1 and(2 1 , so that sgn(. 
=.1. 
Moreover, 

. 
.( 
j ..(i 
= 
.(2 ..(1 .(3 ..(1 .(3 ..(2 
1.i< 
j.3 
j.i 2.1 3.1 3.2 

2.31.31.2 

==(.13 =.1=sgn(. 


2.13.13.2 

This observation can be generalized as follows. 

Lemma 7.6 For each. 
.Sn we have 

.( 
j ..(i 

sgn(. 
= 
(7.2)

j.i 

1.i< 
j.n 

Proof If n =1,thenthelefthandsideof(7.2)isanempty product,whichis defined 
tobe1(cp. Sect. 3.2), so that(7.2)holds forn =1. 

Let n > 
1 and . 
. 
Sn with sgn(. 
= 
(.1 k, i.e., k is the number of pairs 
(.(i .( 
j with i < 
jbut.(i > 
.( 
j .Then 

kk

(.( 
j ..(i =(.1 |.( 
j ..(i |=(.1 ( 
j.i 

1.i< 
j.n 1.i< 
j.n 1.i< 
j.n 

In thelast equationwehaveusedthefact that thetwoproductshavethe samefactors 
(except possiblyfor theirorder). . 
	

2Pierre Frederic Sarrus(1798–1861). 


7 DeterminantsofMatrices 

Theorem 7.7 For all .1 .2 . 
Sn we have sgn(.1 ..2 = 
sgn(.1 sgn(.2 .In 
particular, sgn(..1 =sgn(. 
for all . 
.Sn. 

Proof By Lemma7.6 we have 

.1(.2( 
j ..1(.2(i 

sgn(.1 ..2 = 


j.i 

1.i< 
j.n 

. 
... 


.1(.2( 
j ..1(.2(i .2( 
j ..2(i 

. 
... 


= 


.2( 
j ..2(ij.i 

1.i< 
j.n 1.i< 
j.n 

.. 


.1(.2( 
j ..1(.2(i 

.. 


= 
sgn(.2

.2( 
j ..2(i 

1..2(i )<.2( 
j .n 

.. 


.1( 
j ..1(i 

.. 


= 
sgn(.2

j.i 

1.i< 
j.n 

=sgn(.1 sgn(.2 

For each. 
.Sn we have1=sgn([12 n =sgn(. 
...1 =sgn(. 
sgn(..1, 
so that sgn(. 
=sgn(..1. . 
	

Theorem 7.7 shows that the map sgn is a homomorphism between the groups 
(Sn . 
and ({1 .1}·, where the operation in the second group is the standard 
multiplicationofthe integers1and .1. 

Definition 7.8 A transposition is a permutation . 
. 
Sn, n . 
2, that exchanges 
exactly two distinct elements k . 
.{12 n}, i.e., .(k = 
, .(. 
= 
k and 
.( 
j = 
jfor all j.{12 n}\{k ,}. 

Obviously ..1 =. 
for every transposition . 
.Sn. 

Lemma 7.9 Let . 
. 
Sn be the transposition, that exchanges k and . 
for some 
1.k <. 
.n. Then. 
hasexactly 2(.k .1inversions and, hence,sgn(. 
=.1. 

Proof Wehave . 
=k+jfor a j.1and thus. 
is givenby 

. 
=[1 k.1 k+jk+1 k+( 
j.1 k . 
+1 n 

wherethe points denotevaluesof . 
in increasing and thus “correct”order.Asimple 
countingargument showsthat . 
has exactly2j.1=2(. 
.k .1inversions. . 
	


7.2 Propertiesofthe Determinant 85 
7.2 Propertiesofthe Determinant 
In this sectionweprove important propertiesofthe determinant map. 

Lemma 7.10 For A .Rn n thefollowing assertions hold: 

(1) For . 
.R, 
. 


. 
. 


01 n

det 

=det 

=. 
det(A

A . 


A

0n 1 

n

(2) IfA =[aij is upper or lowertriangular,then det(A = 
1 aii.
i=

(3) If Ahas a zero row or column, then det(A =0. 
(4) If n .2andAhas two equalrows or two equal columns,then det( 
A =0. 
(5) det(A =det(AT . 
Proof 

(1) Exercise. 
(2)Thisfollowsbyan applicationof(1)tothe upper(orlower)triangular matrix A. 
(3) If A has a zero row or column, then for every . 
.Sn at least onefactor in the 
n

product 1 ai .(i is equal to zero and thus det(A =0.

i=

(4) Let the rows k and , with k <,of A =[aij be equal, i.e., akj = 
aj for 
j=1 n.Let . 
.Sn be thetranspositionthatexchanges theelements kand 
, and let 
Tn :={. 
.Sn |.(k < 
.(. 
} 
Since the set Tn contains all permutations . 
. 
Sn for which .(k < 
.(. 
,we 
have |Tn|=|Sn|/2and 

Sn Tn ={. 
.. 
|. 
.Tn} 


Moreover, 

. 


.. 
ai .(ii 
=k . 


ai ,(... 
)(i = 
ak .(. 
i =k 

. 


. 


a. 
.(ki =. 


We have ak .(. 
= 
a. 
.(. 
and a. 
.(k = 
ak .(k , Thus, using Theorem7.7 and 
Lemma 7.9, we obtain 

nn 

sgn(. 
ai .(i = 
sgn(. 
.. 
ai ,(... 
)(i 

..SnTn i=1 ..Tn i=1 

n 

= 
(.sgn(. 
ai ,(... 
)(i 

..Tn i=1 

n 

=. 
sgn(. 
ai .(i 

..Tn i=1 


7 DeterminantsofMatrices 

This implies 

n 

det(A = 
sgn(. 
ai .(i 

..Sn i=1 

n 
n 

= 
sgn(. 
ai .(i + 
sgn(. 
ai .(i =0 

..Tn i=1 ..SnTn i=1 

Theproof forthe caseoftwo equal columnsis analogous. 

(5) We observe first that 
1

{(.(ii |1.i .n }={(i ..(i |1.i .n } 


forevery . 
.Sn.Tosee this,let iwith1.i .n be fixed. Then .(i =jif and 
onlyifi =..1( 
j .Thus, (.(ii =( 
ji is an element of the first setifand 
onlyif( 
j ..1( 
j =( 
ji is an element of the second set. Since . 
is bijective, 
thetwo sets are equal. 
Let A=[aij and AT =[bij with bij =aji.Then 

n 
n 

det( 
AT = 
sgn(. 
bi .(i = 
sgn(. 
a.(ii 

..Sn i=1 ..Sn i=1 

n 
n 

1 
1

= 
sgn(..a.(ii = 
sgn(..ai ..1(i 

..Sn i=1 ..Sn i=1 

n 

= 
sgn(. 
ai .(i =det( 
A 

..Sn i=1 

Here we have used that sgn(. 
=sgn(..1 (cp. Theorem 7.7)and thefact that 

nn

thetwo products 1 a.(ii and ..1(i have thesamefactors. . 
	

i=i=1 ai 

Example7.11 Forthe matrices 

.. 
.. 
.. 


123 120 112 

.. 
.. 
..

A= 
045 B = 
130 C = 
113 
006 140 114 

from Z33 we obtain det( 
A =1·4·6 =24by(2)inLemma7.10, and det(B = 
det(C =0by(3)and(4)inLemma7.10.Wemayalso computethese determinants 
usingthe Sarrusrule fromExample 7.5. 

Item (2) in Lemma 7.10 shows in particular that det(In = 
1 for the identity 
matrix In =[e1 e2 en ].Rn n.For this reason the determinant mapis called 
normalized. 


7.2 Propertiesofthe Determinant 87 
For . 
.Sn the matrix 

P. 
:=[e.(1 e.(2 e.(n 

is called the permutation matrix associated with ..Thismap from thegroup Sn to 
thegroupof permutationmatricesin Rn n is bijective. Theinverseofa permutation 
matrixis its transpose(cp. Theorem 4.16)and we can easily check that 

P.1 PT

==P..1

.. 


If A=[a1 a2 an ].Rn n, i.e., aj .Rn 1 is the jth column of A,then 

AP. 
=[a.(1 a.(2 a.(n 

i.e., the right-multiplication of Awith P. 
exchanges the columns of Aaccording to 
the permutation .. If, on the other hand, ai .R1 n is the ith row of A,then 

.. 


a.(1 

..

a.(2 

..

PT = 
.

. 
A. 


.. 


a.(n 

i.e., theleft-multiplication of A by PT exchanges the rows of A according to the

. 


permutation .. 
We next study the determinantsofthe elementarymatrices. 

Lemma 7.12 (1) For . 
.Sn and theassociated permutationmatrixP. 
.Rnn we 
have sgn(. 
=det(P. 
.Ifn .2andPij is defined asin(5.1), then det(Pij = 
.1. 

(2) If Mi(. 
and Gij(. 
are defined as in (5.2) and (5.3), respectively, then 
det(Mi(. 
=. 
and det(Gij(. 
=1. 
Proof 

(1) If

. 
.Sn and P

. 
=[aij . 
Rn n, then a 
.( 
jj =1for j =12 n, and all 
other entries of P

. 
are zero. Hence 
nn 

det(P 

. 
=det(P. 
T = 
sgn(. 
a.( 
jj =sgn(

. 
a 
.( 
jj =sgn(

. 


..Sn j=1 j=1 

. 
. 
. 
=1 
=0for .
=
. 


The permutation matrix Pij is associated with thetranspositionthatexchanges 
i and j.Hence, det(Pij =.1follows from Lemma7.9. 

(2) Since Mi(. 
and Gij(. 
arelower triangular matrices,the assertionfollows from 
(2 in Lemma7.10. . 
	

7 DeterminantsofMatrices 

Theseresults leadto some important computational rulesfor determinants. 

Lemma 7.13 For A .Rn n,n .2, and . 
.Rthe following assertions hold: 

(1) The multiplicationofarowofAby . 
leadsto themultiplicationof det( 
A by.: 
det(Mi(. 
A =. 
det(A =det(Mi(. 
det( 
A. 
(2) The additionofthe .–multipleofarowofAtoanotherrowofAdoes notchange 
det(A: 
det(Gij(. 
A =det( 
A =det(Gij(. 
det(A , and 
det(Gij(. 
TA =det(A =det(Gij(. 
T det(A. 
(3) Exchanging two rows of A changes the sign of det(A: 
det(PijA =.det(A =det(Pij det A. 
Proof 

(1) If A=[amk and A=Mi(. 
A=[amk ,then 
amk m 
=i 

amk = 
.amk m =i 

and hence 

n 
n 

det(A = 
sgn(. 
am .(m = 
sgn(. 
ai .(i 
am .(m 

. 
. 


..Sn m=1 ..Sn m=1 
=.ai .(im
=i =am .(m 

=. 
det( 
A 

(2) If A=[amk and A=Gij(. 
A=[amk ,then 
amk m 

= 
j 

amk = 
ajk +.aik m = 
j 

and hence 

n 

det(A = 
sgn(. 
)(aj.( 
j +.ai .( 
j am .(m 

..Sn 
m=1 
m
=j 

n 
n 

= 
sgn(. 
am .(m + 
. 
sgn(. 
ai .( 
j am .(m 

..Sn m=1 ..Sn m=1 

m
=j 

Thefirsttermis equaltodet(A , and the second is equal to the determinant of a 
matrix with two equal columns, and thus equal to zero. Theproof forthe matrix 
Gij(. 
TAis analogous. 


7.2 Propertiesofthe Determinant 
89 
(3) The permutation matrix Pij exchanges rows i and j of A, where i < 
j.This 
exchange can be expressed by the following four elementary row operations: 
Multiply row jby .1; add row i to row j;add the(.1 –multiple of row jto 
row i;add rowi to row j.Therefore, 
Pij =Gij(1 )(Gij(.1 TGij(1 Mj(.1 

(One mayverifythisalsobycarrying out thematrixmultiplications.) Using (1 
and (2 we obtain 

det(PijA =det Gij(1 )(Gij(.1 TGij(1 Mj(.1 A 

=det(Gij(1 det((Gij(.1 T det(Gij(1 det(Mj(.1 det( 
A 

=(.1 det(A 
. 
	

Since det(A =det( 
AT (cp.(5)inLemma 7.10),theresultsin Lemma7.13for 
the rows of Acanbe formulated analogously forthe columnsof A. 

Example7.14 Consider the matrices 

. 
. 
.. 


130 310 

33 

. 
. 
..

A= 
120 B = 
210 . 
Z124 214 

A simple calculation shows that det( 
A =.4. Since B is obtained from A by 
exchanging thefirsttwo columns wehave det(B =.det(A =4. 

The determinantmap canbe interpretedasamapof(Rn 1 n to R, i.e.,asamapof 
the n columnsof thematrix A.Rn n to thering R.Ifai aj .Rn 1 aretwo columns 

of A, 
A=[ 
ai aj 
then 
det(A =.det([ 
aj ai 

by(3)inLemma7.13.Duetothis propertythe determinantmapis calledan alternating 
mapofthe columnsof A.Analogously,the determinant mapisanalternating 
mapofthe rowsof A. 

( 
j

If the kth row of A has theform .a(1 +.a(2 for some .. 
. 
R and a= 


( 
j ( 
jn

aa.R1, j=1 2, then

k1 kn 


7 DeterminantsofMatrices 

.. 
.. 


..... 
. 
n 

.. 
.. 
(1 (2

det(A =det 
..
.a(1 +.a(2 
.. 
= 
sgn(..a.(k +.aai .(i

kk .(k 

.. 
.. 


..Sn 
i=1 
i
=k 

n 
n 

(1 
(2 

=. 
sgn(. 
aak .(k + 
. 
sgn(. 
aai .(i

k .(kk .(k 
..Sn i=1 ..Sn i=1 
i
=ki
=k 

.... 
.... 


.... 
.... 


.... 
....

(1 (2

=. 
det 
..
a.. 
+ 
. 
det 
..
a.. 


.... 
.... 


This propertyis calledthe linearity of the determinant map with respect to the rows 
of A.Analogously we have the linearity with respect to the columns of A.Linear 
maps willbe studiedin detailinlater chapters. 

Thenextresultis calledthe multiplicationtheorem for determinants. 

Theorem 7.15 IfKisa field andAB . 
Kn n, then det(AB = 
det(A det(B. 
Moreover,ifAisinvertible,then det(A.1 =(det(A .1 . 

Proof By Theorem5.2 we knowthat for A. 
Kn n there exist invertible elementary 
matrices S1 St such that A= 
St S1Aisin echelon form.ByLemma7.13we 

have 
det(A =det(S.1 ··· 
det(S.1 det(A

1 t 

as well as 

S.1 


det(AB =det ···S.1AB

1 t 

=det(S.1 ··· 
det(S.1 det(AB

1 t 

Thereare two cases:If A is not invertible, then A and thus also AB have a zero 

row. Then det(A = 
det(AB 
= 
0, which implies that det(A = 
0, and hence 

det( 
AB = 
0 = 
det(A det(B .Onthe other hand, if Ais invertible,then A = 
In, 

since Aisin echelon form.Nowdet(In =1again gives det( 
AB =det(A det(B . 

Finally,if A is invertible,then1 = 
det(In = 
det(AA.1 = 
det(A det( 
A.1, 
and hence det( 
A.1 =(det(A .1. . 
	

Since our proof reliesonTheorem 5.2,whichisvalid formatrices overa field 
K,wehaveformulatedTheorem7.15 for AB . 
Kn n.However,the multiplication 
theoremfor determinantsalso holdsformatricesoveracommutativering Rwith unit. 
Adirectproofbasedonthesignatureformulaof Leibnizcanbefound,forexample, 
in the book “Advanced Linear Algebra” by Loehr [Loe14, Sect.5.13]. That book 
also containsaproofofthe Cauchy-Binet formula for det( 
AB with A . 
Rn m and 
B . 
Rm n for n . 
m.Below we will sometimes usethat det(AB = 
det(A det(B 


7.2 Propertiesofthe Determinant 91 
holds for all AB . 
Rn n,althoughwehaveshownthe resultin Theorem7.15only 
for AB . 
Kn n . 

Theproofof Theorem 7.15 suggests that det(A can be easily computed while 
transforming A. 
Kn n into its echelon form using elementary row operations. 

Corollary 7.16 For A . 
Kn n letS1 St . 
Kn n be elementary matrices, such 

that A = 
St S1Aisinechelon form. Then either A hasazerorow and hence 

.1 .1

det( 
A =0,or A= 
In and hence det(A =(det(S1 ··· 
(det(St . 

AsshowninTheorem5.4,every matrix A. 
Kn n canbefactorized as A= 
PLU, 
and hence det(A = 
det(P det(L det(U . The determinants of the matrices on 
the right hand side are easily computed, since these are permutation and triangular 
matrices.An LU-decomposition of a matrix Athereforeyields an efficient wayto 
compute det(A . 

MATLAB-Minute. 

Lookat thematrices wilkinson(n) for n=2,3, ,10 in MATLAB. Can you 

finda general formulafor their entries?For n=2,3, ,10 compute 

A=wilkinson(n) 

[L,U,P]=lu(A) (LU-decomposition; cp. theMATLAB-Minute above Defi


nition 5.6) 

det(L), det(U), det(P), det(P).det(L).det(U), det(A) 

Which permutationisassociated with the computed matrix P?Whyis det(A) 

an integer for odd n? 

7.3 Minors andthe LaplaceExpansion 
Wenowshowthatthe determinantcanbeusedforderivingformulasfortheinverse 
of an invertible matrix and for the solution of linear systems of equations. These 
formulas are, however, more of theoretical than practical relevance. 

Definition 7.17 Let R be a commutative ring with unit and let A . 
Rn n , n . 
2. 
Then the matrix A( 
ji . 
Rn.1 n.1 thatis obtainedby deletingthe jth row and ith 
column of Ais called a minor3 of A.The matrix 

. 
Rn n 

adj( 
A =[bij with bij :=(.1 i+j det(A( 
ji 

is calledthe adjunct of A. 

The adjunct is also called adjungate or classical adjoint of A. 

3This termwasintroducedin 1850by JamesJosephSylvester (1814–1897). 


7 DeterminantsofMatrices 

Theorem 7.18 For A .Rn n,n .2, we have 
Aadj( 
A = 
adj(AA = 
det( 
A In 

In particular A is invertible if and only if det(A . 
R is invertible. In this case 
(det( 
A .1 =det(A.1 and A.1 =(det( 
A .1adj(A. 

Proof Let B =[bij have the entries bij =(.1 i+j det(A( 
ji .Then C =[cij = 
adj(AAsatisfies 

nn 

cij = 
bikakj = 
(.1 i+k det( 
A(k i akj 

k=1 k=1 

Let a. 
be the th column of Aand let 

. 
Rn n 

A(ki :=[a1 ai.1 ek ai+1 an 

where ek is the kth column of the identity matrix In.Then there exist permutation 
matrices Pand Qthat perform k.1row andi.1columnexchanges,respectively, 
such that 

1

PA(kiQ = 


0 

A(ki 

Using(1)in Lemma 7.10 we obtain 

det( 
A(k i =det . 
1 . 
0 A(k i . 
=det(P

A(k i Q 

=det(P det(A(ki det(Q 

=(.1 (k.1 +(i.1 det(A(ki 

=(.1 k+i det(A(ki 

The linearityofthe determinantwith respecttothe columnsnowgives 

n 

i+kk+i

cij = 
(.1 (.1 akj det(A(ki 

k=1 
=det([a1 ai.1 aj ai+1 an 

0 i 

= 
j 

= 


det( 
Ai = 
j 
=.ij det(A 

and thus adj(AA=det(A In.Analogously we can showthat Aadj( 
A =det(A In. 


7.3 Minorsand theLaplace Expansion 93 
If det(A . 
Ris invertible,then 

In = 
(det(A .1adj(AA= 
A(det(A .1adj(A 

i.e., Ais invertible with A.1 = 
(det( 
A .1adj(A .If, on the other hand, Ais invert-
ible,then 

1= 
det(In = 
det(AA.1 = 
det(A det(A.1 = 
det(A.1 det(A 

where we have used the multiplication theorem for determinants over R (cp. our 
comment following the proof of Theorem 7.15). Thus, det(A is invertible with 
(det( 
A .1 = 
det(A.1 , and again A.1 = 
(det(A .1adj(A . . 
	

Example7.19 

(1) For 
41 

22

A=. 
Z

21 

we have det( 
A = 
2 and thus A is not invertible. But A is invertible when 

1

considered as an element of Q22,sinceinthis casedet( 
A.1 = 
(det( 
A .1 = 
2. 

(2) For 
t . 
1t . 
2 

. 
(Z[t 22

A= 


tt . 
1 

we have det(A = 
1. The matrix Ais invertible, since1. 
Z[t is invertible. 

Note that if A . 
Rn n is invertible, then Theorem7.18 shows that A.1 can be 
obtained by inverting only one ring element, det(A . 

We now use Theorem7.18 and the multiplication theorem for matrices over a 
commutative ring with unittoprovearesultalready announcedin Sect. 4.2:Inorder 
to show that A . 
Rn n is the(unique) inverseof A . 
Rn n, only one of the two 

equations AA = 
In or AA= 
In needs to be checked. 

Corollary 7.20 LetA. 
Rn n.Ifamatrix A. 
Rn n exists with AA = 
In orAA= 
In, 

thenAisinvertible and A
= 
A.1 . 

Proof If 

In,then themultiplicationtheorem for determinantsyields

AA = 


1= 
det(In = 
det(AA = 
det(A det( 
A = 
det(A det(A 

i.e., det(A . 
R is invertible with (det(A .1 = 
det(A .Thus also A is invertible 
and has a unique inverse A.1.For n = 
1thisisobvious and forn . 
2itwas shown 
in Theorem7.18.Ifwemultiply the equation AA = 
In fromthe right with A.1 we 


A.1 
Theproof starting from AA= 
In is analogous. . 
	

get A= 
. 


7 DeterminantsofMatrices 

Letussummarizetheinvertibility criteriaforasquarematrixovera fieldthatwe 
have shownsofar: 

Theorem 52

A. 
GLn(K .• 
The echelon form of Ais theidentity matrix In 

Definition 510

.• 
rank(A = 
n 

clear

.• 
rank(A = 
rank([ 
Ab = 
n for all b. 
Kn 1 

Algorithm 66

.• 
| 
L(Ab |= 
1 for allb. 
Kn 1 

Theorem 718

.• 
det(A 

= 
0 (7.3) 

Alternatively we obtain: 

Theorem 52

A./GLn(K .• 
The echelon form of Ahas at least one zero row 

Definition 510

.• 
rank(A < 
n 

clear

.• 
rank([ 
A 0 < 
n 

Algorithm 66

.• 
L(A 0 
={ 
0}

Theorem 718

.• 
det(A = 
0 (7.4) 

In the fields QR and C wehavethe(usual) absolutevalue |·| 
of numbers and 
can formulatethe following usefulinvertibility criterion formatrices. 

Theorem 7.21 If A . 
Kn n withK .{ 
QRC} 
is diagonally dominant, i.e., if 

n 

| 
aii | 
> 
| 
aij| 
for all i = 
1 n 

j= 
1 
j

= 
i 

then det(A 

= 
0. 

Proof We prove theassertionby contraposition, i.e.,by showingthat det(A = 
0 
impliesthat Ais not diagonally dominant. 

If det(A = 
0, then L(A 0 
={ 
0} 
, i.e., the homogeneous linear system of 

T

equations Ax = 
0 has at least one solution
'
x =['x1 
'xn 

= 
0.Let 
'xm be an 
entry of 
'
x with maximal absolute value, i.e., |'xm|.|'xj| 
for all j = 
1 n.In 
particular, wethen have |'xm| 
> 
0.The mth row of A
'
x = 
0isgiven by 

n 

am1
'x1 + 
am2
'x2 ++ 
amn
'xn = 
0 v 
amm
'xm =. 
amj
'xj 

j= 
1 
j

= 
m 

We nowtake absolute values on bothsides and usethe triangleinequality,which 
yields 


7.3 Minorsand theLaplace Expansion 95 
nn n 

|amm ||'xm|. 
|amj||'xj|. 
|amj||'xm| 
hence |amm |. 
|amj|

j=1 j=1 j=1 
j
=mj
=mj
=m 

so that Anot diagonally dominant. . 
	

The converseofthistheorem does not hold:Forexample, thematrix 

12 

22

A=.Q

10 

has det(A =.2
=0,but Ais not diagonally dominant. 

From Theorem 7.18 we obtainthe Laplace expansion4 of the determinant,which 
is particularly useful when Acontains manyzero entries (cp. Example 7.24 below). 

Corollary 7.22 For A.Rn n,n .2,the following assertions hold: 

(1) For eachi =12 n we have 
n 

det( 
A = 
(.1 i+jaij det(A(ij 
j=1 

(Laplace expansion ofdet(A withrespecttotheithrowA.) 

(2) For eachj=12 n we have 
n 

det( 
A = 
(.1 i+jaij det(A(ij 
i=1 

(Laplace expansion ofdet(A withrespecttothejth columnofA.) 

Proof The two expansions for det(A follow immediately by comparison of the 
diagonal entries in the matrix equations det(A In = 
Aadj(A and det(A In = 
adj(AA. . 
	

TheLaplaceexpansions allowsarecursive definitionofthe determinant:For A. 
Rn n with n .2,let det(A be definedasin(1)or(2)in Corollary7.22.Wecan choose 
an arbitraryrowor columnofA.The formulafor det(A then contains onlymatrices 
ofsize (n.1 .(n.1 .For eachof thesewecan usetheLaplaceexpansionagain,now 
expressing each determinantin termsofdeterminantsof(n .2 .(n .2 matrices. 
We can do this recursively until only1.1matrices remain.For A =[a11 . 
R11 
we define det(A :=a11. 

Finally we state Cramer’srule,5 whichgivesanexplicit formulaforthesolutionof 
alinearsysteminformof determinants.Thisruleisonlyoftheoreticalvalue, because 
in order to compute the n componentsofthe solutionit requirestheevaluationof 
n +1determinants ofn .n matrices. 

4Pierre-Simon Laplace (1749–1827) publishedthisexpansionin 1772. 
5GabrielCramer(1704–1752). 


7 DeterminantsofMatrices 

Corollary 7.23 LetKbea field,A . 
GLn(K and b . 
Kn 1. Then the unique 
solutionofthe linear systemof equationsAx =bisgiven by 

TA.1b

'
x =['x1 
'xn ==(det(A .1 adj(Ab 

with 

det[a1 ai.1 b ai+1 an 

'xi = 
i =1 n 
det( 
A 

Example7.24 Consider 

.. 
.. 


1300 1 

.. 
..

1200 2 

.. 
44 
.. 
41 

A=.Qb=.Q

. 


. 
..

1210 1 

1231 0 

TheLaplaceexpansionwith respecttothelast columnyields 

.. 
.. 


130 

13 

.. 
..

det(A =1·det 120 =1·1·det =1·1·(.1 =.1

12

121 

Thus, Ais invertible and Ax =bhasa unique solution 
'
x =A.1b.Q41, which by 
Cramer’s rule has thefollowing entries: 

.. 
.. 


1300 

.. 
..

2200 

.. 
.. 


'x1 =det / 
det(A =.4/(.1 =4 
.. 
..

1210 

0231 

.. 
.. 


1100 

.. 
..

1200 

.. 
.. 


'x2 =det / 
det(A =1/(.1 =.1 
.. 
..

1110 

1031 

.. 
.. 


1310 

.. 
..

1220 

.. 
.. 


'x3 =det / 
det(A =1/(.1 =.1 
.. 
..

1210 

1201 

.. 
.. 


1301 

.. 
..

1202 

.. 
.. 


'x4 =det / 
det(A =.1/(.1 =1 
.. 
..

1211 

1230 


7.3 
Minorsand theLaplace Expansion 97 
Exercises 

7.1 Apermutation . 
.Sn is calledan r-cycle if thereexistsasubset {i1 ir}.
{12 n}with r .1elements and 
.(ik =ik+1 for k =12 r .1 .(ir =i1 .{i1 ir}

.(i 
=i for i /

Wewrite anr-cycleas . 
=(i1 i2 ir .Inparticular,atransposition . 
.Sn 
is a2-cycle. 

(a) Let n =4and the2-cycles.12 =(12 , .23 =(2 3 and.34 =(34 be 
given. Compute .12 ..23, .12 ..23 .. 
.1 
12, and .12 ..23 ..34. 

(b) Let n .4and. 
=(1234 .Determine . 
j for j=234 5. 
(c) Showthat theinverseof thecycle (i1 ir is givenby (ir i1. 
(d) Showthattwocycleswith disjoint elements,i.e.(i1 ir and ( 
j1 js 
with {i1 ir}.{j1 js}=O, commute. 
(e) Showthatevery permutation . 
.Sn canbe writtenasproductof disjoint 
cycles that are,except forthe order, uniquely determinedby .. 
7.2ProveLemma7.10 (1)using(7.1). 
7.3 Showthat thegroup homomorphism sgn 
: 
(Sn .>({1 .1}· 
satisfies the 
following assertions: 
(a) The set An ={. 
.Sn |sgn(. 
=1}isasubgroup of Sn (cp. Exercise 3.8). 
(b) For all . 
.An and . 
.Sn we have . 
.. 
...1 .An. 
7.4Computethe determinantsofthe following matrices: 
(a) 
A=[en en.1 e1 ].Znn, where ei is the ith columnofthe identity 
matrix. 
(b) 
B = 
bij .Znn with 
. 


.
2 for |i .j|=0 

. 


bij =.1for |i .j|=1 

. 


.

0 for |i .j|.2 

(c) 
. 
. 


10 1 0000

. 


. 
e 
0 e. 
451 . 
. 


. 
vvv. 
. 


. 
21 17 
.

e678 10 

. 
31 
. 


. 
30 ..e 
. 
77 

C = 
ee . 
e 0 .R

. 
. 


.

.
e4 010001 0 ..10 e2. 


. 
. 
. 


. 
60 
.

e2000 .1 

00 1 0000 


7 DeterminantsofMatrices 

(d) The 4 . 
4 Wilkinson matrix6 (cp. the MATLAB-Minute at the end of 
Sect. 7.2). 
7.5 Construct matrices AB . 
Rnn for some n . 
2 and with det(A + 
B 

= 
det( 
A +det(B . 
7.6 Let R be a commutative ring with unit, n . 
2 and A . 
Rn n. Show that the 
following assertions hold: 
(a) adj(In = 
In. 
(b) adj(AB =adj(B adj(A ,if Aand B . 
Rn n areinvertible. 
(c) adj(.A =.n.1adj(A for all . 
. 
R. 
T
(d) adj(AT =adj( 
A . 
(e) det(adj(A =(det(An.1,if Ais invertible. 
(f) adj(adj( 
A =det(An.2A. 
(g) adj(A.1 =adj( 
A .1,if Ais invertible. 
Can onedroptherequirementofinvertibilityin(b)or(e)? 

1

7.7 Let n . 
2 and A =[aij . 
Rnn with aij = 
for some x1 xn,
xi+yj 

y1 yn .R.Hence, in particular, xi +yj 
=0for allij.(Suchamatrix A 
is called a Cauchy matrix.7) 

(a) Showthat 
1.i< 
j.n (xj .xi )(yj . 
yi

det( 
A = 


n 
ij=1 (xi +yj 

(b)Use(a)toderivea formulaforthe determinantofthe n .nHilbertmatrix 
(cp. theMATLAB-Minute above Definition 5.6). 
7.8 Let Rbe a commutative ring with unit. If .1 .n . 
R, n .2, then 
.. 


.n.1

1 .1 ··· 


1 
. 
".
1 .2 ··· 
.n.1 
. 
Vn := 
. 
j.1 = 
.. 
2 
.. 
. 
Rn n 

i 

.. 


.n.1

1.n ··· 


n 

8

is called a Vandermonde matrix. 

(a) Showthat 
det(Vn = 
(.j ..i 
1.i< 
j.n 

6JamesHardyWilkinson(1919–1986). 
7AugustinLouisCauchy(1789–1857). 
8Alexandre-TheophileVandermonde (1735–1796). 


7.3 Minorsand theLaplace Expansion 
99 
(b) Let Kbea field and let K[t .n.1be thesetof polynomialsin thevariable 
tof degree at most n.1.Showthattwo polynomials pq.K[t .n.1 are 
equalif thereexist pairwisedistinct .1 .n .Kwith p(. 
j =q(.j . 
7.9 Showthe following assertions: 
(a) Let Kbea fieldwith1 +1
=0and let A.Kn n with AT =.A.If nis 
odd, then det(A =0. 
(b) If A.GLn(R with AT =A.1,then det(A .{1 .1}. 
7.10 Let Kbe a field and 
. 
A11 A12
A= 


A21 A22 

for some A11 . 
Kn1 n1, A12 . 
Kn1 n2, A21 . 
Kn2 n1, A22 . 
Kn2 n2. Show the 
following assertions: 

(a) If A11 .GLn1(K ,then det(A =det(A11 det A22 .A21 A.1.
11 A12 

(b) If A22 .GLn2(K ,then det(A =det( 
A22 det A11 .A12 A.1.
22 A21 

(c) If A21 =0, then det(A =det(A11 det( 
A22 . 
Can you show this also when thematrices are definedovera commutative ring 
with unit? 

7.11 Construct matrices A11 A12 A21 A22 .Rnn for n .2with 
A11 A12

det 
=det( 
A11 det(A22 .det(A12 det( 
A21

A21 A22 

7.12 Let 
A =[aij . 
GLn(R with aij . 
Z for ij = 
1 n. Show that the 
following assertions hold: 
(a) A.1 .Qnn . 
(b) A.1 .Znn if and onlyifdet(A .{.11}. 
(c) The linear system of equations Ax =bhas a unique solution 
'
x .Zn 1 for 
every b.Zn 1 if and onlyifdet(A .{.11}. 
7.13 Showthat G={A.Znn |det( 
A .{.11}}is a subgroup of GLn(Q . 

Chapter8 
The CharacteristicPolynomial 
and Eigenvalues of Matrices 

Wehavealready characterized matrices usingtheirrankand their determinant.Inthis 
chapterweusethe determinantmapinordertoassigntoeverysquarematrixaunique 
polynomialthatis calledthe characteristic polynomialofthe matrix.This polynomial 
contains important information about thematrix.Forexample, one can readoffthe 
determinant and thus seewhether thematrixisinvertible.Evenmoreimportant are 
the roots of the characteristic polynomial, which are called the eigenvalues of the 
matrix. 

8.1 
TheCharacteristicPolynomial 
andthe Cayley-Hamilton Theorem 
Let R be a commutative ring with unit and let R[t be the corresponding ring of 
polynomials (cp. Example 3.17).For A=[aij . 
Rn,n we set 

. 
. 


t .a11 .a12 ··· 
.a1n 

. 
.. 
. 


.. 

. 
. 
.

.a21 t .a22 . 

. 
.

tIn . 
A:= 
.(R[t )n,n . 

. 
. 
.

.. 

. 
.. 

... . .an.1,n . 
.an1 ··· 
.an,n.1 t .ann 

The entries of the matrix tIn . 
A are elements of the commutative ring with unit 
R[t , where the diagonal entries are polynomials of degree 1, and the other entries 
are constant polynomials.Using Definition 7.4wecanformthe determinantofthe 
matrix tIn . 
A, which is an element of R[t . 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_8 

8 The CharacteristicPolynomialand EigenvaluesofMatrices 

Definition 8.1 Let Rbe a commutative ring with unit and A. 
Rn,n.Then 
PA := 
det(tIn . 
A) . 
R[t 

is calledthe characteristic polynomial of A. 

Example8.2 If n = 
1and A=[a11 ,then 

PA = 
det(tI1 . 
A)= 
det([t . 
a11 )= 
t . 
a11. 
For n = 
2and 

a11 a12

A= 


a21 a22 

we obtain 

t . 
a11 .a12

PA = 
det = 
t2 . 
(a11 + 
a22)t + 
(a11a22 . 
a12a21)..a21 t . 
a22 

UsingDefinition 7.4 we seethat the general formof PA for a matrix A. 
Rn,n is 
givenby 

n 

PA = 
sgn(.) .i,.(i)t . 
ai,.(i). (8.1) 
..Sn i=1 

Thefollowing lemma presents basic propertiesofthe characteristic polynomial. 

Lemma 8.3 For A . 
Rn,n we havePA = 
PAT and 

PA = 
tn . 
.n.1tn.1 + 
... + 
(.1)n.1.1t + 
(.1)n.0 

n

with .n.1 ==1 aii and .0 = 
det(A).

i

Proof Using(5)in Lemma 7.10 we obtain 
T. 
AT

PA = 
det(tIn . 
A)= 
det((tIn . 
A))= 
det(tIn )= 
PAT . 

Using PA asin(8.1)wesee that 
nn 

PA = 
(t . 
aii ) + 
sgn(.) .i,.(i)t . 
ai,.(i). 

i=1 ..Sn i=1 

.=[1··· 
n 


8.1 The Characteristic Polynomialand theCayley-Hamilton Theorem 103 
Thefirsttermonthe right hand sideisof theform 

n 

tn . 
aii tn.1 + 
(polynomial ofdegree .n .2), 
i=1 

n

and thesecond termisa polynomialofdegree .n .2. Thus,.n.1 = 
1 aii as

i=

claimed. Moreover, Definition 8.1 yields 

PA(0)=det(.A)=(.1)n det(A), 

so that .0 =det(A). . 


This lemma shows that the characteristic polynomial of A . 
Rn,n always is of 
degree n.The coefficient of tn is1 . 
R. Sucha polynomialis called monic.The 
coefficient of tn.1isgivenbythe sumofthe diagonal entriesof A.This quantity is 
calledthe trace of A, i.e., 

n 

trace(A):= 
aii . 
i=1 

Thefollowing lemma showsthat forevery monic polynomial p . 
R[t of degree 
n .1there exists a matrix A.Rn,n with PA = 
p. 

Lemma 8.4 If n . 
N and p = 
tn +.n.1tn.1 +... +.0 . 
R[t , then p is the 
characteristic polynomialofthe matrix 

.. 


0 ..0 

. 
.. 
. 


. 
.. 
.

1 .. 
.. 
. 
Rn,n

A= 
. 

..

. 

. 
. 
.

. 0 ..n.2 
1..n.1 

(Forn =1we haveA=[..0 .) The matrixAis calledthe companion matrix of p. 

Proof We prove theassertionbyinductiononn. 
For n =1wehave p =t +.0, A=[..0 and PA =det([t +.0 )= 
p. 
Letthe assertion holdfor some n .1.We consider p =tn+1 +.ntn +... +.0 

and 

.. 


0 ..0 

. 
.. 
. 


. 
.. 
.

1 .. 
.. 
. 
Rn+1,n+1

A= 
. 

..

. 

. 
. 
.

. 0..n.1 
1 ..n 


8 The CharacteristicPolynomialand EigenvaluesofMatrices 

Using the Laplace expansion with respect to the first row (cp. Corollary 7.22)and 
theinductionhypothesis we get 

PA = 
det(tIn+1 . 
A) 

.. 
.. 


t .0 

.. 
. 
..

. 

.. 
.. 
..

.1 .. 

.. 
.. 


= 
det 

.. 
..

. 

.. 
. 
. 


. t .n.1 . 
.1t + 
.n 


.. 
.. 
.... 


t .1 .1 t 

.. 
. 
.. 
.. 
. 
..

.. 

.. 
.. 
.. 
.. 
.. 
..

.1 .. .. 

.. 
.. 
.... 


= 
t · 
det + 
(.1)n+2 · 
.0 · 
det 

.. 
.. 
....

.. 

.. 
. 
.. 
.. 
. 
..

. t .n.1 . t 
.1t + 
.n .1 

= 
t · 
(tn + 
.ntn.1 + 
... + 
.1)+ 
(.1)2n+2.0 

= 
tn+1 + 
.ntn + 
... + 
.1t + 
.0 

= 
p. . 


Example8.5 The polynomial p = 
(t . 
1)3 = 
t3 . 
3t2 + 
3t . 
1 . 
Z[t has the 
companion matrix 

.. 


00 1 

3,3 

..

A= 
10 .3 . 
Z. 

01 3 

Theidentity matrix I3 has the characteristic polynomial 

3

PI3 = 
det(tI3 . 
I3)= 
(t . 
1)= 
PA. 

Thus,different matrices mayhavethe same characteristic polynomial. 

In Example3.17 wehave seenhowtoevaluatea polynomial p . 
R[t at a scalar 
. 
. 
R.Analogously, we can evaluate p at a matrix M . 
Rm,m (cp. Exercise 4.8). 
For 

p = 
.ntn + 
.n.1tn.1 + 
... + 
.0 . 
R[t 

we define 

p(M):= 
.nMn + 
.n.1Mn.1 + 
... + 
.0Im . 
Rm,m , 

wherethe multiplicationontherighthandsideisthescalar multiplicationof.j . 
R 
and Mj . 
Rm,m , j = 
0,1,...,n.(Recall that M0 = 
Im.) Evaluating a given 
polynomial atmatrices M . 
Rm,m therefore defines a map from Rm,m to Rm,m . 


8.1 The Characteristic Polynomialand theCayley-Hamilton Theorem 105 
In particular,using(8.1), the characteristic polynomial PA of A. 
Rn,n satisfies 

n 

PA(M)= 
sgn(.) .i,.(i)M.ai,.(i)Im for all M . 
Rm,m . 

..Sn i=1 

Note that for M . 
Rn,n and PA = 
det(tIn . 
A)the “obvious” equation PA(M)= 
det(M . 
A)is wrong.Bydefinition, PA(M). 
Rn,n and det(M . 
A). 
R, so that 
thetwoexpressions cannotbe thesame,even for n =1. 

Thefollowing resultis calledthe Cayley-Hamiltontheorem.1 

Theorem 8.6 For every matrix A . 
Rn,n and its characteristic polynomial PA . 
R[t we havePA(A)=0. 
Rn,n. 

Proof For n = 
1wehave A =[a11 and PA = 
t .a11, so that PA(A)=[a11 . 
[a11 ]=[0. 

Letnow n .2and letei be the ith columnofthe identity matrix In . 
Rn,n.Then 

Aei =a1ie1 +a2ie2 +... +anien, i =1,...,n, 

whichis equivalent to 

n 

(A.aiiIn)ei + 
(.ajiIn)ej =0, i =1,...,n. 

j=1 
j=i 

Thelast n equations can be written as 

A.a11 In . 
.a21 In ··· 
.an1In e1 . 
. 
. 
. 
0 
. 
.a12 In A.a22 In ··· 
.an2In . 
.
e2. 
.
0. 
. 
. 
. . . . 
. 
. 
. 
. . 
. 
= 
. 
. 
. . 
. 
, or B. 
= 
0. 
. . . . . 
. 
. 
. . . . 
. 
. . 
. 
.. 
.a1n In .a2n In ··· 
A.ann In en 0 

Hence B .(R[A )n,n with R[A ]:={p(A)|p . 
R[t ]}. 
Rn,n.The set R[A forms 
acommutativering with unitgivenbytheidentity matrix In (cp. Exercise 4.8).Using 
Theorem 7.18 we obtain 

adj(B)B =det(B)In,



1Arthur Cayley(1821–1895) showed this theoremin1858 for n =2and claimedthathehadverified 
it for n =3.Hedidnotfeelit necessarytogiveaproofforgeneral n.SirWilliam RowanHamilton 
(1805–1865) provedthe theoremfor the case n = 
4in 1853inthecontextofhisinvestigationsof 
quaternions.Oneof thefirstproofsfor general n wasgivenby FerdinandGeorgFrobenius (1849– 
1917)in 1878. JamesJosephSylvester (1814–1897) coined thenameof the theoremin 1884by 
calling it the “no-little-marvelous Hamilton-Cayleytheorem”. 


8 The CharacteristicPolynomialand EigenvaluesofMatrices 



where det(B). 
R[A and In is theidentity matrixin (R[A )n,n.(This matrix has n 

times theidentity matrix In on its diagonal.) Multiplyingthis equationfromthe right 
by . 
yields 

adj(B)B. 
= 
det(B)In., 

whichimpliesthat det(B)= 
0. 
Rn,n.Finally, using Lemma8.3 gives 

n 

0= 
det(B)= 
sgn(.)(.i,.(i)A. 
a.(i),iIn) 

..Sn i=1 

n 

= 
sgn(.)(..(i),iA. 
a.(i),iIn) 

..Sn i=1 

= 
PAT (A) 

= 
PA(A), 

which completes theproof. . 


8.2 Eigenvalues and Eigenvectors 
In this sectionwepresent an introductiontothe topicofeigenvalues and eigenvectors 
of square matrices over a field K.These conceptswillbe studiedin more detailin 
later chapters. 

Definition 8.7 Let A. 
Kn,n.If. 
. 
K and v . 
Kn,1 \{0} 
satisfy Av = 
.v,then . 
is called an eigenvalue of Aand v is called an eigenvector of Acorresponding to .. 

While by definition v = 
0canneverbeaneigenvectorofamatrix, . 
= 
0may be 
an eigenvalue.Forexample, 

1.11 1 

= 
0 .

.111 1 

If v is an eigenvector corresponding to the eigenvalue . 
of Aand . 
. 
K \{0},then 
.v 
= 
0and 

A(.v) = 
. 
(Av) = 
. 
(.v) = 
. 
(.v). 

Thus, also .v is an eigenvector of Acorresponding to .. 


8.2 
Eigenvalues andEigenvectors 107 
Theorem 8.8 For A.Kn,n thefollowing assertions hold: 

(1) 
. 
is aneigenvalueofAifand onlyif. 
isarootofthecharacteristic polynomial 
ofA, i.e.,PA(.)=0.K. 
(2) 
. 
=0is aneigenvalueofAifand onlyifdet(A)=0. 
(3) 
. 
is aneigenvalueofAifand onlyif. 
is an eigenvalue of AT . 
Proof 

(1) The equation PA(.)=det(.In .A)=0holdsifandonlyifthe matrix.In .A 
is not invertible (cp.(7.4)), and this is equivalent to L(.In .A, 0) ={0}. 
This,however,means that thereexistsavector 

x =0with (.In .A)

x =0, or 
A

x =.
x. 
(2) By(1), . 
=0isaneigenvalueof Aif and onlyif PA(0)=0. Theassertionnow 
follows from PA(0)=(.1)n det(A)(cp. Lemma 8.3). 
(3)Thisfollows from(1) and PA =PAT (cp. Lemma 8.3). 
. 

Whether a matrix A .Kn,n has eigenvalues or not may depend on the field K 
over which Ais considered. 

Example8.9 The matrix 

01 

2,2

A=.R

.10 

has the characteristic polynomial PA =t2 +1 .R[t .This polynomial does not 
have roots, since the equation t2 +1=0has no (real)solutions.Ifwe consider Aas 
an element of C2,2,then PA .C[t has the roots iand .i.Then thesetwo complex 
numbers are the eigenvalues of A. 

Item (3) in Theorem8.8 shows that A and AT have the same eigenvalues. An 
eigenvector of A,however, may not be an eigenvector of AT . 

Example8.10 The matrix 

33 

2,2

A=.R

11 

has the characteristic polynomial PA =t2.4t =t·(t.4),and hence its eigenvalues 

are 0and 4. We have 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
A 
1 
.1 =0 
1 
.1 
and AT 1 
.1 = 
2 
2 =. 
1 
.1 


8 The CharacteristicPolynomialand EigenvaluesofMatrices 

for all . 
. 
R.Thus, [1, .1 T is an eigenvector of A corresponding to the eigenvalue0,
butitis not an eigenvectorof AT.Onthe other hand, 

11 1 .61

AT = 
0 and A = 

= 
.

.3 .3 .3 .2 .3 

for all . 
. 
R.Thus, [1, .3 T is an eigenvector of AT corresponding to the eigenvalue0,
butitis not an eigenvectorof A. 

Theorem8.8 impliesfurther criteriafor theinvertibilityof A. 
Kn,n (cp.(7.3)): 

A. 
GLn(K). 
0is not an eigenvalue of A 
. 
0is not a root of PA. 

Definition 8.11 TwomatricesA, B . 
Kn,n are called similar,ifthereexistsamatrix 
Z . 
GLn(K)with A= 
ZBZ.1. 

One can easily show that this defines an equivalence relationonthe set Kn,n (cp. 
theproof following Definition 5.13). 

Theorem 8.12 If two matrices A, B . 
Kn,n aresimilar, thenPA = 
PB. 

Proof If A= 
ZBZ.1,then themultiplicationtheorem for determinantsyields 

PA = 
det(tIn . 
A)= 
det(tIn . 
ZBZ.1)= 
det(Z(tIn . 
B)Z.1) 
= 
det(Z)det(tIn . 
B)det(Z.1)= 
det(tIn . 
B) det(ZZ.1) 
= 
PB 

(cp. the remarks below Theorem 7.15). . 


Theorem8.12and(1)inTheorem 8.8showthattwosimilarmatriceshavethesame 
eigenvalues.The conditionthat Aand B aresimilarissufficient,but not necessary 
for PA = 
PB. 

Example8.13 Let 

11 10

A= 
, B == 
I2.

01 01 

Then PA = 
(t . 
1)2 = 
PB,but for every matrix Z . 
GLn(K)we have ZBZ.1 = 
I2 
= 
A. Thus, we have PA = 
PB although A and B are not similar(cp. also 
Example 8.5). 


8.2 Eigenvalues andEigenvectors 109 
MATLAB-Minute. 

The roots ofa polynomial p =.ntn +.n.1tn.1 +... +.0 can be computed 
(or approximated) in MATLAB using the command roots(p), where pis a 
1.(n+1)matrix with the entries p(i)=.n+1.i for i =1,...,n+1.Compute 
roots(p) forthe monic polynomial p =t3 .3t2 +3t .1.R[t and display 
the output using format long.What are the exact roots of p and howlarge 
is the numerical errorinthe computationofthe rootsusing roots(p)? 
Form the matrixA=compan(p) and compare its structure with the one of the 
companion matrix fromLemma 8.4.Can you transferthe proofof Lemma8.4 
to thestructureofthe matrix A? 
Compute the eigenvalues of A with the command eig(A) and compare the 
output with the one of roots(p).What do you observe? 

8.3 EigenvectorsofStochasticMatrices 
We now consider the eigenvalue problem presented in Sect.1.1 in the context of 
thePageRank algorithm. Themathematical modelingleads to the equations(1.1), 
which can be writteninthe form Ax =x.Here A =[aij ].Rn,n (n is the number 
of documents) satisfies 

n 

aij .0 and aij =1for j=1,...,n. 
i=1 

Such a matrix A is called column-stochastic. Note that A is column-stochastic if 
and onlyif AT is row-stochastic. Such matrices also occurredinthe car insurance 
application considered in Sect. 1.2 and Example 4.7. We want to determine x = 
[x1,...,xn 
T .Rn,1 \{0}with Ax =x,wherethe entry xi describes theimportance 
of document i.The importancevalues shouldbe nonnegative, i.e., xi .0for i = 
1,...,n.Thus, we want to determine an entrywise nonnegative eigenvector of A 
corresponding to the eigenvalue . 
=1. 

We first check whether this problem has a solution, and then study whether the 
solutionis unique. Ourpresentationis based on thearticle [BryL06]. 

Lemma 8.14 Acolumn-stochastic matrixA.Rn,n has an eigenvector correspondingtothe 
eigenvalue1. 

Proof Since Ais column-stochastic,wehave AT[1,...,1 T =[1,...,1 T,sothat1 
is an eigenvalueofAT.Now(3)inTheorem8.8showsthat also Ahas theeigenvalue 
1, and hence there exists a corresponding eigenvector. . 



8 The CharacteristicPolynomialand EigenvaluesofMatrices 

Amatrix with real entries is called positive,ifall its entries are positive. 

Lemma 8.15 If A .Rn,n is positive and column-stochastic and if x .Rn,1 is an 
eigenvector of Acorresponding to the eigenvalue 1, then either x or .xis positive. 

Proof If x =[x1,..., xn 
T is an eigenvector of A =[aij corresponding to the 
eigenvalue1,then 

n 

xi = 
aijxj, i =1,...,n. 
j=1 

Supposethat not all entriesof x are positive or not all entries of x arenegative. Then 
thereexistsatleast one index k with 

nn 

. 




|xk|= 
akj xj < akj |xj|, 

j=1 j=1 

whichimplies 

nnn nn n 
. 
n 
. 
n 

|xi|< aij |xj|= 
aij |xj|= 
|xj|· 
aij =|xj|. 
i=1 i=1 j=1 j=1 i=1 j=1 i=1 j=1 

. 
. 
. 


=1 

This is impossible, so that indeed x or .x must be positive. . 


We can nowprove thefollowing uniquenessresult. 

Theorem 8.16 If A . 
Rn,n is positive and column-stochastic, then there exists a 

n

unique positive x =[x1,...,xn 
T .Rn,1 with xi =1and Ax =x.

i=1 

Proof By Lemma8.15, Ahasaleastonepositiveeigenvector correspondingtothe 

 T  T

(1)(1)(1)(2)(2)(2)
eigenvalue 1. Suppose that x= 
x1 ,..., xand x= 
x1 ,...,x

nn 

n (j)

are two such eigenvectors. Suppose that these are normalized by = 
1,

i=1 xi 
j = 
1,2. This assumption can be made without loss of generality, since every 
nonzero multiple of an eigenvector is still an eigenvector. 

(1)(1)+.x(2).Rn,1

We will show thatx=x(2).For . 
.Rwe define x(.):=x, 
then 

(1)+.x(2)

Ax(.)=Ax(1)+.Ax(2)=x=x(.). 

(1)(2)
If 
!
. 
:= 
.x1 /x1 , then the first entry of x(
!.)is equal to zero and thus, by 
Lemma 8.15, x(
!

.)cannot be an eigenvector of Acorresponding to theeigenvalue1. 
Now Ax(
!=x(.)impliesthat x(
!.)

.)
!=0, and hence 

(1)(2) 
!

xi +.xi =0, i =1,...,n. (8.2) 


8.3 EigenvectorsofStochasticMatrices 111 
Summing up these n equations yields 

nn 

(1)(2)
xi +!
. 
xi =0, 
i=1 i=1 

. 
. 


=1 =1 

(1)(2)
so that 
!
1. From(8.2)we get xi =xi for i

. 
=.=1,...,n, and therefore 

(1)(2)

x=x. . 


The unique positive eigenvector x in Theorem8.16is calledthe Perron eigenvector2 
of the positive matrix A.The theoryofeigenvalues and eigenvectorsofpositive 
(ormore general nonnegative)matricesisan important areaof Matrix Theory, since 
thesematrices ariseinmanyapplications. 

By construction, the matrix A .Rn,n in the PageRank algorithm is columnstochasticbut 
not positive,since thereare (usually many)entries aij =0. In order 
to obtaina uniquelysolvableproblem one can usethe following trick: 

Let S =[sij ].Rn,n with sij =1/n. Obviously, S is positive and columnstochastic.
Forareal number . 
.(0,1 we define the matrix 



A(.):=(1..)A+.S. 

This matrix is positive and column-stochastic, and hence it has a unique positive 
eigenvector 

u correspondingto theeigenvalue1.Wethushave 

. 


T 




u =A(.)

u =(1..)A

u +.S

u =(1..)A

u +[1,...,1 . 

n 

Foravery large numberof documents(e.g. the entireinternet)the number./n is 
very small, so that (1..)A

u .u.Thereforeasolutionofthe eigenvalueproblem 



A(.)

u =
u for small . 
potentially gives a good approximation ofa u .Rn,1 that 
satisfies Au =u.The practical solutionofthe eigenvalueproblem with thematrix 



A(.)isatopicofthe fieldofNumerical Linear Algebra. 
The matrix Srepresents a link structure where all document are mutually linked 
and thus all documentsare equally important.The matrix A(.)=(1..)A+.S



thereforemodelsthe following internet “surfing behavior”:Auserfollowsaproposed 
linkwith theprobability1.. 
andan arbitrarylinkwiththeprobability ..Originally, 
GoogleInc. used thevalue . 
=0.15. 

2OskarPerron(1880–1975). 


8 The CharacteristicPolynomialand EigenvaluesofMatrices 

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

8.1Determine the characteristic polynomialsof thefollowing matricesover Q: 
.. 


20 .1

20 4421 

..

A= 
, B = 
, C = 
, D = 
02 0 .

02 .10 02 

.40 2 

Verify theCayley-Hamiltontheorem in each casebydirect computation. Are 
twoofthe matrices A,B,C similar? 

8.2 Let Rbe a commutative ring with unit and n . 
2. 
(a) Show that for every A . 
GLn(R)there exists a polynomial p . 
R[t of 
degree at most n . 
1 with adj(A)= 
p(A).Conclude that A.1 = 
q(A) 
holds for a polynomial q . 
R[t of degree at most n . 
1. 
n,n

(b) Let 
A . 
Rn,n. Apply Theorem7.18 to the matrix tIn . 
A . 
(R[t )
and deriveanalternativeproofoftheCayley-Hamiltontheoremfromthe 
formula det(tIn . 
A)In = 
(tIn . 
A)adj(tIn . 
A). 
8.3 Let 
A . 
Kn,n beamatrixwith Ak = 
0for some k . 
N.(Such a matrixis 
called nilpotent.) 
(a) Showthat . 
= 
0isthe onlyeigenvalueof A. 
(b) Determine PA and show that An = 
0. 
n 
. 


(Hint: Youmay assume thatPA has theform (t..i)forsome .1,...,.n 
i=1 

. 
K.) 

(c) Showthat .In . 
Aisinvertibleif and onlyif . 
. 
K \{0}. 
(d) Showthat (In . 
A).1 = 
In + 
A+ 
A2 + 
... + 
An.1. 
8.4 Determine the eigenvalues and corresponding eigenvectors of the following 
matrices over R: 
.. 


... 
. 


0.100

111 3816 

..

1000 

.. 


... 
.

A= 
011 , B = 
078 , C = 
. 

..

00 .21

001 0.4.5 

000 .2 

Is there anydifference when you consider A,B,C as matrices over C? 

8.5 Let n . 
3and. 
. 
R.Consider the matrix 
.. 


11 

. 
. 
.

. 

. 
. 

. 
. 
.

. 

..

A(.)= 


..

. 

. 
. 
.

. 1 
. 
1 


8.3 
EigenvectorsofStochasticMatrices 113 
as an element of Cn,n and determine all eigenvalues in dependence of ..How 
manypairwisedistinct eigenvalues does A(.)have? 

8.6 Determine the eigenvalues and corresponding eigenvectors of 
. 
... 


22 . 
a 2. 
a 
110 

3,3 
3,3 

. 
...

A= 
04 . 
a 2. 
a . 
R, B= 
101 . 
(Z/2Z). 
0.4+ 
2a.2+ 
2a 011 

(For simplicity,the elementsof Z/2Z are here denoted by kinstead of [k.) 

8.7 Let 
A. 
Kn,n , B . 
Km,m , n . 
m, and C . 
Kn,m with rank(C)= 
m and 
AC = 
CB. Showthat theneveryeigenvalueof Bis an eigenvalue of A. 
8.8 Showthe following assertions: 
(a) trace(.A+ 
.B)= 
. 
trace(A)+ 
. 
trace(B)holds for all .,. 
. 
Kand 
A,B. 
Kn,n . 
(b) trace(AB)= 
trace(BA)holds for all A,B. 
Kn,n . 
(c) If A,B. 
Kn,n are similar, then trace(A)= 
trace(B). 
8.9Prove or disprove thefollowing statements: 
(a) There exist matrices A,B. 
Kn,n with trace(AB)
= 
trace(A)trace(B). 
(b) There exist matrices A,B. 
Kn,n with AB. 
BA= 
In. 
8.10 Suppose that the matrix 
A =[aij . 
Cn,n has only real entries aij. Show 
that if . 
. 
CR is an eigenvalue of Awith corresponding eigenvector v = 
[.1,...,.nT . 
Cn,1, then also . 
is an eigenvalue of Awith corresponding 
T

eigenvector v:= 
[.1,...,.n . 


Chapter9 
Vector Spaces 

Intheprevious chapterswehavefocussedonmatricesand theirproperties.Wehave 
defined algebraic operations with matrices and derived important concepts associated 
with them, including their rank, determinant, characteristic polynomial, and 
eigenvalues.Inthis chapter we place these conceptsinamore abstract framework 
by introducingtheideaofavector space. Matrices form oneofthemostimportant 
examples of vector spaces, and properties of certain (namely, finite dimensional) 
vector spaces canbe studiedinatransparentwayusing matrices.Inthenext chapter 
we will study (linear) maps between vector spaces, and there the connection with 
matrices will play a central role as well. 

9.1 BasicDefinitionsandPropertiesofVector Spaces 
We begin with the definition ofa vector space over a field K. 

Definition 9.1 Let K bea field.A vector spaceoverK , or shortly K-vector space, 
is a set V with two operations, 

+:V .V >V ,(v 
w 
>v 
+w 
(addition) 

·:K .V >V ,(. 
v 
>. 
·v 
(scalar multiplication) 

that satisfy thefollowing: 

(1) (V +is a commutative group. 
(2) For all v,w 
.V and .. 
.K thefollowing assertions hold: 
(a) . 
·(. 
·v 
=(.. 
·v 
(b) 1·v 
=v 
(c) . 
·(v 
+w 
=. 
·v 
+. 
·w 
(d) (. 
+. 
·v 
=. 
·v 
+. 
·v 
©SpringerInternationalPublishing Switzerland 2015 115 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_9 

9 Vector Spaces 

An element v 
. 
V is called a vector,1 an element .. 
K is called a scalar. 

Again, we usually omit thesignofthe scalar multiplication, i.e., we usually write 
.v 
instead of .· 
v.Ifitisclear fromthe context(or not important)which fieldwe 
are using, we often omit the explicit reference to K and simply write vector space 
instead of K-vector space. 

Example9.2 

(1) The set Kn m with the matrix addition and the scalar multiplication forms a 
m

K-vector space.Forobvious reasons,the elementsof Kn 1 and K1 aresometimes 
called column and row vectors, respectively. 

(2) The set K[t] 
forms a K-vector space, if the additionis defined as in Example3.17 
(usual addition of polynomials) and the scalar multiplication for 
p = 
.0 + 
.1t ++ 
.ntn . 
K[t] 
is defined by 
.· 
p := 
(..0 + 
(..1 t ++ 
(..n tn 

(3) The continuous and real valued functions defined on a real interval [..] 
with 
the pointwise addition and scalar multiplication, i.e., 
( 
f + 
g )(x := 
f(x + 
g(x and (.· 
f )(x := 
.f(x 

form an R-vector space. This canbe shownby using that the additionoftwo 
continuous functions as well as themultiplicationof a continuous functionby 
a real number yield again a continuous function. 

Since,bydefinition, (V + 
isacommutativegroup,we alreadyknowsomevector 
space propertiesfromthetheoryofgroups(cp.Chap. 3).In particular,everyvector 
space containsaunique neutralelement (with respectto addition)0V,which is called 
the null vector.Every vector v 
. 
V hasa unique (additive)inverse .v 
. 
V with 
v 
+ 
(.v 
= 
v 
. 
v 
= 
0V.As usual, we will write v 
. 
w 
instead of v 
+ 
(.w 
. 

Lemma 9.3 Let V beaK-vector space.If 0K and 0V arethe neutral(null) elements 
ofK and V,respectively, then thefollowing assertions hold: 

(1) 0K · 
v 
= 
0V for all v 
. 
V. 
(2) .· 
0V = 
0V for all .. 
K. 
(3) .(.· 
v 
= 
(.. 
· 
v 
= 
.· 
(.v 
for all v 
. 
V and .. 
K. 
1This termwasintroducedin 1845by SirWilliamRowan Hamilton (1805–1865)in thecontext of 
his quaternions.ItismotivatedbytheLatinverb“vehi”(“vehor”,“vectussum”)which meansto 
ride or drive. Also the term “scalar” was introduced by Hamilton; see the footnote on the scalar 
multiplication(4.2). 


9.1 Basic Definitions andPropertiesofVector Spaces 117 
Proof 

(1) For all v 
.V we have0K ·v 
=(0K +0K ·v 
=0K ·v 
+0K ·v.Adding .(0K ·v 
on bothsidesof this identitygives0V =0K ·v. 
(2) For all . 
.K we have.·0V =.·(0V +0V =.·0V +.·0V.Adding .(.·0V 
on bothsidesof this identitygives0V =. 
·0V. 
(3) For all . 
.K and v 
.V we have . 
·v 
+(.. 
·v 
=(. 
.. 
·v 
=0K ·v 
=0V, 
as well as . 
·v 
+. 
·(.v 
=. 
·(v 
.v 
=. 
·0V =0V. . 

In thefollowing we will write0insteadof0K and0V when it is clear which null 
element is meant. 

As in groups, rings and fields we can identify substructures in vector spaces that 
are again vector spaces. 

Definition 9.4 Let (V +· 
be a K-vector space and let U . 
V.If (U +· 
is a 
K-vector space, then it is called a subspace of (V +·. 

Asubstructure mustbeclosedwith respect to thegiven operations,which here 
are addition and scalar multiplication. 

Lemma 9.5 (U +· 
is a subspace of the K-vector space (V +· 
if and onlyif 
O=U .V and thefollowing assertions hold: 

(1) v 
+w 
.U for all v,w 
.U, 
(2) .v 
.U for all . 
.K andv 
.U. 
Proof Exercise. . 


Example9.6 

(1) Every vector space V has thetrivial subspaces U =V and U ={0}. 
(2) Let A . 
Kn m and U = 
L(A 0 . 
Km 1, i.e., U is the solution set of the 
homogeneous linear system Ax = 
0.Wehave0 . 
U,so U is not empty. If 
v,w 
.U,then 
A(v 
+w 
=Av 
+Aw 
=0+0=0 

i.e., v 
+w 
.U. Furthermore,for all . 
.K, 

A(. 
v 
=. 
(Av 
=. 
0=0 

i.e., .v 
.U.Hence, U is a subspace of Km 1. 

(3) For every n .N0 the set K[t].n :={p .K[t]|deg( 
p .n}is a subspace of 
K[t]. 
Definition 9.7 Let V be a K-vector space, n .N, and v1 ,...,vn .V.Avector of 
theform 

n 

.1v1 ++.nvn = 
.ivi . 
V 
i=1 


9 Vector Spaces 

is called a linear combination of v1 ,...,vn with the coefficients .1 .n . 
K. 
The (linear) span of v1 ,...,vn is the set 

. 
n 
. 


span{v1 ,...,vn}:= 
.ivi |.1 .n .K 

i=1 

Let M beaset and supposethat forevery m .M we have a vector vm .V.Let 
thesetofallthesevectors, calledthe system of thesevectors,be denotedby{vm}m.M. 
Then the (linear) span of the system {vm}m.M, denoted by span{vm}m.M,is defined 
as the set of all vectors v 
.V that are linear combinations of finitely manyvectors 
of the system. 

This definition can be consistently extended to the case n = 
0. In this case 
v1 ,...,vn isa listoflength zero, or an empty list.If we define the empty sum of 
vectors as0.V,then we obtain span{v1 ,...,vn}=spanO={0}. 

If in the following we consider a list of vectors v1 ,...,vn or a set of vectors 
{v1 ,...,vn}, we usually mean that n .1. The case ofempty list and the associated 
zero vector space V ={0}will sometimes be discussed separately. 

Example9.8 The vector space K13 ={[.1 .2 .3]|.1 .2 .3 . 
K}is spanned 
by the vectors [100][010][001].The set {[.1 .20]|.1 .2 .K}forms 
a subspace of K1 3 that is spanned by the vectors [100][010]. 

Lemma 9.9 If V is a vector space and v1 ,...,vn .V, then span{v1 ,...,vn}is a 
subspace of V. 

Proof It is clear thatO =span{v1 ,...,vn}.V. Furthermore, span{v1 ,...,vn}is 
by definitionclosedwith respectto additionand scalar multiplication,sothat(1)and 

(2)inLemma9.5 aresatisfied. . 

9.2 Bases andDimensionofVector Spaces 
We will nowdiscussthe centraltheoryofbases and dimensionofvector spaces, and 
start with the concept of linear independence. 

Definition 9.10 Let V be a K-vector space. 

(1) The vectors v1 ,...,vn .V are called linearly independent if the equation 
n 

.ivi =0 with .1 .n .K 
i=1 

n

always implies that .1 =···=.n =0. Otherwise, i.e., when i=1 .ivi =0 
holds for some scalars .1 .n . 
K that are not all equal to zero, then the 
vectors v1 ,...,vn are called linearly dependent. 


9.2 Bases andDimensionofVector Spaces 119 
(2) The empty listis linear independent. 
(3) If M is a set and for every m .M we have a vector vm .V,the corresponding 
system {vm}m.M is called linearly independent when finitely many vectors of 
thesystemarealways linearly independentin thesenseof(1).Otherwise the 
system is called linearly dependent. 
The vectors v1 ,...,vn are linearly independentif and onlyifthe zerovector can 
be linearly combined onlyinthe trivialway0=0·v1 ++0·vn.Consequently, 
if oneof thesevectorsisthe zerovector,then v1 ,...,vn are linearly dependent.A 
single vector v 
is linearly independentif and onlyifv 
=0. 

Thefollowing resultgivesa useful characterizationofthe linear independenceof 
finitely many(butat leasttwo)givenvectors. 

Lemma 9.11 The vectors v1 ,...,vn,n .2,are linearly independentif and onlyif 
no vector vi,i =1 n, canbe writtenasa linear combinationofthe others. 

Proof We prove theassertionbycontraposition. Thevectorsv1 ,...,vn are linearly 
dependent if and onlyif 

n 

.ivi =0 

i=1 

with at least one scalar .j =0. Equivalently, 

n 

v 
j =. 
(..j 
1.i )vi 

i=1 
i=j 

so that v 
j isa linear combinationofthe othervectors. . 


Using the concept of linear independence we can now define the concept of the 
basis of a vector space. 

Definition 9.12 Let V be a vector space. 

(1) A set {v1 ,...,vn}. 
V is called a basis of V, when v1 ,...,vn are linearly 
independent and span{v1 ,...,vn}=V. 
(2)ThesetOisthe basisofthe zerovector space V ={0}. 
(3) Let Mbeaset and supposethat forevery m .Mwe haveavector vm .V.The 
set {vm |m .M}is called a basis of V if the corresponding system {vm}m.M is 
linearly independent and span {vm}m.M =V. 
In short, a basis is a linearly independent spanning set of a vector space. 

Example9.13 

(1) Let Eij .Kn m be thematrixwith entry1inposition (ij and allother entries0 
(cp. Sect. 5.1). Then the set 

9 Vector Spaces 

{Eij |1.i .n and 1 . 
j.m} 
(9.1) 

isa basisof thevector space Kn m (cp.(1)inExample 9.2): Thematrices Eij . 
Kn m,1 .i .n and1 . 
j.m, are linearly independent, since 

nm 

0= 
.ijEij =[.ij]

i=1 j=1 

impliesthat .ij =0for i =1 n and j =1 m.For any A =[aij].Kn m we have 

nm 

A= 
aijEij 
i=1 j=1 

and hence 

span{Eij |1.i .n and 1 . 
j.m}= 
Kn m 

The basis(9.1)is calledthecanonical or standard basis of the vector space 
Kn m.For m =1we denotethe canonical basisvectorsof Kn 1 by 

.... 
.. 


10 0 

.... 
..

01 

.... 
.. 


.... 
..

00

e1 :=.. 
e2 :=.. 
en :=..

0 

.... 
.. 


.... 
..

0 
00 1 

Thesevectorsare also called unit vectors;theyarethe n columnsof theidentity 
matrix In. 

(2) A basis of the vector space K[t](cp. (2)inExample 9.2)isgivenby theset 
{tm |m .N0},since the corresponding system{tm}is linearly independent, 
m.N0 

andevery polynomial p .K[t]isalinear combinationoffinitely manyvectors 
of the system. 

Thenext resultis calledthe basis extensiontheorem. 

Theorem 9.14 Let V be a vector space and let v1 ,...,vr ,w1 ,...,w. 
.V, where 
r . 
.N0.If v1 ,...,vr arelinearly independent and span{v1 ,...,vr ,w1 ,...,w}=
V, then the set {v1 ,...,vr}can be extended to a basis of V using vectorsfromthe 
set {w1 ,...,w}. 

Proof Note that for r =0the listv1 ,...,vr is empty and hence linearly independent 
due to (2)inDefinition 9.10. 
We prove theassertionbyinductionon.If . 
=0, then span{v1 ,...,vr}=V, 
and the linear independence of {v1 ,...,vr}showsthat this setisa basisof V. 


9.2 Bases andDimensionofVector Spaces 121 
Letthe assertion holdfor some . 
.0. Supposethatv1 ,...,vr ,w1 ,...,w+1 .V aregiven, where v1 ,...,vr are linearly independent and span{v1 ,...,vr ,w1 
w+1}= 
V.If {v1 ,...,vr} 
already is a basis of V, then we are done. Suppose, 
therefore, that span{v1 ,...,vr}.V.Then thereexistsatleast one j,1 . 
j.. 
+1, 
such that w 
j .span{v1 ,...,vr}.In particular,wehave w 
j =0. Then 

r 

.w 
j + 
.ivi =0 

i=1 

implies that . 
= 
0 (otherwise we would have w 
j . 
span{v1 ,...,vr}) and, 
therefore, .1 = 
··· 
= 
.r = 
0 due to the linear independence of v1 ,...,vr. 
Thus, v1 ,...,vr w 
j are linearly independent. By the induction hypothesis we 
can extend the set {v1 ,...,vr w 
j} 
to a basis of V using vectors from the set 
{w1 ,...,w+1}\{w 
j}, which contains . 
elements. . 


Example9.15 Consider the vector space V = 
K[t].3 (cp.(3)inExample 9.6)and 
the vectors v1 = 
t, v2 = 
t2, v3 = 
t3. These vectors are linearly independent, 
but {v1 ,v2 ,v3} 
is not a basis of V, since span{v1 ,v2 ,v3} 

= 
V. For example, the 
vectors w1 = 
t2 +1 and w2 = 
t3 .t2 .1 are elements of V,but w1 ,w2 . 
span{v1 ,v2 ,v3}.Wehave span{v1 ,v2 ,v3 ,w1 ,w2}=V.Ifwe extend {v1 ,v2 ,v3}by 
w1,then we get the linearly independentvectors v1 ,v2 ,v3 ,w1 whichindeed span V. 
Thus, {v1 ,v2 ,v3 ,w1}is a basis of V. 

Bythebasisextensiontheoremeveryvectorspacethatisspannedbyfinitelymany 
vectors hasabasis consistingoffinitely manyelements.Acentralresultofthe theory 
of vector spaces is that every such basis has the same number of elements. In order 
to show this result we first prove thefollowing exchangelemma. 

m

Lemma 9.16 Let V bea vector space,let v1 ,...,vm .V and let w 
= 
i=1 .ivi . 
V with .1 =0. Then span{w,v2 ,...,vm}=span{v1 ,v2 ,...,vm}. 

Proof By assumptionwehave 

m 

=..1 ..1 

v11 w 
. 
1 .i vi 
i=2 

If y .span{v1 ,...,vm},say y = 
m 
i=1 .ivi, then 
. 
m 
. 
. 
. 
. 
m 
. 


..1 ..1

y =.11 w 
. 
1 .i vi + 
.ivi 
i=2 i=2 
m 

= 
.1..1 w 
+ 
.i ..1..11.i vi . 
span{w,v2 ,...,vm}

1 
i=2 

m

If, on the other hand, y =.1w 
+ 
2 .ivi .span{w,v2 ,...,vm},then

i=


9 Vector Spaces 

mm 

y =.1 .ivi + 
.ivi 

i=1 i=2 
m 

=.1.1v1 + 
(.1.i +.i vi . 
span{v1 ,...,vm}

i=2 

and thus span{w,v2 ,...,vm}=span{v1 ,v2 ,...,vm}. . 


Usingthislemma we nowprove the exchangetheorem.2 

Theorem 9.17 LetW ={w1 ,...,wn}andU ={u1 um}be finite subsets ofa 
vector space,and let w1 ,...,wn be linearly independent.IfW .span{u1 um}, 
thenn .m,andnelementsofU,ifnumberedappropriatelytheelementsu1 un, 
canbeexchangedagainstn elementsofW in suchaway that 

span{w1 ,...,wn un+1 um}=span{u1 un un+1 um} 


m

Proof By assumptionwehave w1 = 
i1 .iui for some scalars .1 .m that

=

are not all zero(otherwise w1 = 
0, which contradicts the linear independence of 
w1 ,...,wn). Afteran appropriate renumberingwehave .1 =0, and Lemma9.16 
yields 

span{w1 u2 um}=span{u1 u2 um} 


Supposethat forsome r,1 .r .n.1,wehaveexchanged thevectorsu1 ur 
against w1 ,...,wr so that 

span{w1 ,...,wr ur+1 um}=span{u1 ur ur+1 um} 


It is then clear that r .m. 
By assumptionwehave wr+1 .span{u1 um}, and thus 

rm 

wr+1 = 
.iwi + 
.iui 

i=1 i=r+1 

forsomescalars .1 .m.Oneof thescalars .r+1 .m mustbe nonzero(otherwise 
wr+1 . 
span{w1 ,...,wr}, which contradicts the linear independence of 
w1 ,...,wm). Afteranappropriate renumberingwehave.r+1 =0, and Lemma9.16 
yields 

span{w1 ,...,wr+1 ur+2 um}=span{w1 ,...,wr ur+1 um} 


If we continue this construction until r =n .1, then we obtain 

2Inthe literature,histheoremis sometimes calledthe Steinitzexchangetheorem afterErnst Steinitz 
(1871–1928).The resultwasfirstprovedin 1862by HermannGunther Gra.mann(1809–1877). 


9.2 Bases andDimensionofVector Spaces 123 
span{w1 ,...,wn un+1 um}= 
span{u1 un un+1 um} 


wherein particular n . 
m. . 


Usingthisfundamental theorem,the following result aboutthe unique numberof 
basis elementsisa simple corollary. 

Corollary 9.18 If a vector space V is spanned by finitely many vectors, then V has 
a basis consisting offinitely many elements, and any two bases of V have the same 
number of elements. 

Proof The assertion is clear for V ={0} 
(cp. (2) in Definition 9.12). Let V = 
span{v1 ,...,vm} 
with v1 
= 
0. By Theorem9.14, we can extend span{v1} 
using 
elements of {v2 ,...,vm} 
to a basis of V. Thus, V has a basis with finitely many 
elements. Let U := 
{u1 u} 
and W := 
{w1 ,...,wk} 
betwosuch bases.Then 

u} 
Theorem918

W . 
V = 
span{u1 =. 
k . 
. 
Theorem918

U . 
V = 
span{w1 ,...,wk} 
=. 
. 
. 
k 

and thus . 
= 
k. . 


We can now define thedimensionofavector space. 

Definition 9.19 If thereexistsa basisofa K-vector space V that consists of finitely 
manyelements, then V is called finite dimensional, and the unique number of basis 
elements is called the dimension of V. We denote the dimension by dimK(V or 
dim(V ,ifitisclear which fieldismeant. 

If V is not spannedby finitely manyvectors, then V is called infinite dimensional, 
and we write dimK(V =.. 

Note that the zerovector space V ={0} 
hasthebasisOandthusithas dimension 
zero(cp. (2)inDefinition 9.12). 

If V isafinite dimensionalvector spaceandif v1 ,...,vm . 
V with m > 
dim(V , 
then thevectors v1 ,...,vm mustbe linearly dependent.(If thesevectorswere linearly 
independent,thenwe couldextendthemviaTheorem 9.14toabasisof V thatwould 
containmorethan dim(V elements.) 

Example9.20 Thesetin(9.1)formsabasisofthevector spaceKn m.This basis has 
n · 
m elements, and hence dim(Kn m = 
n · 
m.Onthe other hand, thevector space 
K[t] 
isnotspannedby finitelymanyvectors(cp.(2)inExample9.13)andhenceit 
is infinite dimensional. 

Example9.21 Let V bethevector spaceof continuousandrealvalued functionson 
the real interval [01] 
(cp.(3)inExample 9.2). Definefor n = 
12 thefunction 

fn . 
V by 


9 Vector Spaces 

. 


1

0 x < 


. 
n+1 

. 


. 


. 


. 


. 
1 

. 
0 < 
x 

. 
n 
fn(x = 


1 11 

. 
2n(n +1 x .2n .x . 
1 + 


. 
n+12 nn+1 

. 


. 


. 


. 


. 


. 


11

.2n(n +1 x +2n +21 + 
< 
x . 
1 

2 nn+1 n 


k 

Every linear combination . 
jfj is a continuous function that has the value .j 
j=1 
. 
k 

11

at 1 + 
.Thus, the equation .jfj =0 .V implies that all . 
j must be 

2 jj+1 

j=1 
zero, so that f1 fk .V are linearly independent for all k .N.Consequently, 
dim(V =.. 

9.3 Coordinatesand Changesofthe Basis 
We will now study the linear combinations of basis vectors ofa finite dimensional 
vector space.In particular,wewill study what happens witha linear combinationif 
we change to another basis of the vector space. 

Lemma 9.22 If {v1 ,...,vn}isa basisofaK-vector space V,then for every v 
.V thereexist uniquely determined scalars .1 .n .K with v 
=.1v1++.nvn. 
Thesescalarsare calledthe coordinates of v 
with respect to the basis {v1 ,...,vn}. 

nn

Proof Let v 
= 
1 .ivi = 
1 .ivi for some scalars .i .i .K, i =1 n,

i=i=

then 

n 

0=v 
.v 
= 
(.i ..i )vi 
i=1 


9.3 Coordinatesand Changesofthe Basis 125 
The linear independence of v1 ,...,vn impliesthat .i = 
.i for i = 
1 n. . 


By definition,the coordinatesofavector dependonthegiven basis.In particular, 
they depend on theordering(or numbering)of the basisvectors. Becauseof this, 
some authorsdistinguish between the basis as “set”, i.e.,a collection of elements 
withouta particular ordering,andan “ordered basis”.Inthisbookwewillkeepthe 
set notationfora basis {v1 ,...,vn}, where the indices indicate the ordering ofthe 
basis vectors. 

Let Vbea K-vector space, v1 ,...,vn . 
V(theyneed not be linearly independent) 
and 

v 
= 
.1v1 ++ 
.nvn 

for some coefficients .1 .n . 
K. Let us write 
. 
. 
.1 
(v1 ,...,vn . 
. 
. 
. 
:= 
.1v1 + 
+ 
.nvn (9.2) 
.n 

Here (v1 ,...,vn is an n-tuple over V, i.e., 

(v1 ,...,vn . 
Vn = 
V .. 
V 

. 
. 
. 


n times 

For n = 
1wehave V1 = 
V.We then skip the parentheses and write v 
instead of 
(v 
fora 1-tuple. The notation(9.2)formally definesa“multiplication” as map from 
Vn . 
Kn 1 to V. 

For all. 
. 
K we have 

.. 


..1 

..

. 
· 
v 
= 
(. 
· 
.1 )v1 ++ 
(. 
· 
.n )vn = 
(v1 ,...,vn 
.. 


..n 

If .1 .n . 
K and 

.. 


.1 

.. 


u = 
.1v1 ++ 
.nvn = 
(v1 ,...,vn 
.. 


.n 

then 

.. 


.1 + 
.1 

.. 


v 
+ 
u = 
(.1 + 
.1 )v1 ++ 
(.n + 
.n )vn = 
(v1 ,...,vn 
.. 


.n + 
.n 


9 Vector Spaces 

This shows that if vectors are given by linear combinations, then the operations 
scalar multiplication and addition correspond to operations with the coefficients of 
the vectors with respect to the linear combinations. 

We can further extend this notation. Let A=[aij].Kn m and let 

.. 


a1j 

.. 


uj =(v1 ,...,vn 
.. 
j=1 m 

anj 

Then we write the m linear combinations for u1 um as the system 

(u1 um =:(v1 ,...,vnA (9.3) 

On bothsidesof this equationwehaveelementsofVm.The right-multiplication of 
an arbitrary n-tuple (v1 ,...,vn . 
Vn with a matrix A . 
Kn m thus corresponds 
to forming m linear combinations of the vectors v1 ,...,vn, with the corresponding 
coefficientsgivenbythe entriesof A.Formally,this definesa “multiplication” asa 
map from Vn .Kn m to Vm . 

Lemma 9.23 Let VbeaK-vector space,let v1 ,...,vn .Vbe linearly independent, 
let A . 
Kn m, and let (u1 um =(v1 ,...,vn A. Then the vectors u1 um 
are linearly independentif and onlyifrank(A =m. 

Proof Exercise. . 


Now consider also a matrix B =[bij].Km ,.Using(9.3)we obtain 

(u1 um B =((v1 ,...,vnAB 

Lemma 9.24 In theprevious notation, 

((v1 ,...,vnAB =(v1 ,...,vn )(AB 

Proof Exercise. . 


Let {v1 ,...,vn}and {w1 ,...,wn}be bases of V and let v 
.V.ByLemma9.22 
there exist (unique) coordinates .1 .n and .1 .n, respectively, with 

.. 
.. 


.1 .1 

.. 
.. 


v 
=(v1 ,...,vn 
.. 
=(w1 ,...,wn 
.. 


.n .n 

We will now describe a method for transforming the coordinates .1 .n with 
respect to the basis {v1 ,...,vn}into the coordinates .1 .n with respect to the 
basis {w1 ,...,wn}, and vice versa. 


9.3 Coordinatesand Changesofthe Basis 127 
For every basis vector v 
j, j = 
1 n, there exist (unique) coordinates pij, 
i=1 n, such that 

.. 


p1j 

.. 


v 
j =(w1 ,...,wn 
.. 
j=1 n 
pnj 
Defining P =[pij]. 
Kn n we can write these n equations for the vectors v 
j 
analogous to(9.3)as 

(v1 ,...,vn =(w1 ,...,wnP (9.4) 

In the same way, for every basis vector w 
j, j = 
1 n, there exist (unique) 
coordinates qij, i=1 n, such that 

.. 


q1j 

.. 


w 
j =(v1 ,...,vn 
.. 
j=1 n 
qnj 
If we set Q=[qij].Kn n,then analogouslyto(9.4)weget 
(w1 ,...,wn =(v1 ,...,vnQ 

Thus, 

(w1 ,...,wn =(v1 ,...,vnQ=((w1 ,...,wnPQ=(w1 ,...,wn )(PQ 

whichimpliesthat 

(w1 ,...,wn )(In .PQ =(00 

This means that the n linear combinations of the basis vectors w1 ,...,wn, with 
their corresponding coordinatesgivenbythe entriesofthe ncolumns of In .PQ, 
areall equaltothe zerovector.Sincethe basisvectorsare linearly independent,all 

=0.Kn n

coordinates mustbe zero,and hence In.PQ,or PQ=In.Analogously 
we obtainthe equation QP =In.Thereforethe matrix P.Kn n is invertible with 
P.1 

=Q. Furthermore,wehave 

. 
.1 . 
. 
.1 . 
. 
. 
.1 .. 
v 
=(v1 ,...,vn . 
. 
. 
. 
=((w1 ,...,wn P . 
. 
. 
. 
=(w1 ,...,wn . 
. 
P. 
. 
. 
. 
. 
. 
.n .n .n 


9 Vector Spaces 

Duetothe uniquenessofthe coordinatesof v 
with respect to the basis {w1 ,...,wn}we obtain 

. 
. 
. 
.1 . 
. 
. 
=P 
. 
. 
. 
.1 . 
. 
. 
or 
. 
. 
. 
.1 . 
. 
. 
=P.1 . 
. 
. 
.1 . 
. 
. 
.n .n .n .n 

Henceamultiplicationwith thematrix Ptransformsthe coordinatesof v 
with respect 
to the basis {v1 ,...,vn}into thosewith respecttothe basis {w1 ,...,wn};amultiplication 
with P.1 yields theinverse transformation. Therefore, P and P.1 are called 
coordinatetransformation matrices. 

We can summarize the results obtained above as follows. 

Theorem 9.25 Let {v1 ,...,vn}and {w1 ,...,wn}be basesofaK-vector space V. 
Then the uniquely determined matrix P .Kn n is(9.4)isinvertible and yields the 
coordinatetransformationfrom {v1 ,...,vn}to {w1 ,...,wn}:If 

.. 
.. 


.1 .1 

.. 
.. 


v 
=(v1 ,...,vn 
.. 
=(v1 ,...,vn 
.. 


.n .n 

then 
.. 
.. 


.1 .1 

.. 
.. 


.. 
=P.. 


.n .n 

Example9.26 Consider the vector space V =R2 ={(.1 .2 |.1 .2 .R}with 
the entrywise addition and scalar multiplication. A basis of V is given by the set 
{e1 =(10 e2 =(01 },and we have(.1 .2 =.1e1+.2e2 forall (.1 .2 .V. 
Another basisof Vistheset{v1 =(11 v2 =(12 }.Thecorresponding coordinate 
transformation matrices can be obtained from the defining equations (v1 ,v2 = 
(e1 e2 P and (e1 e2 =(v1 ,v2 Qas 

 . 
 . 


11 2.1

1

P = 
Q=P.= 


12 .11 

9.4 RelationsBetweenVector Spaces andTheir Dimensions 
Our first result describes the relation between a vector space and a subspace. 

Lemma 9.27 If V is a finite dimensional vector space and U .V is a subspace, 
then dim(U .dim(V with equalityif and onlyif U =V. 


9.4 Relations BetweenVector Spaces andTheir Dimensions 129 
Proof Let U . 
V and let {u1 um}be a basis of U, where {u1 um}= 
O 
for U ={0}. Using Theorem9.14 we can extend this set to a basis of V.If U is 
a proper subset of V, then at least one basis vector needs to be added and hence 
dim(U < 
dim(V .IfU =V, then every basis of V is also a basis of U, and thus 
dim(U =dim(V . . 


If U1 and U2 aresubspacesofavector space V,then their intersection is givenby 

U1 .U2 ={u .V |u .U1 . 
u .U2} 


(cp. Definition 2.6). The sum of thetwo subspacesis defined as 

U1 +U2 :={u1 +u2 .V |u1 .U1 . 
u2 .U2} 


Lemma 9.28 If U1 and U2 are subspaces of a vector space V,then thefollowing 
assertions hold: 

(1) U1 .U2 and U1 +U2 are subspaces of V. 
(2) U1 +U1 =U1. 
(3) U1 +{0}=U1. 
(4) U1 .U1 +U2,with equalityif and onlyifU2 .U1. 
Proof Exercise. . 


An important resultis thefollowing dimensionformula forsubspaces. 

Theorem 9.29 If U1 and U2 are finite dimensional subspaces of a vector space V, 
then 

dim(U1 .U2 +dim(U1 +U2 =dim(U1 +dim(U2 

Proof Let {v1 ,...,vr} 
be a basis of U1 . 
U2. We extend this set to a basis 
{v1 ,...,vr ,w1 ,...,w}of U1 and to a basis {v1 ,...,vrx1 xk}of U2, where 
we assume that r . 
k .1.(Ifoneofthe listsisempty,thenthefollowingargument 
is easily modified.) 

If suffices to show that {v1 ,...,vr ,w1 ,...,w. 
x1 xk}isa basisof U1+U2. 
Obviously, 

span{v1 ,...,vr ,w1 ,...,w. 
x1 xk}=U1 +U2 

and hence it suffices to show that v1 ,...,vr ,w1 ,...,w. 
x1 xk are linearly 
independent.Let 

r . 
k 

.ivi + 
.iwi + 
.ixi =0 

i=1 i=1 i=1 


9 Vector Spaces 

then 

kr . 


.ixi =. 
.ivi + 
.iwi 

i=1 i=1 i=1 

Ontheleft hand sideof this equationwehave,by definition,avectorin U2;onthe 

k

right hand side a vector in U1.Therefore, 1 .ixi . 
U1 .U2.By construction,

i=

however,{v1 ,...,vr}isabasisof U1.U2 and thevectors v1 ,...,vr ,w1 ,...,w. 
are 

linearly independent.Therefore, 1 .iwi =0implies that.1 =···=.. 
=0.

i=

Butthen also 

rk 

.ivi + 
.ixi =0 

i=1 i=1 

and hence .1 =···=.r =.1 =···=.k =0due to the linear independence of 
v1 ,...,vrx1 xk. . 


If at least one of the subspaces in Theorem9.29 is infinite dimensional, then 
the assertion is still formally correct, since in this case dim(U1 +U2 =.and 
dim(U1 +dim(U2 =.. 

Example9.30 Forthe subspaces 

U1 ={[.1 .20]|.1 .2 .K} 
U2 ={[0 .2 .3]|.2 .3 .K}.K13 

we have dim(U1 =dim(U2 =2, 

U1 .U2 ={[0 .20]|.2 .K} 
dim(U1 .U2 =1 
=K13

U1 +U2 dim(U1 +U2 =3 

Theabovedefinitionofthesumcanbeextendedtoanarbitrary(butfinite)number 
of subspaces:If U1 Uk, k . 
2, are subspaces of the vector space V, then we 
define 

k 
. 
k 
. 


U1 ++Uk = 
Uj := 
uj |uj .Ujj=1 k 

j=1 j=1 

This sumis called direct,if 

k 

Ui . 
Uj ={0} 
for i =1 k 

j=1 
j=i 

andin this casewewrite the(direct)sum as 


9.4 Relations BetweenVector Spaces andTheir Dimensions 131 
k 

. 


U1 ..Uk = 
Uj 

j=1 

In particular,asum U1+U2 of twosubspaces U1 U2 .Vis direct if U1 .U2 ={0}. 

Thefollowing theorem presentstwoequivalent characterizationsofthedirectsum 
of subspaces. 

Theorem 9.31 If U =U1 ++Uk isasumofk .2subspaces of a vector space 
V,then thefollowing assertions are equivalent: 

(1) The sum U is direct, i.e., Ui . 
i Uj ={0}fori =1 k.
j=

k

(2) Every vectoru .U hasarepresentationofthe formu = 
1 uj with uniquely
j=

determined uj .Uj forj=1 k. 

k

(3) 1 uj = 
0 with uj . 
Uj for j = 
1 kimpliesthat uj = 
0 for j = 
j=

1 k. 

Proof 

k k

(1) . 
(2 :Let u = 
j1 uj = 
j=1#uj with uj 
#uj . 
Uj, j = 
1 k.For
=

everyi =1 k we then have 

ui .#ui =. 
(uj .#uj . 
Ui . 
Uj 

j=ij=i 

Now Ui . 
ji Uj ={0}implies that ui .#ui = 
0, and hence ui =#ui for

=

i =1 k. 

(2) .(3 :This is obvious. 
(3) . 
(1 :For a given i,let u . 
Ui . 
i Uj. Then u = 
iuj for some 
. 
j=j=uj .Uj, j =i, and hence .u + 
iuj =0. In particular,thisimpliesthat 

. 
j=u =0, and thusUi . 
=i Uj ={0}. . 


j

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

9.1. Whichofthe following sets (with theusual addition and scalar multiplication) 
are R-vector spaces? 
[.1 .2].R12|.1 =.2 [.1 .2].R12|.21 +.2 =1

2 

[.1 .2].R12|.1 ..2 [.1 .2].R12|.1 ..2 =0and2.1 +.2 =0 

Determine,if possible,a basis and thedimension. 

9.2. Determinea basisofthe R-vector space Cand dimR(C .Determinea basisof 
the C-vector space C and dimC(C . 
9.3. Showthat a1 an .Kn 1 are linearly independentifandonlyifdet([a1 …, 
an] 
=0. 

9 Vector Spaces 

9.4. Let V be a K-vector space, . 
a nonempty set and Map(. 
V the set of maps 
from . 
to V. Showthat Map(. 
V with the operations 
+:Map(. 
V .Map(. 
V >Map(. 
V ( 
fg 
> 
f +g 
with ( 
f +g )(x := 
f(x +g(x for all x .. 
·:K .Map(. 
V >Map(. 
V (. 
f >. 
· 
f 
with(. 
· 
f )(x :=. 
f(x for all x .. 


is a K-vector space. 

9.5. Showthatthefunctionssinand cosinMap(RR are linearly independent. 
9.6. Let V be a vector space with n =dim(V .N and let v1 ,...,vn .V. Show 
that thefollowing statementsare equivalent: 
(1) v1 ,...,vn are linearly independent. 
(2) span{v1 ,...,vn}=V. 
(3) {v1 ,...,vn}is a basis of V. 
9.7. Show that (Kn m +· 
is a K-vector space (cp. (1)inExample 9.2). Finda 
subspace of this K-vector space. 
9.8. Show that 
(K[t]+· 
is a K-vector space (cp. (2) in Example 9.2). Show 
further that K[t].n isasubspaceof K[t](cp. (3)inExample 9.6)and determine 
dim(K[t].n . 
9.9. Show that the polynomials 
p1 = 
t5 +t4, p2 = 
t5 .7t3, p3 = 
t5 .1, 
p4 =t5 +3t are linearly independent in Q[t].5 and extend {p1 p2 p3 p4}to a basis of Q[t].5. 
9.10. 
Let n .N and 
. 
n 
. 


$

j 

$

K[t1 t2]:= 
.ijt1it.ij .K

2 
ij=0 

An element of K[t1 t2]is called bivariate polynomial over K in the unknowns 
t1 and t2. Define a scalar multiplication and an addition so that K[t1 t2]becomes a vector space. Determine a basis of K[t1 t2]. 

9.11. 
ShowLemma 9.5. 
9.12. 
Let A.Kn m and b.Kn 1.Isthe solutionset L(Ab of Ax =basubspace 
of Km 1? 
9.13. 
Let A . 
Kn n and let . 
. 
K be an eigenvalue of A. Show that the set {v 
. 
Kn 1 |Av 
=.v}is a subspace of Kn 1. 
9.14. 
Let A . 
Kn n and let .1 =.2 be two eigenvalues of A. Show that any two 
associated eigenvectors v1 and v2 are linearly independent. 
9.15. 
Showthat B ={B1 B2 B3 B4}and C ={C1 C2 C3 C4}with 
. 
. 
 . 
 . 
 . 


111010 11

B1 = 
B2 = 
B3 = 
B4 = 


000010 01 


9.4 Relations Between Vector Spaces and Their Dimensions 133 
and 
. 
. 
. 
. 
. 
. 
. 
. 
C1 = 
10 
01 
C2 = 
10 
10 
C3 = 
10 
00 
C4 = 
01 
10 

are bases of the vector space K22, and determine corresponding coordinate 
transformation matrices. 

9.16. 
Examine the elements of the following sets for linear independence in the 
vector space K[t].3: 
U1 ={tt2 +2tt2 +3t +1 t3} 
U2 ={1 tt +t2 t2 +t3} 


U3 ={1 t2 .tt2 +tt3} 


Determine the dimensions of the subspaces spanned by the elements of U1 
U2 U3.Is oneof thesesetsa basisof K[t].3? 

9.17. 
Showthatthesetof sequences {(.1 .2 .3 |.i .Q i .N}with entry-
wise addition and scalar multiplicationforms an infinite dimensional vector 
space, and determine a basis system. 
9.18. 
Prove Lemma 9.23. 
9.19. 
Prove Lemma 9.24. 
9.20. 
Prove Lemma 9.28. 
9.21. 
Let U1 U2 be finite dimensional subspaces of a vector space V. Showthat the 
sum U1 +U2 is direct if dim(U1 +U2 =dim(U1 +dim(U2. 
9.22. 
Let U1 Uk, k .3, be finite dimensional subspaces of a vector space V. 
Supposethat Ui .Uj ={0}for all i 
= 
j.Isthe sum U1 ++Uk direct? 
9.23. 
Let U be a subspace of a finite dimensional vector space V. Show that there 
exists another subspace U#
with U .U = 
V.(The subspace U is called a
. 
#

complement of U.) 

9.24. 
Determine three subspaces U1 U2 U3 of V = 
R31 with U2 
= 
U3 and V = 
U1 .U2 =U1 .U3.Istherea subspace U1 of V with a uniquely determined 
complement? 

Chapter10 
LinearMaps 

In this chapter we study maps betweenvector spaces that are compatible with thetwo 
vector space operations, addition and scalar multiplication. These maps are called 
linear mapsor homomorphisms.We firstinvestigatetheirmostimportant properties 
and then show that in the case offinite dimensional vector spaces every linear map 
canbe representedbyamatrix,whenbasesintherespectivespaceshavebeen chosen. 
If the bases are choseninacleverway,then we can readoffimportant propertiesof 
a linear map from its matrix representation. This centralidea will arisefrequentlyin 
later chapters. 

10.1 BasicDefinitionsandPropertiesofLinearMaps 
We start ourinvestigations with the definitionoflinear maps betweenvector spaces. 

Definition 10.1 Let V and W be K-vector spaces.A map f . 
V > 
W is called 
linear, when 

(1) f(.v 
. 
. 
f(v 
, and 
(2) f(v 
. 
w 
. 
f(v 
. 
f(w 
, 
holdfor all v,w 
v 
V and . 
v 
K.The set of all these maps is denoted by L(VW . 
Alinear map f . 
V > 
W is also calleda linear transformation or (vector space) 
homomorphism.Abijective linear mapis calledan isomorphism.Ifthere exists an 
isomorphism between V and W, then the spaces V and W are called isomorphic, 
whichwe denoteby 

V .

. 
W 

Amap f v 
L(VV is called an endomorphism, andabijective endomorphismis 
called an automorphism. 
©SpringerInternationalPublishing Switzerland 2015 135 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_10 

136 10 Linear Maps 

Itisaneasyexercisetoshowthatthe conditions(1)and(2)inDefinition 10.1 
holdifand onlyif 

f(.v 
. 
.w 
. 
. 
f(v 
. 
. 
f(w 


holds for all .. 
v 
K and v,w 
v 
V. 

Example 10.2 

(1) Every matrix Av 
Kn m defines a map 
A . 
Km 1 > 
Kn 1 x 
> 
Ax 

This mapis linear,since 

A(.x . 
.Ax for all x v 
Km 1 and . 
v 
K, 
A(x . 
y . 
Ax . 
Ay for all xy v 
Km 1 

(cp. Lemmas 4.3 and 4.4). 

n

(2) The maptrace . 
Kn n > 
K, A =[aij 
> 
trace(A :. 
i=1aii ,is linear (cp. 
Exercise 8.8). 
(3) The map 
f . 
Q[t .3 > 
Q[t .2 .3t3 . 
.2t2 . 
.1t . 
.0 
> 
2.2t2 . 
3.1t . 
4.0 

is linear.(Showthisasanexercise).The map 

g 
. 
Q[t .3 > 
Q[t .2 .3t3 . 
.2t2 . 
.1t . 
.0 
> 
.2t2 . 
.1t . 
.2 

0 

is not linear.Forexample,if p1 . 
t . 
2and p2 . 
t . 
1, then g( 
p1 . 
p2 . 
2t . 
9
. 
2t . 
5. 
g( 
p1 . 
g( 
p2. 

The set of linear maps between vector spaces forms a vector space itself. 

Lemma 10.3 Let V and W beK-vector spaces.For f g 
v 
L(VW and . 
v 
K 
define f . 
g 
and . 
. 
fby 

( 
f . 
g 
)(v 
:. 
f(v 
. 
g(v 
(. 
. 
f )(v 
:. 
. 
f(v 


for all v 
v 
V. Then (L(VW +. 
isaK-vector space. 
Proof Cp.Exercise 9.4. . 
	
The nextresult deals with the existence and uniqueness oflinear maps. 

Theorem 10.4 Let V and W beK-vector spaces,let {v1 ,...,vm. 
be a basis of V, 
and let w1 ,...,wm v 
W. Then there exists a unique linear map f v 
L(VW with 
f(vi . 
wi fori . 
1 m. 


10.1 Basic Definitions andPropertiesofLinear Maps 137 
(v 
.(v

Proof For every v 
v 
V there exist (unique) coordinates .1 m with v 
. 


m (v 
vi (cp. Lemma 9.22).We define themap f . 
V > 
W by

i=1 .i 

m 

f(v 
:. 
.(v 
wi for all v 
v 
V.

i 
i=1 

By definition, f(vi . 
wi for i . 
1 m. 

We next show that f is linear.Forevery . 
v 
K we have .v 
. 
im 
=1(..(v 
)vi,

i 

and hence 

mm 

(v 
(v

f(.v 
. 
(..i )wi . 
..i wi . 
.f(v 
i=1 i=1 

m (um (v 
(u

If u . 
vi v 
V,then v 
. 
u =. 
.)vi, and hence

i=1 .ii=1(.ii 

m mm 

(v 
(u (v 
(u

f(v 
. 
u . 
(.. 
.)wi . 
.wi . 
.wi . 
f(v 
. 
f(u

ii ii 
i=1 i=1 i=1 

Thus, f v 
L(VW . 
Supposethat g 
v 
L(VW also satisfies g(vi . 
wi for i . 
1 m.Then for 

everyv 
. 
im 
=1 .(v 
vi we have

i 

. 
m 
. 
m mm 
. 
m 
. 


(v 
(v 
(v(v 
(v

f(v 
. 
f .vi . 
.f(vi . 
.wi . 
.g(vi . 
g.vi . 
g(v

iiii i 
i=1 i=1 i=1 i=1 i=1 

and hence f . 
g, sothat f is indeed uniquely determined. . 
	

Theorem 10.4 shows that the map f v 
L(VW is uniquely determined by the 
imagesof f atthegiven basisvectorsofV.Notethat theimagevectors w1 ,...,wm v 
W maybe linearly dependent, and that W maybeinfinite dimensional. 

In Definition 2.12wehaveintroducedtheimageand pre-imageofamap.Wenext 
recallthese definitions for completeness and introduce thekernelofa linear map. 

Definition 10.5 If V and W are K-vector spaces and f v 
L(VW ,then the kernel 
and the imageof f are defined by 

ker( 
f :. 
{v 
v 
V . 
f(v 
. 
0. 
im( 
f :. 
. 
f(v 
. 
v 
v 
V. 


For w 
v 
W the pre-image of w 
in the space V is defined by 

f.1

(w 
:. 
f.1({w. 
={v 
v 
V . 
f(v 
. 
w. 


Thekernelofa linear mapissometimes calledthe null space (or nullspace)of 
the map, and some authors use the notation null( 
f instead ofker( 
f . 


138 10 Linear Maps 

Note that thepre-image f.1(w 
is a set, and that f.1 here does not mean the 
inversemap of f (cp. Definition 2.12).In particular,wehave f.1(0 =ker( 
f , and 
if w 
.im( 
f ,then f.1(w 
=O, 

Example 10.6 For A .Kn m and the corresponding map A .L(Km 1 Kn 1 from 

(1)inExample 10.2 wehave 
ker(A ={x .Km 1 |Ax =0. 
and im(A ={Ax |x .Km 1. 


Note that ker(A . 
L(A 0 (cp. Definition 6.1). Let aj v 
Kn 1 denote the jth 
column of A, j=1 m.For x =[x1 xm 
T .Km 1 we then can write 

m 

Ax . 
xjaj 
j=1 

Clearly,0 .ker(A .Moreover, we see fromthe representationof Ax thatker(A . 
{0}if and onlyifthe columnsof Aare linearly independent.The setim( 
A is given 
by the linear combinations of the columns of A, i.e., im(A =span{a1 am}. 

Lemma 10.7 If V and W are K-vector spaces, then for every f v 
L(VW the 
following assertions hold: 

(1) f(0 =0and f (.v 
=.f(v 
for all v 
.V. 
(2)Iffisan isomorphism,thenf .1 .L(WV . 
(3) ker( 
f is a subspace ofV and im( 
f is a subspace ofW. 
(4) f is surjectiveifand onlyifim( 
f =W. 
(5) f is injectiveifand onlyifker( 
f ={0}. 
(6) If f is injective and if v1 ,...,vm .V are linearly independent,thenf (v1 
f(vm .W are linearly independent. 
(7) If v1 ,...,vm .V are linearly dependent,thenf (v1 f(vm .W are linearly 
dependent,or, equivalently,iff (v1 f(vm .W are linearly independent,
then v1 ,...,vm .V are linearly independent. 
(8) If w 
.im( 
f and if u v 
f.1(w 
is arbitrary, then 
f.1

(w 
=u +ker( 
f :={u +v 
|v 
.ker( 
f . 


Proof 

(1) Wehave f(0V . 
f(0K ·0V =0K . 
f(0V =0V as well as f(v 
. 
f(.v 
. 
f(v 
+(.v 
. 
f(0 =0for allv 
.V. 
(2) Theexistenceof theinverse map f.1 :W >Vis guaranteedbyTheorem 2.20, 
so we just have to show that f.1 is linear. If w1 ,w2 v 
W, then there exist 
uniquely determined v1 ,v2 .V with w1 . 
f(v1 and w2 . 
f(v2 .Hence, 
f.1(w1 +w2 . 
f.1( 
f(v1 . 
f(v2 . 
f.1( 
f(v1 +v2 =v1 +v2 

. 
f.1(w1 . 
f.1(w2 


10.1 Basic Definitions andPropertiesofLinear Maps 139 
Moreover, for every . 
.K we have 

f.1. 
f.1

(.w1 . 
f.1(. 
f(v1 ( 
f(.v1 =.v1 =. 
f.1(w1 

(3) and (4 are obvious from the corresponding definitions. 
(5) Let f be injective and v 
v 
ker( 
f , i.e., f(v 
. 
0. From (1) we know that 
f(0 =0. Since f(v 
. 
f(0,the injectivityof f yields v 
=0. Suppose now 
thatker( 
f ={0}and let u v 
.V with f(u . 
f(v 
.Then f(u .v 
=0, i.e., 
u .v 
.ker( 
f , which implies u .v 
=0, i.e.,u =v. 

m

(6) Let i=1 .if(vi =0. The linearity of f yields 
. 
m 
. 
. 
m 
. 
f .ivi =0 i.e., .ivi .ker( 
f 
i=1 i=1 

m

Since f is injective, we have i=1 .ivi . 
0by (5 , and hence .1 . 
··. 
. 
.m =0due to the linear independence ofv1 ,...,vm.Thus, f(v1 f(vm 
are linearly independent. 

m

(7) Ifv1 ,...,vm are linearly dependent,then i=1 .ivi =0forsome.1 .m v 
K that are not all equal to zero. Applying f on bothsides and usingthe linearity 
m

yields i=1 .if(vi =0, hence f(v1 f(vm are linearly dependent. 

(8) Let w 
.im( 
f and u v 
f.1(w 
. 
If v 
v 
f.1(w 
,then f(v 
. 
f(u , and thus f(v 
.u =0, i.e.,v 
.u .ker( 
f 
or v 
.u +ker( 
f .This shows that f.1(w 
.u +ker( 
f . 
If, on theother hand, v 
.u+ker( 
f ,then f(v 
. 
f(u =w,i.e.,v 
v 
f.1(w 
. 
This showsthat u +ker( 
f . 
f.1(w 
. . 
	
Example 10.8 Consider a matrix A v 
Kn m and the corresponding map A v 
L(Km 1 Kn 1 from(1)in Example 10.2.Foragiven b .Kn 1 we have A.1(b . 
L(Ab .Ifb.im( 
A ,thenL(Ab =O(case(1)in Corollary6.6).Nowsuppose 
that b.im(A and let 

x .L(Ab be arbitrary. Then (8)inLemma 10.7 yields 

L(Ab =
x +ker(A 

whichisthe assertion ofLemma 6.2.Ifker(A ={0}, i.e., the columns of A are 
linearly independent,then |L(Ab |=1(case(2)inCorollary6.6).Ifker(A ={0}, 
i.e., the columns of A are linearly dependent, then |L(Ab . 
> 
1(case (3) in 
Corollary 6.6). If {w1 ,...,w}isa basisofker(A ,then 

L( 
Ab . 

x . 
.iwi . 
.1 .. 
.K 
i=1 

Thus,the solutionsof Ax =bdepend of . 
.m parameters. 


140 10 Linear Maps 

Thefollowing result, whichgives an important dimensionformula for linear maps, 
is also known as the rank-nullity theorem:The dimensionofthe imageof f is equal to 
therankofamatrixassociated with f (cp. Theorem 10.22 below),and thedimension 
ofthekernel(or nullspace)of f is sometimes calledthe nullity1 of f. 

Theorem 10.9 Let V and W be K-vector spaces and let V be finite dimensional. 
Then for every f .L(VW we have thedimensionformula 

dim(V =dim(im( 
f +dim(ker( 
f 

Proof Let v1 ,...,vn v 
V.If f(v1 f(vn v 
W are linearly independent, 
then by (7 in Lemma 10.7 also v1 ,...,vn are linearly independent, and thus 
dim(im( 
f . 
dim(V . Since ker( 
f . 
V,wehavedim(ker( 
f . 
dim(V ,so 
that im( 
f andker( 
f are both finite dimensional. 

Let {w1 ,...,wr}and {v1 ,...,vk}be basesofim( 
f andker( 
f ,respectively,and 
let u1 v 
f.1(w1 ur v 
f.1(wr .We will show that {u1 ur ,v1 ,...,vk}is 
a basis of V,which then impliesthe assertion. 

If v 
v 
V, then by Lemma 9.22 there exist (unique) coordinates .1 .r v 


rr

K with f(v 
. 
i=1 .iwi.Let 


v 
:. 
i=1 .iui, then f(


v 
. 
f(v 
, and hence 

k 
v 
.

v 
v 
ker( 
f , which gives v 
.

v 
. 
i=1 .ivi forsome(unique) coordinates 
.1 .k v 
K.Therefore, 

k rk 

v 
=

v 
. 
.ivi . 
.iui . 
.ivi 
i=1 i=1 i=1 

and thus v 
v 
span{u1 ur ,v1 ,...,vk}.Since {u1 ur ,v1 ,...,vk}. 
V,we 
have 

V =span{u1 ur ,v1 ,...,vk. 


andit remainstoshowthat u1 ur ,v1 ,...,vk are linearly independent.If 

rk 

.iui . 
.ivi =0 
i=1 i=1 

then 

rkr r 

0. 
f(0 . 
f .iui . 
.ivi . 
.if(ui . 
.iwi 

i=1 i=1 i=1 i=1 

and thus .1 =···=.r =0, becausew1 ,...,wr are linearly independent.Finally, 
the linear independence of v1 ,...,vk impliesthat .1 =···=.k =0. . 
	

1This termwasintroducedin 1884by JamesJosephSylvester (1814–1897). 


10.1 Basic Definitions andPropertiesofLinear Maps 141 
Example 10.10 

(1) For the linear map 
.. 
.. 


.1 .1 

31 >Q21 101 .1 +.3 

.. 
..

f :Q.2 
> 
.2 =

101 .1 +.3

.3 .3 

we have 

.. 
. 


.. 


. 
.1 
.

. 


..

im( 
f . 
. 
.Q ker( 
f . 
.2 .1 .2 .Q 

. 
..

..1 

Hence dim(im( 
f =1and dim(ker( 
f =2, so that indeed dim(im( 
f . 
dim(ker( 
f =dim(Q31 . 

(2) If A.Kn m and A.L(Km 1 Kn 1 areasin(1)in Example 10.2,then 
m =dim(Km 1 =dim(ker(A +dim(im( 
A 

Thus,dim(im( 
A =m if and onlyifdim(ker( 
A =0.This holdsifandonlyif 
ker( 
A ={0},i.e.,ifandonlyifthe columnsof Aare linearly independent (cp. 
Example 10.6). If, on the other hand, dim(im(A < 
m, then dim(ker(A . 
m .dim(im(A > 
0, and thusker( 
A ={0}.In this case the columns of A 
are linearly dependent, since there exists an x .Km 1 \{0}with Ax =0. 

Corollary 10.11 If V and W are K-vector spaces with dim(V . 
dim(W v 
N 
and if f .L(VW ,then thefollowing statementsare equivalent: 

(1) f is injective. 
(2) f is surjective. 
(3) f is bijective. 
Proof If (3 holds, then (1 and (2 hold by definition. We now show that (3 is 
impliedby(1 as well as by(2. 

If f is injective, thenker( 
f ={0}(cp. (5)inLemma 10.7)and thedimension 
formulaofTheorem 10.9 yields dim(W =dim(V =dim(im( 
f .Thus,im( 
f . 
W (cp. Lemma 9.27), so that f is also surjective. 

If f is surjective, i.e., im( 
f =W,then thedimensionformula and dim(W . 
dim(V yield 

dim(ker( 
f =dim(V .dim(im( 
f =dim(W .dim(im( 
f =0 

Thus,ker( 
f ={0}, sothat f is also injective. . 
	

UsingTheorem 10.9wecanalso characterizewhentwofinite dimensionalvector 
spaces areisomorphic. 


142 10 Linear Maps 

Corollary 10.12 Two finite dimensionalK-vector spaces V and W areisomorphic 
if and onlyifdim(V =dim(W . 

Proof If V .

=W,then there exists a bijective map f .L(VW .By(4) and (5)in 
Lemma 10.7 we have im( 
f =W andker( 
f ={0}, and thedimensionformulaof 
Theorem 10.9 yields 

dim(V =dim(im( 
f +dim(ker( 
f =dim(W +dim({0}=dim(W 

Let now dim(V =dim(W .We need to show that there exists a bijective f v 
L(VW .Let{v1 ,...,vn}and {w1 ,...,wn}be bases of Vand W.ByTheorem 10.4 
there exists a unique f .L(VW with f(vi =wi, i =1 n.Ifv 
=.1v1 . 


+.nvn .ker( 
f ,then 

0. 
f(v 
. 
f(.1v1 ++.nvn =.1 f(v1 ++.nf(vn 
=.1w1 ++.nwn 

Since w1 ,...,wn are linearly independent,wehave.1 =···=.n =0,hencev 
=0 
and ker( 
f ={0}. Thus, f is injective. Moreover, the dimension formula yields 
dim(V =dim(im( 
f =dim(W and, therefore, im( 
f =W (cp. Lemma 9.27), 
so that f is also surjective. . 
	

Example 10.13 

(1) Thevector spaces Kn m and Km n bothhavethe dimension n·m and aretherefore 
isomorphic.An isomorphismisgivenbythe linear map A>AT . 
(2) The R-vector spaces R12 and C ={x +iy |xy .R}bothhavethe dimension2andarethereforeisomorphic.
An isomorphismisgivenbythe linearmap 
[xy >x +iy. 

(3) The vector spaces Q[t .2 and Q13 bothhavedimension3 and aretherefore 
isomorphic.An isomorphismisgivenby the linear map .2t2 +.1t +.0 
> 
[.2 .1 .0. 
Although Mathematicsisa formal andexact science, wheresmallest details mat-
ter,one sometimesusesan“abuseofnotation”inordertosimplifythe presentation. 
Wehaveusedthisforexampleinthe inductiveexistenceproofofthe echelonform 
in Theorem 5.2.Therewekept,for simplicity,the indicesofthelarger matrix A(1 in 

(2

thesmallermatrix A(2 =[a.The matrix A(2 had,of course,an entryin position

ij 

(2 (2

(11 ,but this entrywas denotedby arather than a11.Keeping theindicesin the

22 

inductionmade theargument much less technical,while theproof itselfremained 
formally correct. 

An abuse ofnotation should always be justified and should not be confused with 
a “misuse”ofnotation.In the fieldofLinear Algebraa justificationisoftengiven 
by an isomorphism that identifies vector spaces with each other.Forexample, the 
constant polynomials over a field K, i.e., polynomials of theform .t0 with . 
.K, 
areoften writtensimplyas.,i.e.,as elementsofthefield itself.Thisis justifiedsince 


10.1 Basic Definitions andPropertiesofLinear Maps 143 
K[t .0 and Kareisomorphic K-vector spaces (ofdimension1).We already used 
this identificationabove.Similarly,wehaveidentifiedthevector space Vwith V1 and 
writtenjust v 
instead of (v 
in Sect. 9.3.Another commonexampleinthe literature 
is the notation Kn that in our text denotes the set of n-tuples with elementsfrom 
K,but whichisoften used forthe (matrix) setsof the “columnvectors” Kn 1 or the 
“row vectors” K1 n.The actual meaning then shouldbeclear fromthe context.An 
attentivereadercan significantly benefitfromthesimplificationsduetosuchabuses 
of notation. 

10.2 Linear Maps andMatrices 
Let V and W be finite dimensional K-vector spaces with bases {v1 ,...,vm. 
and 
{w1 ,...,wn},respectively,and let f v 
L(VW .ByLemma 9.22,forevery f(v 
j v 
W, j. 
1 m,there exist (unique) coordinates aij v 
K, i. 
1 n, with 

f(v 
j . 
a1jw1 +. 
anjwn 

We define A:. 
[aij v 
Kn m and write,similarlyto(9.3), the mequations for the 
vectors f(v 
j as 

( 
f(v1 f(vm . 
(w1 ,...,wnA (10.1) 

The matrix Ais determined uniquelyby fand thegiven basesof V and W. 
If v 
. 
.1v1 +. 
.mvm v 
V,then 

f(v 
. 
f(.1v1 +. 
.mvm . 
.1 f(v1 +. 
.mf(vm 

.. 


.1 

..

. 
( 
f(v1 f(vm 

.m 

.. 


.1 

..

. 
((w1 ,...,wnA 

.m 

.. 
.. 


.1 

.. 
..

. 
(w1 ,...,wnA 

.m 

The coordinates of f(v 
with respecttothegiven basisof W arethereforegiven by 

.. 


.1 

..

A 

.m 


144 10 Linear Maps 

Thus,wecan computethe coordinatesof f(v 
simplybymultiplyingthe coordinates 
of v 
with A.Thismotivates thefollowing definition. 

Definition 10.14 The uniquely determined matrixin(10.1)iscalledthematrixrepresentation 
of f .L(VW with respect to the bases B1 ={v1 ,...,vm}of V and 
B2 ={w1 ,...,wn}of W.We denotethismatrixby [fB1 B2. 

The constructionofthe matrix representationand Definition 10.14 canbe consistentlyextendedtothe 
casethat(atleast) oneofthe K-vector spaces has dimension 
zero. If, forinstance, m =dim(V .N and W ={0}, then f(v 
j =0for every 
basis vector v 
j of V.Thus, every vector f(v 
j is an empty linear combination of 
vectorof the basisO of W.The matrix representation of f then is an empty matrix 
of size0.m.Ifalso V ={0},then thematrixrepresentationof f is an emptymatrix 
of size0 .0. 

Thereare manydifferent notationsforthe matrix representationoflinear mapsin 
the literature.The notationshouldreflect that thematrix depends on the linear map 
f and thegiven bases B1 and B2.Examples of alternative notations are [fB1 and

B2 

M( 
fB1 B2 (where “M” means “matrix”). 
An important special caseis obtained for V =W, hence in particular m =n, and 
f =IdV,the identity on V.Wethen obtain 

(v1 ,...,vn =(w1 ,...,wn [IdV B1 B2 (10.2) 

so that [IdV B1 B2 isexactly thematrix P in(9.4), i.e., the coordinatetransformation 
matrixin Theorem 9.25.Onthe other hand, 

(w1 ,...,wn =(v1 ,...,vn [IdV B2 B1 

and thus 

 .1

[IdV B1 B2 =[IdV B2 B1 

Example 10.15 

(1) Consider the vector space Q[t .1 with the bases B1 ={1 t}and B2 ={t . 
1 t .1}.Then the linear map 
f :Q[t .1 >Q[t .1 .1t +.0 
> 
2.1t +.0 

has the matrix representations 

1 31

10 2122

[fB1 B1 . 
02 [fB1 B2 =.1 [fB2 B2 . 
13

1

2 22 

(2) For the vector space K[t .n with the basis B ={t0 t1 tn}and the linear 
map 

10.2 Linear Maps andMatrices 145 
f :K[t .n >K[t .n 

.ntn +.n.1tn.1 ++.1t +.0 
> 
.0tn +.1tn.1 ++.n.1t +.n 

we have f(tj =tn.j for j=01 n, sothat 

..

1 

.Kn+1 n+1 
..

[f BB . 


1 

Thus, [f BB is a permutation matrix. 

Theorem 10.16 Let V and W be finite dimensional K-vector spaces with bases 
B1 ={v1 ,...,vm}andB2 ={w1 ,...,wn}, respectively. Then the map 

L(VW >Knm f >[fB1 B2 

. 
Knm and dim(L(VW =dim(Kn m 
n ·m. 

is an isomorphism.Hence L(VW .. 


Proof In this proof we denote the map f >[fB1 B2 by mat, i.e., mat( 
f . 
[fB1 B2.Wefirstshowthat this mapis linear.Let f g 
.L(VW ,mat( 
f =[fij 
and mat(g 
=[gij .For j=1 m we have 

n nn 

( 
f +g 
)(v 
j . 
f(v 
j +g(v 
j . 
fijwi . 
gijwi . 
( 
fij +gij )wi 

i=1 i=1 i=1 

and thus mat( 
f +g 
=[fij +gij ]=[fij ]+[gij ]=mat( 
f +mat(g 
.For . 
.K 
and j=1 m we have 

nn 

(. 
f )(v 
j =. 
f(v 
j =. 
fijwi . 
(. 
fij )wi 
i=1 i=1 

and thus mat(. 
f =[. 
fij ]=. 
[fij ]=. 
mat( 
f . 

It remainstoshowthat matisbijective.If f .ker(mat ,i.e., mat( 
f =0.Kn m , 
then f(v 
j =0for j . 
1 m.Thus, f(v 
. 
0for all v 
v 
V, so that f =0 
(the zeromap) and matisinjective(cp. (5 in Lemma10.7). If, on the other hand, 
A =[aij ].Kn m is arbitrary, we define the linear map f :V >W via f(v 
j :. 


n 

i=1 aijwi, j =1 m (cp. theproof of Theorem 10.4). Then mat( 
f . 
Aand 
hence matisalsosurjective(cp. (4 in Lemma10.7). 

Corollary 10.12 now shows that dim(L(VW =dim(Kn m =n ·m (cp. also 
Example 9.20). . 
	


146 10 Linear Maps 

Theorem 10.16 shows, in particular, that f g 
.L(VW satisfy f =g 
if and 
onlyif[fB1 B2 =[g 
B1 B2 holds forgiven bases B1 of V and B2 of W.Thus,we can 
prove the equalityof linear maps viathe equalityof theirmatrixrepresentations. 

We now consider the map from the elements ofa finite dimensional vector space 
to their coordinates with respecttoagiven basis. 

Lemma 10.17 IfB ={v1 ,...,vn}isa basisofaK-vector space V,then the map 

.. 


.1 

..

B . 
V >Kn 1 v 
=.1v1 ++.nvn >B(v 
:. 


.n 

is an isomorphism, calledthe coordinate map of V withrespecttothe basisB. 

Proof The linearity of B is clear.Moreover, we obviously have B(V . 
Kn 1, 
i.e., B is surjective. If v 
.ker(B , i.e., .1 =···=.n =0, thenv 
=0, so that 
ker(B ={0}and B is also injective(cp. (5)inLemma 10.7). . 
	

Example 10.18 In the vector space K[t .n with the basis B ={t0 t1 tn}we 
have 

.. 


.0 
.
.1. 


B(.ntn +.n.1tn.1 ++.1t +.0 =.. 
.Kn+1 
.. 


.n 



tn.1

On the other hand, the basis B ={tn t0}yields 

.. 


.n 

.. 


B(.ntn +.n.1tn.1 . 
. 
.n.1 
. 
.Kn+1 

+.1t +.0 . 
.. 


.0 

If B1 and B2 are bases of the finite dimensional vector spaces V and W, respectively, 
then we can illustrate the meaning and the construction ofthe matrix representation 
[fB1 B2 of f .L(VW in thefollowing commutativediagram: 

f 

VW 

B1 

B2 

[fB1 B2

Km 1Kn 1 

We see that different compositions of maps yield the same result. In particular, we 
have 

.1

f . 
.[fB1 B2 .B1 (10.3)

B2 


10.2 Linear Maps andMatrices 
147 
wherethe matrix [fB1 B2 .Kn m is interpreted as a linear map from Km 1 to Kn 1, 
and we usethat the coordinatemap B2is bijectiveand henceinvertible.Inthe same 
waywe obtain 

B2 . 
f =[fB1 B2 .B1 

i.e., 

B2( 
f(v 
=[fB1 B2B1(v 
for all v 
.V. (10.4) 

Inwords, the coordinatesof f(v 
with respect to the basis B2 of W aregivenby the 
product of [fB1 B2 and the coordinates of v 
with respect to the basis B1 of V. 

We next show that the consecutive application oflinear maps corresponds to the 
multiplication oftheir matrix representations. 

Theorem 10.19 Let V, W and X beK-vector spaces.Iff v 
L(VW and g 
v 
L(WX ,then g 
. 
f .L(VX .Moreover,ifV, W and X are finite dimensional 
withrespective basesB1,B2 andB3,then 

[g 
. 
fB1 B3 =[g 
B2 B3 [fB1 B2 

Proof Let h :=g 
. 
f.We show firstthat h .L(VX .For u v 
.V and .. 
.K 
we have 

h(.u +.v 
=g( 
f(.u +.v 
=g(. 
f(u +. 
f(v 
=.g( 
f(u +.g( 
f(v 
=.h(u +.h(v 


Now let B1 ={v1 ,...,vm}, B2 ={w1 ,...,wn. 
and B3 ={x1 xs}.If 
[fB1 B2 =[fij and [g 
B2 B3 =[gij ,then for j=1 m we have 

nn ns 

h(v 
j =g( 
f(v 
j =g 
fkjwk . 
fkjg(wk . 
fkj gikxi 

k=1 k=1 k=1 i=1 

sn sn 

. 
fkjgik xi . 
gikfkj xi 
i=1 k=1 i=1 k=1 


=:hij 

Thus, [hB1 B3 =[hij ]=[gij ][fij ]=[g 
B2 B3 [fB1 B2. 
. 
	

Using this theorem we can study how a change of the bases affects the matrix 
representation ofa linear map. 


148 10 Linear Maps 

Corollary 10.20 Let V and W be finite dimensional K-vector spaces with bases 
B1,B1 of V andB2,B2 of W.If f .L(VW ,then 

[fB1 B2 =[IdW 
[f 


[IdV B1 

(10.5)

B2 B2B1 B2B1 
In particular,the matrices [fB1 B2 and [f 


are equivalent.

B1 
B2 

Proof Applying Theorem 10.19 twicetothe identity f =IdW . 
f .IdV yields 

[fB1 B2 =[(IdW . 
f .IdV B1 B2 
=[IdW . 
f 

[IdV B1 


B1 B2 B1 

=[IdW 
[f 


[IdV B1 


B2 B2 B1 B2 B1 
Thematrices [fB1 B2 and [f 



areequivalent,since both [IdW 

and [IdV B1 


B1 B2 B2 B2 B1 

areinvertible. . 
	
If V =W, B1 =B2, and B1 =B2,then(10.5)becomes 

[fB1 B1 =[IdV 
[f 


[IdV B1 
=([IdV B1 

.1[f 


[IdV B1 


B1 B1B1 B1B1 B1 B1 B1B1 
Thus, the matrix representations [fB1 B1 and [f 



of the endomorphism f .

B1 B1 

L(VV are similar (cp. Definition 8.11). 
Thefollowing commutative diagram illustratesCorollary 10.20: 

[fB1 B2

Km 1 Kn 1 (10.6) 

B1 B2 . 


f

[IdV B1 


[IdW 


VW

B1 

B2 B2

. 

. 
. 



B1 
B2 


.
.
[f 


. 


B1 B2

Km 1 Kn 1 
Analogously to(10.3)wehave 

.1 .1

f =.[fB1 B2 . 
B1 =.[f 


.


B2 


B1 B2 B1

B2 

Example 10.21 Forthe following basesof thevector spaceQ2 2 , 

. 
. 
. 
. 
. 


10 01 0000

B1 . 


00 00 1001 

. 
. 
. 
. 
. 


10 10 11 00

B2 . 


01 0000 10 


10.2 Linear Maps andMatrices 149 
wehave the coordinatetransformationmatrices 

.. 


000 1 

..

1.10 .1 

..

[IdV B1 B2 . 


..

010 0 

001 0 

and 

.. 


1110 

..

0010 

[IdV B2 B1 . 
([IdV B1 B2 .1 . 
.. 


..

0001 

1000 

The coordinate maps are 

B1 ). 
a11 a12 
a21 a22 
. 
. 
. 
. 
. 
. 
a11 
a12 
a21 
a22 
. 
. 
. 
. 
,B2 ). 
a11 a12 
a21 a22 
. 
. 
. 
. 
. 
. 
a22 
a11 . 
a12 . 
a22 
a12 
a21 
. 
. 
. 
. 
and one can easily verify that 
). 
a11 a12 . 
). 
a11 a12 . 


B2 . 
([IdV B1 B2 . 
B1

a21 a22 a21 a22 

Theorem 10.22 Let Vand WbeK-vector spaces with dim(V . 
m anddim(W . 
n,respectively. Then thereexist basesB1 of V andB2 of W suchthat 

v 
Kn m 
[ 
fB1 B2 . 
Ir 0 

00 

where 0 . 
r . 
dim(im( 
f . 
min{nm}.Furthermore, r . 
rank(F , where F is 
the matrix representation of f with respect to arbitrary bases of V and W, and we 
define rank( 
f :. 
rank(F . 
dim(im( 
f. 

Proof Let B1 ={
v1 

vm. 
and B2 ={w


1 w


n. 
be two arbitrary bases of V and W, respectively. Let r :. 
rank([ 
f 


. Then by Theorem 5.11 there exist 

B1 
B2 
invertible matrices Qv 
Kn n and Z v 
Km m with 

Q[ 
f 


Z . 
Ir 0 
(10.7)

B1 
B2 

00 


150 10 Linear Maps 

where r . 
rank([f 

. 
min{nm}. Let us introduce two new bases B1 =

B1 
B2 

{v1 ,...,vm}and B2 ={w1 ,...,wn}of V and W via 

(v1 ,...,vm :=(

v1 

vm Z 
(w1 ,...,wn :=(


w1 


wn Q.1 hence (


w1 


wn =(w1 ,...,wn Q 
Then, by construction, 
Z =[IdV B1 
B1 Q=[IdW 
B2 B2 

From(10.7)and Corollary10.20 we obtain 

Ir 0 
=[IdW 
[f 

[IdV B1 
=[fB1 B2

B2 B2 B1 
B2 B1

00 

We thus have found bases B1 and B2 that yield the desired matrix representation 
of f.Every other choiceof bases leads,byCorollary 10.20,toan equivalent matrix 
whichthereforealso has rank r.Itremains to show that r =dim(im( 
f . 

Thestructureofthe matrix [fB1 B2 showsthat 

w 
j 1. 
j.r 

f(v 
j . 


0 r +1. 
j.m 

Therefore, vr+1 ,...,vm .ker( 
f ,which impliesthat dim(ker( 
f .m .r.Onthe 
other hand, w1 w 
j .im( 
f and thus dim(im( 
f .r.Theorem 10.9 yields 

dim(V =m =dim(im( 
f +dim(ker( 
f 

and hence dim(ker( 
f =m .r and dim(im( 
f =r. . 
	

Example 10.23 For A.Kn m and the corresponding map A.L(Km 1 Kn 1 from 

(1)inExamples 10.2and 10.6,wehaveim(A =span{a1 am}.Thus, rank(A 
is equal to the number of linearly independent columns of A. Since rank( 
A . 
rank(AT (cp. (4)inTheorem 5.11), this number is equal to the number of linearly 
independent rows of A. 
Theorem 10.22 is a first example of a general strategy that we will use several 
timesin thefollowing chapters: 

By choosing appropriate bases,the matrix representationshouldreveala desired 
information abouta linear mapinanefficientway. 

In Theorem 10.22this informationistherankofthelinearmap f, i.e., thedimension 
of its image. 

Thedimensionformulafor linearmapscanbe generalizedtothe compositionof 
maps as follows. 


10.2 Linear Maps andMatrices 151 
Theorem 10.24 If V, W and X are finite dimensional K-vector spaces, 
f .L(VW and g 
.L(WX ,then 
dim(im(g 
. 
f =dim(im( 
f .dim(im( 
f .ker(g 
Proof Let 


g 
:=g|im( 
f be the restriction ofg 
to theimageof f, i.e., the map 


g 
.L(im( 
f X v 
>g(v 


Applying Theorem 10.9 to 


g 
yields 
dim(im( 
f =dim(im(


g 
+dim(ker(


g 
Now 
im(


g 
={g(v 
.X |v 
.im( 
f }=im(g 
. 
f 
and 
ker(


g 
={v 
.im( 
f |
g(v 
=0}=im( 
f .ker(g 
implythe assertion. . 
	
Note that Theorem 10.22 with V . 
W, f . 
IdV, and g 
v 
L(VX gives 
dim(im(g 
=dim(V .dim(ker(g 
, which is equivalent to Theorem 10.9. 
If we interpret matrices A v 
Kn m and B v 
Ks n as linear maps, then Theorem 
10.24 impliesthe equation 
rank(BA =rank(A .dim(im( 
A .ker(B 
Forthe special case K =R and B =AT wehave thefollowing result. 
Corollary 10.25 If A.Rn m,then rank(ATA =rank(A. 
Proof Let w 
=[.1 .nT v 
im( 
A .ker( 
AT . Then w 
. 
Ay for a vector 
y .Rm 1.Multiplyingthis equationfromthe leftby AT,and usingthatw 
.ker( 
AT , 
we obtain0=AT w 
=AT Ay, which implies 
n 

T

0=yTAT Ay =ww 
. 
.2 
j 
j=1 

Since this holds onlyfor w 
=0, wehaveim(A .ker(AT ={0}. . 
	


152 
10 Linear Maps 

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

.. 


201 

10.1 Consider the linear map on R31 givenbythe matrix A . 
.
210 
. 
v 
R33 . 
411 
Determineker (A ,dim(ker(A and dim(im(A . 

10.2 Construct 
a map f v 
L(VW such that for linearly independent vectors 
v1 ,...,vr v 
V theimages f(v1 f(vr v 
W are linearly dependent. 
10.3 The map 
f . 
R[t .n > 
R[t .n.1 

.ntn . 
.n.1tn.1 +. 
.1t . 
.0 
> 
n.ntn.1 . 
(n . 
1 .n.1tn.2 +. 
2.2t . 
.1 

is called the derivative of the polynomial p v 
R[t .n with respect to the 
variable t. Showthat f is linear and determineker( 
f and im( 
f . 

. 
. 


. 
.. 
.. 
. 


. 
. 
. 


. 
100 
. 


10

10.4 For the bases B1 . 
. 
0.. 
1.. 
0. 
of R31 and B2 . 
.. 
01

001 
of R21,let f v 
L(R31 R21 have the matrix representation [ 
fB1 B2 . 


023 

.

1.20 

. 
. 


. 
.... 
. 


. 
21 .1 
. 



. 
.... 
.
(a) Determine [ 
f 


for the bases B1 . 
102 of
B1 
B2 

. 
.

.13 1 

. 
. 
. 


11 

of R21 

R31 and B

2 . 
.

1 .1 

T 



(b) Determine the coordinates of f([4 1 3 with respect to the basis B2. 
10.5 Construct a map f v 
L(K[tK[t with thefollowing properties: 
(1) f( 
pq . 
( 
f( 
pq . 
p( 
f(q for all pq v 
K[t . 
(2) f(t . 
1. 
Is this map uniquely determinedby thesepropertiesorare therefurther maps 
with thesameproperties? 

10.6 Let . 
v 
K and Av 
Kn n. Showthat themaps 
K[t > 
Kp 
> 
p(. 
and K[t > 
Kmm p 
> 
p( 
A 

are linear and justifythe name evaluation homomorphism forthis map. 

10.7 Let S v 
GLn(K . Showthat themap f . 
Kn n > 
Knn A 
> 
S.1AS is an 
isomorphism. 

10.2 
Linear Maps andMatrices 153 
10.8 Let K bea fieldwith1 +1=0and let A.Kn n.Consider the map 
f :Kn 1 >Kx >xT Ax 

Is f a linear map? Showthat f =0ifand onlyif A+AT =0. 

10.9 Let 
V be a Q-vector space with the basis B1 ={v1 ,...,vn}and let f v 
L(VV be defined by 
v 
j +v 
j+1 j=1 n .1

f(v 
j 
. 


v1 +vnj=n 

(a) Determine [fB1 B1. 
(b) Let B2 ={w1 ,...,wn}with w 
j . 
jvn+1.j, j =1 n. Show that 
B2 is a basis of V. Determine the coordinate transformation matrices 
[IdV B1 B2 and [IdV B2 B1, as well as the matrix representations [fB1 B2 
and [fB2 B2. 
10.10 
Can you extend Theorem 10.19 consistently to the case W ={0}?What are 
thepropertiesofthe matrices [g 
. 
fB1 B3, [g 
B2 B3 and [fB1 B2? 
10.11 
Consider the map 
f . 
R[t .n >R[t .n+1 

.ntn +.n.1tn.1 ++.1t +.0 
> 
1 
.ntn+1 

n +1 

. 
1 
.n.1tn +. 
1 
.1t2 +.0t 

n 2 

(a) Showthat f is linear.Determineker( 
f and im( 
f . 
(b) Choose bases B1, B2 in the two vector spaces and verify that for your 
choice rank([fB1 B2 =dim(im( 
f holds. 
10.12 
Let .1 .n .R,n .2,bepairwisedistinct numbersandletnpolynomials 
in R[t be defined by 
n 
). 


1 

pj . 
(t ..kj=1 n 
k=1 .j ..k 

k=j 

(a) Showthat theset B ={p1 pn}is a basis of R[t .n.1.(This basis is 
calledthe Lagrange basis2 of R[t .n.1.) 
(b) Showthat the corresponding coordinatemapisgivenby 
2Joseph-LouisdeLagrange (1736–1813). 


154 
10 Linear Maps 

.. 


p(.1 
B . 
R[t .n.1 > 
Rn 1 p 
> 


.. 


p(.n 

(Hint: You can useExercise7.8 (b).) 

10.13 
Verifydifferentpathsinthe commutativediagram(10.6)forthevector spaces 
and bases of Example 10.21 and linear map f . 
Q22 > 
Q2 2 , A
> 
FA with 
11

F . 
.

.11 


Chapter11 
LinearForms and BilinearForms 

Inthischapterwestudydifferent classesofmaps betweenoneortwo K-vector spaces 
and the one dimensional K-vector space defined by the field K itself. These maps 
play an important role in manyareas of Mathematics, including Analysis, Functional 
Analysis and thesolutionofdifferential equations.They will alsobe essentialfor 
thefurtherdevelopmentsinthis book: Usingbilinear and sesquilinear forms, which 
areintroducedin this chapter,wewill defineand study Euclideanand unitaryvector 
spacesin Chap. 12.Linear forms and dual spaces willbe usedin theexistence proof 
of theJordan canonical formin Chap.16. 

11.1 LinearFormsand Dual Spaces 
We start with the set of linear maps from a K-vector space to the vector space K. 

Definition 11.1 If V is a K-vector space, then f . 
L(V K is called a linear form 
on V.The K-vector space V. 
:. 
L(V K is calledthe dual space of V. 

Alinear formis sometimes calledalinear functional ora one-form,which stresses 
that it (linearly) maps into a one dimensional vector space. 

Example 11.2 If V is the R-vector spaceofthe continuousandrealvalued functions 
on the real interval [.. 
and if . 
.[.. 
,then thetwo maps 

f1 . 
V . 
R g 

. 
g(. 


. 


f2 . 
V . 
R g 

. 
g(x dx 

. 


are linear forms on V. 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_11 

156 11 LinearFormsand BilinearForms 

If dim(V =n,then dim(V.=n by Theorem 10.16.Let B1 =} 
1 n}be 
a basis of V and let B2 ={1}be a basis of the K-vector space K.If f .V., then 
f( 
i =.i for some .i .K, i =1 n, and 

n

[fB1 B2 =[.1 .n ].K1 

n 

For an element . 
.ii .V we have 
i=1 

.. 
. 
n 
. 
nn 

.1 

..

f( 
. 
f .ii . 
.if( 
i . 
.i.i =[.1 .n 
.. 


i=1 i=1 i=1 

n

.K1 .n 

.Kn 1 

=[fB1 B2 B1( 


wherewehaveidentified theisomorphicvector spaces K and K11 with each other. 
For a given basis of a finite dimensional vector spaceV we will now construct a 
special, uniquely determined basis of the dual space V. 
. 

Theorem 11.3 If V is K-vector space with the basis B =} 
1 n},then there 

..

existsa unique basisB.. 
of V. 
suchthat

1 n 

. 


i ( 
j =.ij ij=1 n 

whichis calledthe dual basis ofB. 

Proof By Theorem 10.4,a unique linear mapfrom V to K can be constructed by 
prescribing its images at thegiven basis B.Thus,for each i =1 n,thereexists 

..

a unique map i .L(V K with i ( 
j =.ij, j=1 n. 

.

It remains to show that B.:=} 
. 
}is a basis of V..If.1 .n . 
K

1 n

are such that 

n 

.

.ii =0V..V. 


i=1 

then 

n 

.

0=0V.( 
j . 
.ii ( 
j =.jj=1 n 

i=1 

..

Thus, are linearly independent, and dim(V.. 
n implies that B. 
is a

1 n 

basis of V. 
(cp. Exercise 9.6). . 


Example 11.4 Consider V . 
Kn 1 with the canonical basis B ={e1 en}.If 

... 
.

eeis the dual basis of B,then ei (ej =.ij,which showsthat e=

1 n iB {1. 
Tn


ei .K1, i =1 n. 


11.1 LinearFormsand Dual Spaces 
157 
Definition 11.5 Let V and W be K-vector spaces with their respective dual spaces 
V. 
and W., and let f .L(VW .Then 

f.. 
W.>V. 
h 
. 
f.(h :=h. 
f 

is calledthe dual map of f. 

We next derivesomepropertiesofthe dual map. 

Lemma 11.6 If V, W and X are K-vector spaces, then the following assertions 
hold: 

(1) Iff .L(VW ,then the dual map f. 
is linear, hence f..L(W. 
V. 
. 
(2) Iff .L(VW and g 
.L(WX ,then (g 
. 
f ..L(X. 
V. 
and (g 
. 
f .. 
.
f..g. 

(3) If f 
.L(VW is bijective, then f..L(W. 
V. 
is bijective and ( 
f..1 . 
( 
f.1 . 
. 

Proof (1) Ifh1 h2 .W. 
, .1 .2 .K,then 

f.

(.1h1 +.2h2 =(.1h1 +.2h2 . 
f =(.1h1 . 
f +(.2h2 . 
f 

=.1(h1 . 
f +.2(h2 . 
f =.1 f.(h1 +.2 f.(h2 

(2) and (3) are exercises. 
As thefollowing theorem shows, the conceptsofthe dual map and thetransposed 
matrix are closely related. 

Theorem 11.7 Let V and W be finite dimensional K-vector spaces with bases 
B1 and B2, respectively. Let B1 . 
and B2 . 
be the corresponding dual bases. If 

f .L(VW ,then 

T

[f. 
B. 


2 B.. 
([fB1 B2

1 

..

Proof 
. 
Let B1 ={
. 
1 m}, B2 ={w1 ,...,wn}, and let B1 .. 
1 m , 

..

B2 .. 
w,...,wn .Let[fB1 B2 =[aij ].Kn m, i.e.,

1 

n 

f( 
j . 
aijwij=1 m 

i=1 

and [f. 
B2 . 
B.].Km n, i.e.,

1 =[bij 

m 

f.. 
. 


w 
j . 
biji j=1 n 

i=1 


158 11 LinearFormsand BilinearForms 

For everypair (k . 
with1. 
k . 
n and1. 
. 
. 
m we then have 

n 
. 
n 
. 


.. 
..

ak. 
. 
aiwk(wi . 
wk aiwi . 
wk( 
f(v. 
. 
f. 
wk (v. 


i=1 i=1 
. 
m 
. 
m 


. 
bik i . 
(v. 
. 
bik i .(v. 
i=1 i=1 


. 
bk 

.

where we have used the definition of the dual map as well as wk(wi . 
.ki and 

. 


i (v. 
. 
.i. . 


Becauseofthe closerelationship between thetransposed matrix and the dual map, 
some authors callthe dual map f. 
the transpose of the linear map f. 
Applied to matrices, Lemma 11.6 and Theorem 11.7 yield the following rules 
known fromChap. 4: 

(AB T . 
BTAT for A. 
Kn m and B . 
Km ,, and 

(A.1 T . 
( 
AT .1 for A. 
GLn(K . 

Example 11.8 Forthetwo basesofR21 , 

. 
. 
. 
. 
. 
. 


10 11

B1 . 
1 . 
2 . 
B2 . 
w1 . 
,w2 =

02 01 

theelementsofthe corresponding dual bases aregivenby 

. 
21 . 
R 
.1 . 
21 . 
R 
.11 

1 . 
R
. 
.1 . 
02 . 
R
. 
0. 
.2

.2 .22 

. 
21 . 
R 
.1 . 
21 . 
R 
.1 

w1 . 
R
. 
.1 . 
.2 ,w2 . 
R
. 
0. 
.2

.2 .2 

The matrix representations of these maps are 

.. 


B1 {1}. 
10 B1 {1}. 
01

12 

.. 


w 
. 
10 w 
. 
01

12

B2 {1. 
B2 {1. 


Forthe linear map 

21 . 
R21 .1 .1 . 
.2

f . 
R
. 


.23.2 


11.1 LinearFormsand Dual Spaces 159 
we have 
1.4 10

[fB1 B2 =[f. 
B2 . 
B1 .=

06 .46 

11.2 BilinearForms 
We now consider special maps froma pairofK-vector spaces to the K-vector space 

K. 
Definition 11.9 Let V and W be K-vector spaces.Amap . 
:V.W >K is called 
a bilinear form on V .W, when 

(1) .( 
1 . 
2 w 
=.( 
1 w 
+.( 
2 w 
, 
(2) .(vw1 +w2 =.(vw1 +.(vw2, 
(3) .(. 
w 
=.( 
.w 
=..(vw 
, 
holdfor all 1 2 .V, w,w1 ,w2 .W, and . 
.K. 

Abilinear form. 
is called non-degenerateinthe first variable,if .(vw 
=0for 
all w 
.W impliesthat =0. Analogously,itiscallednon-degenerateinthe second 
variable,if .(vw 
=0for all .V implies that w 
=0.If . 
is non-degenerate 
in bothvariables,then . 
is called non-degenerate and the spaces VW are called a 
dual pair with respect to .. 

If V . 
W, then . 
is called a bilinear form on V. If additionally .(vw 
. 
.(w 
holds for all w 
.V, then . 
is called symmetric.Otherwise, . 
is called 
nonsymmetric. 

Example 11.10 

(1) If A.Kn m,then 
. 
. 
Km 1 .Kn 1 >K ,(w 
>w 
TA 

isabilinear form on Km 1 .Kn 1 that is non-degenerateifand onlyif n =m 
and A.GLn(K ,(cp. Exercise 11.10). 

(2) The bilinear form 
21 .R21 >R T 11 

. 
. 
R,(xy >yx

11 

isdegeneratein bothvariables:For 

x =[1 .1 T,wehave .(
xy =0for all 
y .R21;for 

y =[1 .1 T we have .(x 

y =0for all x .R2 1.The setof 
all x =[x1 x2 
T . 
R21 with .(xx =1is equal to the solution set of the 

22

quadratic equationintwovariables x1 +2x1x2 +x2 =1, or (x1 +x22 =1, for 
x1 x2 .R.Geometrically,thissetisgivenbythetwostraight lines x1 +x2 =1 
and x1 +x2 =.1inthe cartesian coordinatesystemofR2. 


160 11 LinearFormsand BilinearForms 

(3) If V is a K-vector space, then 
. 
. 
V .V.>K ( 
f 
. 
f( 


isabilinear form on V .V., since 

.( 
1 . 
2 f . 
f( 
1 . 
2 . 
f( 
1 . 
f( 
2 =.( 
1 f +.( 
2 f 

.( 
f1 . 
f2 =( 
f1 . 
f2 ( 
. 
f1( 
. 
f2( 
=.( 
f1 +.( 
f2 

.(. 
f . 
f(. 
=. 
f( 
=..( 
f =(. 
f ( 
=.( 
. 
f 

hold for all 1 2 . 
V, ff1 f2 . 
V. 
and . 
. 
K. This bilinear form is 
non-degenerate and thus VV. 
are a dual pair with respect to . 
(cp. Exercise 

11.11 forthe casedim(V .N). 
Definition 11.11 Let V and W be K-vector spaces with bases B1 =} 
1 m. 
and B2 ={w1 ,...,wn}, respectively. If . 
isabilinear form on V .W,then 

].Kn m 

[. 
B1.B2 =[bij bij :=.( 
j ,wi 

is calledthe matrix representation of . 
with respect to the bases B1 and B2. 

mn

If . 
j=1 .jj .V and w 
. 
i=1 .iwi .W,then 

mn nm 

.(vw 
. 
. 
j.i.( 
j ,wi . 
.i bij. 
j . 
. 
B2(w 
T [. 
B1.B2 B1( 
j=1i=1 i=1 j=1 


wherewehaveusedthe coordinatemap from Lemma 10.17. 

(m (m (n (n

Example 11.12 If B1 . 
eeand B2 . 
eeare the canon


1 m 1 n 
ical bases of Km 1 and Kn 1, respectively, and if . 
is thebilinear form from(1) in 
Example 11.10 with A=[aij ].Kn m,then [. 
B1.B2 =[bij , where 

(m (n (nT

bij =. 
ej ei . 
ei Ae(
jm =aij 

and hence [. 
B1.B2 =A. 

Thefollowing result showsthat symmetric bilinear formshavesymmetric matrix 
representations. 

Lemma 11.13 For a bilinear form . 
on a finite dimensional vector space V the 
following statements are equivalent: 

(1) . 
is symmetric. 
(2)Forevery basisB of V the matrix [. 
B.B is symmetric. 
(3) Thereexistsa basisB of V suchthat [. 
B.B is symmetric. 

11.2 BilinearForms 161 
Proof Exercise. . 


We will now analyze the effect of a basis change on the matrix representation of 
abilinear form. 

Theorem 11.14 Let V and W be finite dimensional K-vector spaces with bases 
B1 B1 of V andB2 B2 of W.If. 
isabilinear form on V .W,then 

T

=[IdW B2 [. 
[IdV B1 

[. 
B1.B2 B2 B1.B2 B1 

Proof Let B1 =} 
1 m}, B1 ={
1 

m}, B2 ={w1 ,...,wn}, B2 . 


{w

1 w

n}, and 

( 
1 m =(

1 

mP where P =[pij ]=[IdV B1 B
1 

(w1 ,...,wn =(w

1 w

nQ where Q=[qij ]=[IdW B2 

B2 

With[. 
B1.B2 =[bij , where

j w

i , wethen have


bij =.(
. 
mn 
. 
nm 

.( 
j ,wi =. 
pkj

kqiw
. 
. 
qi .(

k w
. 
pkj 
k=1 =1 =1 k=1 
nm 

. 
qibkpkj 

=1 k=1 
.. 
..

T q1ip1j 

.. 
..

=
.. 
[. 
B1.B2 .. 


qni pmj 

whichimpliesthat [. 
B1.B2 =QT[. 
B1.
P, and hence theassertionfollows. . 


B2 

If V =W and B1 B2 are two bases of V, then we obtain the following special 
caseofTheorem 11.14: 

T

[. 
B1.B1 =[IdV B1 B2 [. 
B2.B2[IdV B1 B2 

Thetwomatrix representations [. 
B1.B1 and [. 
B2.B2 of . 
in this caseare congruent, 
whichweformally define as follows. 

Definition 11.15 Iffortwomatrices AB .Kn n thereexistsamatrix Z .GLn(K 
with B =ZTAZ,then Aand B are called congruent. 

Lemma 11.16 Congruenceisan equivalencerelationonthesetKnn . 

Proof Exercise. . 



162 
11 LinearFormsand BilinearForms 

11.3 SesquilinearForms 
For complex vector spaces we introduce another special classofforms. 

Definition 11.17 Let V and W be C-vector spaces.Amap s . 
V.W . 
Cis called 
a sesquilinear form on V . 
W, when 

(1) s( 
1 . 
2 w 
. 
s( 
1 w 
. 
s( 
2 w 
, 
(2) s(. 
w 
. 
.s(vw 
, 
(3) s(vw1 . 
w2 . 
s(vw1 . 
s(vw2, 
(4) s( 
.w 
. 
.s(vw 
, 
holdfor all 
1 2 . 
V, w,w1 ,w2 . 
W and . 
. 
C. 
If V . 
W, then s is called a sesquilinear form on V.Ifadditionally s(vw 
. 
s(w 
holds for all w 
. 
V,then s is called Hermitian.1 

Theprefix sesqui is Latin and means “one anda half”.Notethatasesquilinear 
formis linearinthefirstvariableand semilinear (“half linear”)inthesecondvariable. 

Thefollowing result characterizes Hermitiansesquilinear forms. 

Lemma 11.18 Asesquilinear form on theC-vector space Vis Hermitianifand only 
if s( 
. 
R for all . 
V. 

Proof If s is Hermitianthen, in particular, s( 
. 
s( 
for all . 
V, and thus 
s( 
. 
R. 

If, on the other hand, w 
. 
V,then by definition 

s( 
. 
w 
. 
w 
. 
s( 
. 
s(vw 
. 
s(w 
. 
s(w 
w 
(11.1) 
s( 
. 
iw 
. 
iw 
. 
s( 
. 
is(w 
. 
is(vw 
. 
s(w 
w 
(11.2) 

The first equation implies that s(vw 
. 
s(w 
. 
R, since s( 
. 
w 
. 
w 
s( 
s(w 
w 
. 
R by assumption. Thesecond equationimplies analogously 
that is(w 
. 
is(vw 
. 
R.Therefore, 

s(vw 
. 
s(w 
. 
s(vw 
. 
s(w 
.is(vw 
. 
is(w 
. 
is(vw 
. 
is(w 


Multiplyingthe second equationwith iand adding theresulting equationtothe first 
we obtain s(vw 
. 
s(w 
. 


Corollary 11.19 Fora sesquilinear formsontheC-vector space V we have 

2s(vw 
. 
s( 
. 
w 
. 
w 
. 
is( 
. 
iw 
. 
iw 
. 
(i. 
1 )(s( 
. 
s(w 
w 


for all w 
. 
V. 

1CharlesHermite (1822–1901). 


11.3 SesquilinearForms 163 
Proof The result follows from multiplication of (11.2)with iand adding the result 
to(11.1). . 


Corollary 11.19 shows that a sesquilinear form ona C-vector space V is uniquely 
determined by the values of s( 
for all .V. 

Definition 11.20 The Hermitian transpose of A=[aij ].Cnm is the matrix 

T .Cmn

AH :=[aij 

If A=AH,then Ais called Hermitian. 

If a matrix Ahas real entries,then obviously AH . 
AT.Thus, a real symmetric 
matrixis also Hermitian.If A=[aij ].Cnn is Hermitian, thenin particular aii =aii 
for i =1 n, i.e., Hermitian matrices have real diagonal entries. 

TheHermitian transposition satisfies similar rules as the(usual)transposition 
(cp. Lemma 4.6). 

A
. 
Cnm,B . 
Cm ,

Lemma 11.21 For A and . 
. 
C the following assertions 
hold: 

(1) (AH H =A. 
(2) (A+A

H =AH +A

H. 
(3) (.AH =. 
AH. 
(4) (AB H =BHAH. 
Proof Exercise. . 


Example 11.22 For A.Cnm the map 

s . 
Cm 1 .Cn 1 >C ,(w 
>w 
HA 

is a sesquilinear form. 

The matrix representation of a sesquilinear form is defined analogously to the 
matrix representationofbilinear forms(cp. Definition 11.11). 

Definition 11.23 Let V and W be C-vector spaces with bases B1 =} 
1 m. 
and B2 ={w1 ,...,wn}, respectively. If s is a sesquilinear form on V .W,then 

].Cnm

[sB1.B2 =[bij bij :=s( 
j ,wi 

is calledthe matrix representation of s with respect to the bases B1 and B2. 

(m (n

(m (n

Example 11.24 If B1 . 
eeand B2 . 
eearethe canonical

1 m 1 n 

bases of Cm 1 and Cn 1,respectively, and s isthesesquilinearformof Example 11.22 
with A=[aij ].Cnm,then [sB1.B2 =[bij with 


164 11 LinearFormsand BilinearForms 

(m (n (nH (nT 

ee. 
eAe(m . 
eAe(m

bij =sji ij ij =aij 

and, hence, [sB1.B2 . 
A. 

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

11.1. Let V beafinite dimensional K-vector space and .V. Showthat f( 
=0 
for all f .V. 
if and onlyif =0. 
11.2. Consider the basis B ={10 t .1 t2 .t}of the3-dimensionalvector space 
R[t .2.Computethe dual basis B. 
to B. 
..

11.3. Let V be an n-dimensional K-vector space and let 1 n be a basis 
of V..Prove or disprove:Thereexistsa unique basis } 
1 n}of V with 
. 


i ( 
j =.ij. 

11.4. Let V be a finite dimensional K-vector space and let f g 
.V. 
with f 
=0. 
Showthat g 
=. 
f for a . 
. 
K \{0}holdsif and onlyifker( 
f =ker(g 
.Is 
it possibletoomitthe assumption f 
=0? 
11.5. Let V be a K-vector space and let U be a subspace of V.The set 
U0 :={f .V.~ 
f(u =0for allu .U. 


is calledthe annihilator of U. Showthe following assertions: 

(a) U0 is a subspace of V. 
. 
(b) For subspaces U1 U2 of V we have 
(U1 +U20 =U10 .U0 ,(U1 .U20 =U10 +U0 

22 

and if U1 .U2,then U20 .U10. 
0

(c) If W is a K-vector space and f .L(VW ,thenker( 
f.=(im( 
f . 
11.6. Prove Lemma 11.6 (2) and (3). 
11.7. Let V and W be K-vector spaces.Showthatthesetofallbilinear formson 
V .W with the operations 
+. 
(.1 +.2 )(vw 
:=.1(vw 
+.2(vw 


·. 
(. 
·. 
)(vw 
:=. 
·.(vw 


is a K-vector space. 

11.8. Let V and W be K-vector spaces with bases } 
1 m}and {w1 ,...,wn. 
. 
..

and corresponding dual bases {. 
}and {ww 
}, respectively. 

1 m1 n

For i =1 m and j=1 n let 

..

.ij :V .W . 
K ,(w 

. 
i (vw 
j(w 


(a) Showthat .ij isabilinear form on V .W. 

11.3 SesquilinearForms 
165 
(b) Show that the set {.ij ~ 
i . 
1 mj . 
1 n. 
is a basis of the 
K-vector space of bilinear forms on V . 
W (cp. Exercise 11.7) and 
determine thedimensionofthisspace. 
11.9. Let V be the R-vector space of the continuous and real valued functions on 
the real interval [.. 
. Showthat 
. 


. 
. 
V . 
V . 
R ( 
f g 

. 
f(x g(x dx 

. 


is a symmetric bilinear form on V.Is . 
degenerate? 

11.10. 
Showthat themap . 
from(1)inExample 11.10isabilinearform,andshow 
thatitis non-degenerateifand onlyif n . 
m and A. 
GLn(K . 
11.11. 
Let V be a finite dimensional K-vector space. Showthat VV. 
is a dual pair 
with respect to thebilinear form . 
from (3)inExample 11.10, i.e., that the 
bilinear form . 
is non-degenerate. 
11.12. 
Let V be a finite dimensional K-vector space and let U . 
V and W . 
V. 
be subspaces with dim(U . 
dim(W . 
1. Prove or disprove:The spaces 
UW formadualpairwith respecttothebilinearform . 
. 
U . 
W . 
K, 
( 
h 
. 
h( 
. 
11.13. 
Let V and W be finite dimensional K-vector spaces with the bases B1 and 
B2, respectively, and let . 
beabilinear form on V . 
W. 
(a) Showthat thefollowing statementsare equivalent: 
(1) [. 
B1.B2 is not invertible. 
(2) . 
isdegenerateinthe secondvariable. 
(3) . 
isdegenerateinthe firstvariable. 
(b) Conclude from (a): . 
is non-degenerate if and only if [. 
B1.B2 is 
invertible. 
11.14. 
Prove Lemma 11.16. 
11.15. 
Prove Lemma 11.13. 
11.16. 
For a bilinear form . 
on a K-vector space V,the map q. 
. 
V . 
K, 

. 
.( 
,is calledthe quadratic form induced by .. Showthe following 
assertion: 
If1+1

. 
0in K and . 
is symmetric,then .(vw 
. 
1(q. 
( 
+w 
.q. 
( 
.
2

q. 
(w 
holds for all w 
. 
V. 

11.17. 
Showthatasesquilinear form s ona C-vector space V satisfies the polarizationidentity 
1
. 
. 


s(vw 
. 
s( 
+w 
+w 
.s( 
.w 
.w 
+is( 
+iw 
+iw 
.is( 
.iw 
.iw 


4 

for all w 
. 
V. 

11.18. 
Consider thefollowing maps from C31 . 
C31 to C: 
(a) .1(xy . 
3x1x1 . 
3y1y1 . 
x2y3 . 
x3y2, 
(b) .2(xy . 
x1y2 . 
x2y3 . 
x3y1, 
(c) .3(xy . 
x1y2 . 
x2y3 . 
x3y1, 

166 
11 LinearFormsand BilinearForms 

(d) .4(xy =3x1y1 +x1y2 +x2y1 +2ix2y3 .2ix3y2 +x3y3. 
Whichoftheseare bilinear formsorsesquilinear formson C31?Testwhether 
the bilinear form is symmetric or the sesquilinear form is Hermitian, and 
derivethe corresponding matrix representationswith respecttothe canonical 
basis B1 ={e1 e2 e3}and the basis B2 ={e1 e1 +ie2 e2 +ie3}. 

11.19. 
Prove Lemma 11.21. 
11.20. 
Let A.Cnn be Hermitian. Showthat 
s . 
Cn 1 .Cn 1 ,(w 
>w 
HA 

isaHermitiansesquilinear form on Cn 1. 

11.21. 
Let V be a finite dimensional C-vector space with the basis B, and let s be 
a sesquilinear form on V. Show that s is Hermitianifand onlyif [sB.B is 
Hermitian. 
11.22. 
Showthe following assertions for AB.Cnn: 
(a) If AH =.A,then the eigenvalues of Aare purelyimaginary. 
(b) If AH =.A,then trace(A2 .0and(trace(A 2 .0. 
(c) If AH =Aand BH =B,then trace((AB 2 .trace(A2B2. 

Chapter12 
Euclideanand UnitaryVector Spaces 

In this chapter we studyvector spacesoverthe fields Rand C.Using the definitionof 
bilinearand sesquilinear forms,we introduce scalar productsonsuchvector spaces. 
Scalar productsallowtheextensionofwell-known conceptsfromelementary geometry, 
such as length and angles, to abstract real and complex vector spaces. This, 
in particular,leadstotheideaof orthogonalityandto orthonormal basesofvector 
spaces.Asanexamplefortheimportanceof these conceptsinmanyapplicationswe 
study least-squares approximations. 

12.1 Scalar Products and Norms 
We startwith the definitionofa scalar product and theEuclidean or unitaryvector 
spaces. 

Definition 12.1 Let V bea K-vector space, whereeither K . 
Ror K . 
C.Amap 

·,·. 
. 
V . 
V > 
K,(v,w) >v,w, 

is called a scalar product on V,when thefollowing properties hold: 

(1) If K . 
R,then ·,·. 
is a symmetric bilinear form. 
If K . 
C,then ·,·. 
is an Hermitian sesquilinear form. 
(2) 
·,·. 
is positive definite, i.e., v,v. 
0holds for allv . 
V, with equality if and 
onlyif v . 
0. 
An R-vector space with a scalar product is called a Euclidean vector space1, anda 
C-vector space with a scalar product is called a unitary vector space. 

Scalar products are sometimes called inner products.Notethat v,v. 
is nonnegative 
and real also when V is a C-vector space.Itis easytosee thatasubspace U of 

1Euclid of Alexandria (approx. 300 BC). 

©SpringerInternationalPublishing Switzerland 2015 
167 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_12 

168 12 Euclidean andUnitaryVector Spaces 

aEuclidean or unitary vector space V is again a Euclidean or unitary vector space, 
respectively, when the scalar product on the space V is restricted to the subspace U. 

Example 12.2 

(1) Ascalar product on Rn,1 isgivenby 
v,w:=w T v. 

Itis calledthe standard scalar product of Rn,1. 

(2) Ascalar product on Cn,1 isgivenby 
v,w:=w H v. 

Itis calledthe standard scalar product of Cn,1. 

(3) For both K =R and K =C, 
A, B:=Spur(BHA) 

is a scalar product on Kn,m . 

(4)Ascalar productonthevector spaceofthe continuousandrealvalued functions 
on the real interval [.,. 
is givenby 
. 


f,g:. 
f(x)g(x)dx. 

. 


We will nowshowhow to usethe Euclidean or unitary structureofavector space 
in orderto introduce geometric conceptssuchasthelengthof avectorortheangle 
between vectors. 

As a motivation of a general concept of length we have the absolute value of 
real numbers, i.e., the map |·~ 
. 
R >R, x >|x|.Thismap has thefollowing 
properties: 

(1) |.x|=|.|·|x|for all .,x .R. 
(2) |x|.0for allx .R,with equalityif and onlyif x =0. 
(3) |x +y|.|x|+|y|for all x, y .R. 
Thesepropertiesare generalizedtorealorcomplexvector spacesasfollows. 
Definition 12.3 Let V bea K-vector space, whereeither K =Ror K =C.Amap 

	·. 
. 
V >R,v >	v	, 


12.1 Scalar Products andNorms 
169 
is called a norm on V, when for all v,w .V and . 
. 
K thefollowing properties 
hold: 

(1) 
	.v	=|.|·	v	. 
(2) 
	v	.0, with equalityif and onlyifv =0. 
(3) 	v +w	.	v	+	w	(triangleinequality). 
A K-vector space on whicha normis definedis calleda normed space. 
Example 12.4 

(1) If ·,·is the standard scalar product on Rn,1,then 
	v	:=v,v1/2 =(vTv)1/2 

definesa normthatis calledthe Euclidean norm of Rn,1. 

(2) If ·,·is the standard scalar product on Cn,1,then 
	v	:=v,v1/2 =(vHv)1/2 

defines a norm that is called the Euclidean norm of Cn,1.(This is common 
terminology,although thespace itselfis unitary and not Euclidean.) 

(3) For both K =R and K =C, 
m 

. 
n 

. 
1/2

1/2 . 
2

	A	F :=(trace(AHA))|aij|
i=1 j=1 

isa normon Kn,m that is called the Frobenius norm2 of Kn,m.For m =1the 
Frobenius normisequaltotheEuclidean normof Kn,1.Moreover,theFrobenius 
norm of Kn,m isequaltotheEuclidean normof Knm,1 (or Knm ), if we identify 
thesevector spaces viaanisomorphism. 
Obviously, wehave 	A	F =	AT	F =	AH	F for all A.Kn,m . 

(4) If V is the vector space of the continuous and real valued functions on the real 
interval [.,. 
,then 
. 
. 
1/2

1/2 =

	f	:=f, f(f(x))2dx 

. 


isa normon V thatis calledthe L2-norm. 

(5) Let 
K . 
R or K . 
C, and let p . 
R, p . 
1 be given. Then for 
v =[.1,..., .nT .Kn,1 the p-norm ofKn,1 is defined by 
n 

. 
1/p 

p

	v	p :. 
|.i|. 
(12.1) 

i=1 

2FerdinandGeorgFrobenius (1849–1917). 


170 12 Euclidean andUnitaryVector Spaces 

For p =2thisisthe Euclidean normon Kn,1.For this normwetypically omit 
theindex2and write 	·	instead of 	·	2 (asin(1) and (2) above).Taking the 
limit p >.in(12.1), we obtainthe .-norm ofK n,1,given by 

	v	. 
. 
max |.i|. 

1.i.n 

Thefollowing figures illustrate the unitcirclein R2,1 with respect to the p-norm, 
i.e., the set of all v .R2,1 with 	v	p =1, for p =1, p =2and p =.: 


(6) For K =R or K =C the p-norm ofKn,m is defined by 
	Av	p

	A	p :. 
sup . 
v.Km,1\{0}	v	p 

Here we usethe p-norm of Km,1 in the denominator and the p-norm of Kn,1 in 
the numerator.The notation sup means supremum, i.e., theleast upper bound 
thatisknownfromAnalysis.Onecanshowthatthesupremumis attainedbya 
vector v, and thus we maywrite maxinsteadof supinthe definition above. 
In particular,for A=[aij ].Kn,m we have 

n 

	A	1 . 
max |aij|, 

1.j.m 

i=1 
m 

	A	. 
. 
max |aij|. 

1.i.n 

j=1 

These norms are called maximum column sum and maximum row sum norm 
of Kn,m, respectively. We easily see that 	A	1 =	AT	. 
=	AH	. 
and 
	A	. 
=	AT	1 =	AH	1.However,for thematrix 

1/2.1/42,2

A=. 
R

.1/22/3 

wehave 	A	1 =1and	A	. 
=7/6.Thus,thismatrix Asatisfies 	A	1 < 	A	. 
and 	AT	. 
< 	AT	1. The 2-norm of matrices will be considered further in 
Chap.19. 


12.1 Scalar Products andNorms 171 
The normsintheaboveexamples (1)–(4)havethe form 	v	=v,v1/2, where 
·,·isagiven scalar product.Wewill show nowthat themap v >v,v1/2 always 
definesa norm. Ourproofis based on thefollowing theorem. 

Theorem 12.5 If V isaEuclideanor unitary vector space withthescalarproduct 
·,·,then 
|v,w|2 .v,v·w, w. 
for all v,w .V, (12.2) 

with equalityif and onlyifv,w are linearly dependent. 

Proof Theinequalityis trivialfor w =0. Thus,letw =0and let 

v,w. 


. 
:. 
. 

w, w. 


Then 

0.v ..w,v ..w=v,v..v,w..w, v..(..)w,w. 
v,wv,w. 
|v,w|2 

=v,v. 
v,w. 
v,w. 
w,w. 


w,ww,ww,w2 
|v,w|2 

=v,v. 
,

w,w. 


whichimplies(12.2). 
If v,w are linearly dependent,then v =.w for a scalar ., and hence 

|v,w|2 =|.w,w|2 =|.w, w|2 =|.|2|w,w|2 =.. 
w,ww,w. 
=.w, .ww,w=v,vw,w. 

On the other hand, let |v,w|2 =v,vw,w.If w . 
0, then v,w are linearly 
dependent.Ifw =0, then we define. 
as above and get 

|v,w|2 

v ..w, v ..w=v,v. 
=0. 

w, w. 


Since thescalar productis positive definite,wehave v ..w =0, and thusv,w are 
linearly dependent. . 


The inequality(12.2)is called Cauchy-Schwarz inequality.3 It is an important 
toolin Analysis,in particular in the estimation of approximation and interpolation 
errors. 

3AugustinLouisCauchy(1789–1857) andHermann Amandus Schwarz(1843–1921). 


172 12 Euclidean and Unitary Vector Spaces 

Corollary 12.6 If V is a Euclidean or unitary vector space with the scalar product 
·,·,then the map 

	·. 
. 
V >R,v >	v	:=v,v1/2 , 

is a norm on V that is called the norm induced by the scalar product. 

Proof We have to prove the three defining properties of the norm. Since ·,·. 
is 
positive definite,wehave 	v	.0, with equality if and onlyifv =0.Ifv .V and 
. 
.K (where in the Euclidean case K =R and in the unitary case K =C), then 

	.v	2 =.v, .v=..v,v=|.|2v,v, 

and hence 	.v	=|.|	v	. In order to show the triangle inequality, we use the 
Cauchy-Schwarz inequality and the fact that Re(z).|z|for every complex number 

z.For all v,w .V we have 
	v +w	2 =v +w,v +w=v,v+v,w+w,v+w,w. 


=v,v+v,w+v,w+w,w. 


=	v	2 +2Re(v,w)+	w	2 

.	v	2 +2|v,w|+	w	2 

.	v	2 +2	v		w	+	w	2 

=(	v	+	w	)2 , 

and thus 	v +w	.	v	+	w	. . 
 

12.2 Orthogonality 
We will now use the scalar product to introduce angles between vectors.As motivation 
we consider the Euclidean vector space R2,1 with the standard scalar product and the 
induced Euclidean norm 	v	=v,v1/2.The Cauchy-Schwarz inequality shows 
that 

v,w. 


.1..1 for allv,w .R2,1 \{0}. 

	v		w. 


If v,w .R2,1 \{0},then the angle between v and w is the uniquely determined real 
number . 
.[0,. 
with 

v,w. 


cos(.). 
. 

	v		w. 



12.2 Orthogonality 173 
The vectors v,w are orthogonal if . 
. 
./2, so that cos(.). 
0. Thus, v,w are 
orthogonalif and onlyif v,w=0. 

An elementary calculation now leads to the cosine theorem for triangles: 

	v.w	2 =v.w,v.w=v,v.2v,w+w,w. 
=	v	2 +	w	2 .2	v		w	cos(.). 

If v,w are orthogonal, i.e., v,w. 
0, then the cosine theorem implies the 
Pythagorean theorem4: 

	v.w	2 =	v	2 +	w	2 . 

The following figures illustrate the cosine theorem and the Pythagorean theorem for 
vectorsin R2,1: 


In the following definition we generalize the ideasof angles and orthogonality. 

Definition 12.7 Let V be a Euclidean or unitary vector space with the scalar product 
·,·. 

(1) In the Euclidean case, the angle between two vectors v,w . 
V \{0}is the 
uniquely determined real number . 
.[0,. 
with 
v,w. 


cos(.). 
. 

	v		w. 


(2) Two vectors v,w .V are called orthogonal,if v,w=0. 
(3) Abasis {v1,...,vn}of V is called an orthogonal basis,if 
vi,vj=0, i, j=1,...,n and i 
. 
j. 

If, furthermore, 

	vi	=1, i =1,...,n, 

where 	v	=v,v1/2 is the norm induced by the scalar product, then 
{v1,...,vn}is called an orthonormal basis of V.(For an orthonormal basis 
we thereforehave vi,vj=.ij.) 

4Pythagorasof Samos(approx. 570–500 BC). 


174 12 Euclidean and Unitary Vector Spaces 

Note that the terms in(1).(3)are defined with respect to the given scalar product. 
Different scalar products yield different angles between vectors. In particular, the 
orthogonality of two given vectors may be lost when we consider a different scalar 
product. 

Example 12.8 The standard basis vectors e1,e2 . 
R2,1 are orthogonal and {e1,e2. 
is an orthonormal basis of R2,1 with respect to the standard scalar product(cp.(1)in 
Example 12.2). Consider the symmetric and invertible matrix 

21 

2,2

A=. 
R,

12 

which defines asymmetric and non-degenerate bilinear form on R2,1 by 

(v,w)
> 
w TAv 

(cp.(1)inExample 11.10). This bilinear formis positive definite,since for all v . 
[.1,.2 
T . 
R2,1 we have 

TAv. 
.22 

v 1 . 
.22 . 
(.1 . 
.2). 

The bilinear form therefore is a scalar product on R2,1,which we denote by ·,·A. 
We denote the induced norm by	·	A. 

With respect to the scalar product·,·A the vectors e1,e2 satisfy 

TTT

e1,e1A . 
e1 Ae1 . 
2, e2,e2A . 
e2 Ae2 . 
2, e1,e2A . 
e2 Ae1 . 
1. 

Clearly, {e1,e2. 
is not an orthonormal basis of R2,1 with respect to ·,·A.Also note 

v 
that 	e1	A =	e2	A . 
2. 

On the other hand, the vectors v1 =[1, 1 T and v2 =[.1, 1 T satisfy 

TTT

v1,v1A . 
v1 Av1 . 
6, v2,v2A . 
v2 Av2 . 
2, v1,v2A . 
v2 Av1 . 
0. 

vv 
Hence 	v1	A . 
6and	v2	A . 
2, so that{6.1/2 v1, 2.1/2 v2. 
is an orthonormal 

basis of R2,1 with respect to the scalar product ·,·A 

We now show that every finite dimensional Euclidean or unitary vector space has 
an orthonormal basis. 

Theorem 12.9 Let V be a Euclidean or unitary vector space with the basis 
{v1,...,vn}. Then there exists an orthonormal basis {u1,...,un. 
of V with 

span{u1,...,uk}. 
span{v1,...,vk}, k . 
1,...,n. 


12.2 Orthogonality 175 
Proof Wegive the proof by induction on dim(V)=n.Ifn =1, then we set u1 :. 
	v1	.1 v1.Then 	u1	=1, and {u1}is an orthonormal basis of V with span{u1}. 
span{v1}. 

Let the assertion hold for an n .1. Letdim(V)=n +1and let{v1,...,vn+1. 
bea basisof V.Then Vn :=span{v1,...,vn}is an n-dimensional subspace of V.By 
the induction hypothesis there exists an orthonormal basis {u1,...,un}of Vn with 
span{u1,...,uk}=span{v1,...,vk}for k =1,...,n.We define 

n 

un+1 :=vn+1 .vn+1,ukuk, un+1 :=||un+1||.1 
un+1. 

k=1 

Since vn+1 ./Vn . 
span{u1,...,un},wemusthave 
un+1 
. 
0, and Lemma 9.16 
yields span {u1,...,un+1}= span {v1,...,vn+1}. 

For j=1,...,n we have 

un+1,uj=	un+1	.1 
un+1,uj. 


n 

=	un+1	.1 vn+1,uj. 
vn+1,ukuk,uj. 


k=1 

=	un+1	.1 vn+1,uj.vn+1,uj. 


=0. 

Finally, un+1,un+1=	un+1	.2un+1,
un+1=1which completes theproof. . 


The proof of Theorem 12.9 shows how a given basis {v1,...,vn}can be orthonormalized, 
i.e., transformed into an orthonormal basis {u1,...,un}with 

span{u1,...,uk}=span{v1,...,vk}, k =1,...,n. 

The resulting algorithm is called the Gram-Schmidt method 5: 

Algorithm 12.10 Given a basis {v1,...,vn}of V. 

(1) Set u1 :=	v1	.1 v1. 
(2) For j=1,...,n .1set 
j 

uj+1 :=vj+1 .vj+1,ukuk, 

k=1 

uj+1 :=	uj+1	.1 
uj+1. 

5Jorgen Pedersen Gram (1850–1916) andErhardSchmidt (1876–1959). 


176 12 Euclidean andUnitaryVector Spaces 

Aslight reorderingand combinationofstepsintheGram-Schmidtmethod yields 

.. 


	v1	v2,u1... vn,u1. 
. 
.. 
. 


. 
.. . 
.

	u2. 
. 

..

(v1,v2,...,vn). 
(u1,u2,...,un). 

.. 


. 
. 
. 
. 

. 
. 
.

. vn,un.1. 
	un. 


.Vn .Vn 

The upper triangular matrixontherighthandsideisthe coordinatetransformation 
matrix fromthe basis {v1,...,vn}to the basis {u1,...,un}of V (cp. Theorem 9.25 
or 10.2). Thus,wehaveshown thefollowing result. 

Theorem 12.11 If Visafinite dimensional Euclidean or unitary vector space witha 
given basisB1,then theGram-Schmidtmethod appliedtoB1 yields an orthonormal 
basisB2 of V, suchthat [IdV B1,B2 is an invertible upper triangular matrix. 

Rn,1

Consider an m-dimensional subspace of or Cn,1 with the standard scalar 
product ·,·,and write them vectorsofanorthonormal basis{q1,...,qm}as columns 
of a matrix, Q:=[q1,...,qm .Then we obtaininthe real case 

T

QTQ=[qi qj ]=[qj,qi. 
=[. 
ji ]=Im, 

and analogously in the complex case 

H

QHQ=[qi qj ]=[qj,qi. 
=[.ji ]=Im. 

If, on theother hand, QTQ=Im or QHQ=Im foramatrix Q.Rn,m or Q.Cn,m , 
respectively, then the m columns of Qform an orthonormal basis (with respect to the 
standardscalar product)ofan m-dimensional subspace of Rn,1 or Cn,1,respectively. 
A“matrixversion”of Theorem 12.11 can thereforebeformulatedasfollows. 

Corollary 12.12 LetK =RorK =Cand let v1,...,vm .Kn,1 be linearly independent. 
Then there exists a matrixQ .Kn,m with itsm columns being orthonormal 
withrespectto the standard scalar product ofKn,1, i.e.,QTQ =Im forK =R or 
QHQ=Im forK =C, and an upper triangular matrixR .GLm(K), suchthat 

[v1,...,vm ]=QR. (12.3) 

The factorization(12.3)is called a QR-decomposition of the matrix [v1,...,vm . 
The QR-decomposition has many applications in Numerical Mathematics (cp. 
Example 12.16 below). 

Lemma 12.13 LetK =R orK =C and letQ .Kn,m be a matrix with orthonormal 
columns withrespecttothestandardscalarproductofK n,1. Then 	v	=	Qv. 
holds for all v.Km,1.(Here 	·	is theEuclidean normofKm,1 and ofK n,1.) 


12.2 Orthogonality 177 
Proof For K =C we have 

HH2

	v	2 =v,v=vv=v(QHQ)v=Qv,Qv=	Qv	, 

and theproof for K =R is analogous. . 


Wenowintroducetwoimportant classesofmatrices. 

Definition 12.14 

(1) Amatrix Q .Rn,n whose columns form an orthonormal basis with respect to 
the standard scalar product of Rn,1 is called orthogonal. 
(2) Amatrix Q .Cn,n whose columns form an orthonormal basis with respect to 
the standard scalar product of Cn,1 is called unitary. 
Amatrix Q=[q1,...,qn ].Rn,n is thereforeorthogonalif and onlyif 

T

QTQ=[qi qj ]=[qj,qi. 
=[. 
ji ]=In. 

In particular, an orthogonal matrix Q is invertible with Q.1 . 
QT (cp. Corollary 
7.20).The equation QQT =In means that the n rows of Qform an orthonormal 
basis of R1,n (with respect to the scalar product v,w:=wv T). 

Analogously, a unitary matrix Q . 
Cn,n is invertible with Q.1 . 
QH and 
QHQ=In =QQH.The n columns of Qform an orthonormal basis of C1,n . 

Lemma 12.15 The sets O(n)of orthogonal and U(n)of unitaryn.n matrices form 
subgroups ofGLn(R)andGLn(C), respectively. 

Proof We consider onlyO(n);the proof forU(n)is analogous. 

Sinceeveryorthogonal matrixisinvertible,wehavethat O(n).GLn(R).The 
identity matrix In is orthogonal, and hence In .O(n)=O. If Q.O(n),then also 
QT . 
Q.1 . 
O(n), since (QT)TQT . 
QQT . 
In.Finally,if Q1,Q2 . 
O(n), 
then 

T

(Q1Q2)(Q1Q2)=Q2 
T(Q1 
TQ1)Q2 =Q2 
TQ2 =In, 

and thus Q1Q2 .O(n). . 


Example 12.16 In many applications measurements or samples lead to a data set 
that is represented by tuples (.i,.i). 
R2, i . 
1,...,m.Here .1 < ··. 
< .m, 
arethe pairwisedistinct measurement points and .1,...,.m arethe corresponding 
measurements.Inorderto approximatethegivendatasetbyasimplemodel,onecan 
tryto constructa polynomial pof smalldegreesothat thevalues p(.1),...,p(.m) 
areasclose as possibletothe measurements .1,...,.m. 

Thesimplest caseisa real polynomialofdegree(at most)1.Geometrically,this 
correspondstothe constructionofa straight linein R2 that has a minimal distance 


178 12 Euclidean andUnitaryVector Spaces 

to the given points, as shown in the figure below (cp. Sect. 1.4). There are many 
possibilitiestomeasurethe distance.Inthefollowingwewill describeoneofthem 
in more detail and usethe Gram-Schmidtmethod, or the QR-decomposition, forthe 
constructionofthe straight line.In Statisticsthismethodis called linearregression. 


Areal polynomialofdegree(at most)1 has theform p . 
.t +. 
and we are 
looking for coefficients .,. 
.R with 

p(.i)=..i +. 
. 
.i, i =1,...,m. 

Using matrices we can write this problem as 

.1 1 . 
. 
. 
. 
. 
.1 . 
. 
. 
. 
. 
. . . 
. . . . 
. 
. 
. 
.. 
. 
. . . . 
. 
or [v1,v2 
. 
. 
. 
y. 
.m 1 .m 

As mentioned above,thereare different possibilitiesfor interpretingthe symbol “.”. 
In particular,therearedifferent normsinwhichwecan measurethe distance between 
thegiven values .1,...,.m and the polynomial values p(.1),...,p(.m).Herewe 
will usethe Euclidean norm 	·	and consider theminimizationproblem 

min [v1,v2 
. 
.y . 

.,..R . 


The vectors v1,v2 . 
Rm,1 are linearly independent, since the entries of v1 are 
pairwisedistinct,while all entriesof v2 are equal.Let 

[v1,v2 ]=[q1,q2 R 

be a QR-decomposition. We extend the vectors q1,q2 . 
Rm,1 to an orthonormal 
basis {q1,q2,q3,...,qm}of Rm,1.Then Q=[q1,...,qm ].Rm,m is an orthogonal 
matrix and 


12.2 Orthogonality 179 
..

min [v1,v2 .y . 
min [q1,q2 R .y 

.,..R ..,..R . 


. 
. 
. 
. 
. 
. 
R . 
. 
. 
min 

Q .y
. 


.,..R 0m.2,2 . 


. 
. 


. 
min 

QR . 
.QTy 
. 


.,..R 0m.2,2 . 


.. 


. 
. 
q1 
Ty 
. 


. 


. 
..

RT 

. 
..
q2 y.

. 


. 
.... 


. 
0 
..
q3 
Ty.

. 
min 
. 
. 
.. 
.. 
. 

.,..R 
. 
. 
.. 
.. 


. 

. 
. 
.. 
. 
.. 


. 

. 
. 
.. 


0 

. 
T 


qy

m 

Here we have used that QQT =Im and 	Qv	=	v	for all v .Rm,1.The upper 
triangular matrix Risinvertible and thus theminimizationproblemis solvedby 

 
q1 
Ty
. 
=R.1 

. 

. qT 

2 y 

Usingthe definitionof the Euclidean norm, we can write theminimizingproperty 

of the polynomial 
  
. 
as

p :=.t +

2 m 

. 


. 
2

[v1,v2 .y . 
(
 p(.i)..i)

. 


i=1 
. 
m 
. 


. 
min ((..i +.)..i)2 . 

.,..R 

i=1 

Since the polynomial 
 
p minimizesthesumof squaresofthedistances betweenthe 
measurements .i and the polynomial values 
 p(.i), this polynomial yields a least 
squares approximation of the measurement values. 

ConsidertheexamplefromSect. 1.4.Inthefour quartersofayear,acompanyhas 
profits of10, 8,9, 11 millionEuros.Under theassumptionthat theprofitsgrows 
linearly,i.e.,likeastraightline,thegoalisto estimatetheprofitinthelast quarter 
ofthefollowingyear.Thegivendataleadstothe approximationproblem 

11 . 
10 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
21 
31 . 
. 
. 
. 
. 
. 
. 
. 
. 
8 
9 . 
. 
. 
or [v1,v2 
. 
. 
.y. 
41 11 


180 12 Euclidean andUnitaryVector Spaces 

The numerical computation ofa QR-decomposition of[v1,v2 yields 

.. 
!. 
10 

. 
. 
vv 
.1 


v1 v2 v3 v4 

..

. 
0.

 
301330 30 30 8 4
30 30 
.

=v 
21.. 
. 
,

. 0 16 vv 
0 .v19. 
8.5

3 666 

. 
. 
. 
11 

=R.1 T

=[q1,q2 

andtheresultingprofit estimateforthelast quarterofthe following yearis 
 p(8). 
11.7, i.e., 11.7millionEuros. 

MATLAB-Minute. 

In Example 12.16 one couldimagine that theprofit growsquadratically instead 
of linearly.Determine, analogouslytotheprocedureinExample 12.16,apoly


nomial 
 
.t2 . 
 

p . 
 
.t . 
. 
that solves theleastsquares problem 

44 
. 


2 

(
 p(.i). 
.i)2 . 
min (..i 
2 . 
..i . 
.). 
.i . 

.,.,..R 

i=1 i=1 

Use the MATLAB command qr for computing a QR-decomposition, and 
determine theestimated profitin thelast quarterofthe following year. 

We will now analyze thepropertiesoforthonormal basesin more detail. 

Lemma 12.17 If V isaEuclideanor unitary vector space withthescalarproduct 
·,·. 
and the orthonormal basis {u1,...,un},then 

n 

v=v,uiui 
i=1 

for all v. 
V. 

Proof For everyv. 
V thereexist uniquely determined coordinates .1,...,.n with 

nn 

v . 
i=1 .iui.For every j . 
1,...,n we then have v,uj. 
i=1 .iui,uj. 
.j. . 


The coordinates v,ui, i . 
1,...,n,ofvwith respect to an orthonormal basis 
{u1,...,un. 
areoften calledthe Fourier coefficients6 of vwith respect to this basis. 

n

The representation v . 
i=1v,uiui is calledthe (abstract) Fourier expansionof 
vin thegiven orthonormal basis. 

6Jean Baptiste JosephFourier(1768–1830). 


12.2 Orthogonality 
181 
Corollary 12.18 If VisaEuclideanorunitary vectorspacewiththescalarproduct 
·,·. 
and the orthonormal basis {u1,..., un},then thefollowing assertions hold: 

n 
n

(1) 
v,w. 
i=1v, uiui,w. 
i=1v, uiw, ui. 
forallv,w . 
V(Parseval’s 
identity7). 
n

(2) 
v,v. 
i=1 |v, ui|2 for all v . 
V (Bessel’sidentity8). 
Proof 

n

(1) Wehave v . 
i=1v, uiui, and thus 
n n n 
. 
. 
. 
. 
. 


v,w. 
v, uiui,w =v, uiui,w. 
v, uiw, ui. 
i=1 i=1 i=1 

(2)isa special caseof(1) for v . 
w. 
. 

By Bessel’sidentity,everyvector v . 
V satisfies 

n 

	v	2 =v,v. 
|v, ui|2 . 
max |v, ui|2 , 

1.i.n 

i=1 

where 	·. 
is the norminducedby thescalar product.The absolutevalueof each 
coordinate ofv with respect to an orthonormal basis of V is therefore bounded by 
the norm ofv.Thisproperty does not holdfora general basisof V. 

Example 12.19 Consider V . 
R2,1 with thestandardscalar product and theEuclidean 
norm, then for every real . 

. 
0the set 

&. 
. 
. 


11 

0 , 
. 


is a basis of V.For every vector v =[.1,.2 
T we then have 

.21 .21 

v . 
.1 .. 
. 

. 
0 .. 


If |.1|,|.2~ 
are moderate numbers and if |.~ 
is (very) small, then |.1 . 
.2/.~ 
and 
|.2/.~ 
are(very)large.In numerical algorithmssuchasituationcanleadto significant 
problems(e.g.dueto roundofferrors)thatareavoidedwhen orthonormalbasesare 
used. 

7Marc-AntoineParseval (1755–1836). 
8FriedrichWilhelm Bessel(1784–1846). 


182 12 Euclidean andUnitaryVector Spaces 

Definition 12.20 Let VbeaEuclideanor unitaryvector spacewiththescalar product 
·,·, and let U .V be a subspace. Then 

U. 
:={v.V|v,u=0 for allu .U. 


is calledthe orthogonal complement of U (in V). 

Lemma 12.21 The orthogonal complement U. 
is a subspace of V. 

Proof Exercise. . 


Lemma 12.22 If V is ann-dimensional Euclidean or unitary vector space, andif 
U .V is anm-dimensional subspace,then dim(U.)=n .m and V =U .U. 
. 

Proof We knowthatm .n (cp. Lemma 9.27). If m =n,then U =V, and thus 

U.=V.={v.V |v,u=0 for allu .V}={0}, 

so that theassertionistrivial. 

Thus let m < n and let {u1,...,um}be an orthonormal basis of U.We extend 
this basis to a basis of V and applythe Gram-Schmidtmethodin orderto obtainan 
orthonormal basis {u1,...,um,um+1,...,un}ofV.Then span{um+1,...,un}.U. 
and therefore V =U +U..Ifw .U .U., then w,w=0, and hencew =0, 
since thescalar productis positive definite.Thus, U .U.={0},which impliesthat 
V . 
U .U. 
and dim(U.). 
n .m (cp. Theorem 9.29).In particular,wehave 
U.=span{um+1,...,un}. . 


12.3 TheVectorProductin R3,1 
Inthis sectionweconsiderafurther productonthevector space R3,1 thatis frequently 
used in Physics and Electrical Engineering. 

Definition 12.23 The vector product or crossproduct in R3,1 is the map 

3,1.R3,1 >R3,1

R,(v,w)>v.w:=[.2.3 ..3.2, .3.1 ..1.3, .1.2 ..2.1]T , 

T

where v=[.1,.2,.3 
T and w=[.1,.2,.3. 

In contrasttothescalar product,thevector productoftwoelementsofthevector 
space R3,1is notascalarbutagainavectorin R3,1.Using the canonical basisvectors 
of R3,1, 

TTT

e1 =[1,0,0 , e2 =[0,1,0 , e3 =[0,0,1 , 


12.3 TheVector ProductinR3,1 183 
we can write the vector product as 
. 
. 
. 


.2 .2 .1 .1 .1 .1 

v .w =det e1 .det e2 +det e3. 

.3 .3 .3 .3 .2 .2 

Lemma 12.24 The vector product is linear in both components and for all v,w . 
R3,1 thefollowing properties hold: 

(1) 
v .w =.w .v, i.e., the vector product is anti commutative or alternating. 
(2) 
	v .w	2 =	v	2 	w	2 .v,w2, where ·,·is the standard scalar product 
and 	·	theEuclidean normofR3,1 . 
(3) 
v,v.w=w, v.w=0,where ·,·is thestandardscalarproductof R3,1 . 
Proof Exercise. 
. 


By(2)andtheCauchy-Schwarzinequality(12.2),it followsthat v.w =0holds 
if and onlyifv,w are linearly dependent.From(3) we obtain 

.v +.w,v .w=.v,v .w+.w,v .w=0, 

for arbitrary .,. 
.R.Ifv,w are linearly independent,then theproduct v .w is 
orthogonaltotheplane throughtheoriginspannedby v and w in R3,1, i.e., 

. 


v .w .{.v +.w |.,. 
.R}. 

Geometrically,therearetwo possibilities: 


The positionsofthethreevectors v,w,v.w on theleftsideofthis figure correspond 
to the “right-handed orientation” of the usual coordinate system ofR3,1, wherethe 
canonical basisvectors e1,e2,e3 areassociated with thumb, index finger and middle 
fingeroftheright hand.Thismotivatesthe name right-hand rule.Inorder toexplain 
this in detail, one needs to introduce the concept of orientation,which we omit here. 

If . 
.[0,. 
is the angle between the vectors v,w,then 

v,w=	v		w	cos(.) 


184 12 Euclidean andUnitaryVector Spaces 

(cp. Definition 12.7)and we can write (2)inLemma12.24 as 

2

	v .w	2 =	v	2 	w	2 .	v	2 	w	2 cos(.)=	v	2 	w	2 sin2(.), 

so that 
	v .w	=	v		w. 
sin(.). 

Ageometricinterpretationofthis equationisthe following:The normofthe vector 
product of v and w is equal to the area ofthe parallelogram spanned by v and w. 

This interpretationis illustratedinthe following figure: 


Exercises 

12.1 Let V be a finite dimensional real or complex vector space. Show that there 
exists a scalar product on V. 
12.2 Showthatthemaps definedin Example 12.2 arescalar productsonthe corresponding 
vector spaces. 
12.3 Let ·, ·be an arbitraryscalar product on Rn,1. Showthat thereexistsamatrix 
A.Rn,n with v,w=w TAv for all v,w .Rn,1. 
12.4 Let V be a finite dimensional R-or C-vector space. Let s1 and s2 be scalar 
products on V with thefollowing property:If v,w .V satisfy s1(v,w) =0, 
then also s2(v,w) . 
0. Prove or disprove:Thereexistsa real scalar. 
> 0 
with s1(v,w) =.s2(v,w) for all v,w .V. 
12.5 Showthat themaps definedin Example 12.4 are norms on the corresponding 
vector spaces. 
12.6 Showthat 
nm 

	A	1 . 
max |aij~ 
and 	A	. 
. 
max |aij~ 


1.j.m 1.i.n 

i=1 j=1 

for all A=[aij . 
Kn,m, where K =Ror K =C(cp. (6)inExample 12.4). 

12.7Sketchfor thematrix Afrom (6)inExample 12.4 and p .{1, 2,.},thesets 
{Av |v .R2,1 , 	v	p =1}.R2,1. 
12.8 Let V beaEuclidean or unitaryvector space and let 	·	be the norminduced 
by a scalar product on V. Showthat 	·	satisfies the parallelogram identity 
	v +w	2 +	v .w	2 =2(	v	2 +	w	2) 

for all v,w .V. 


12.3 TheVector ProductinR3,1 
185 
12.9 Let V be a K-vector space(K =R or K =C)with the scalar product ·,·. 
and theinduced norm 	·	. Showthat v,w .V are orthogonal with respect 
to ·,·if and onlyif 	v+.w	=	v..w	for all . 
. 
K. 
12.10 
Does there exist a scalar product ·,·on Cn,1,such that the1-normof Cn,1 
(cp.(5)inExample 12.4)isthe induced normbythisscalar product? 
12.11 
Showthat theinequality 
n 

. 
n 
2 n 
. 


. 
.i 2 
.i.i . 
(.i.i)2 . 
.i

i=1 i=1 i=1 

holdsforarbitraryreal numbers .1,...,.n,.1,...,.n and positivereal numbers 
.1,...,.n. 

12.12 
Let V beafinite dimensional Euclidean or unitaryvector space with thescalar 
product ·,·.Let f . 
V > 
V be a map with f(v), f(w)=v,wfor all 
v,w .V. Showthat f is an isomorphism. 
12.13 
Let V be a unitary vector space and suppose that f . 
L(V,V)satisfies 
f(v),v=0for allv.V.Prove or disprove that f =0. 
Does thesamestatement also holdfor Euclideanvector spaces? 
12.14 
Let D . 
diag(d1,...,dn). 
Rn,n with d1,...,dn >0. Showthatv,w. 
w TDvisascalar productonRn,1.Analyze whichpropertiesofascalar product 
areviolatedifatleast oneofthe di is zero, or when all di are nonzerobuthave 
different signs. 
12.15 
Orthonormalize the following basis of the vector space C2,2 with respect to 
the scalar product A,B=trace(BHA): 
&. 
. 
. 


10 10 11 11 

,,, .

00 01 01 11 

12.16 
Let Q.Rn,n be an orthogonal or let Q.Cn,n bea unitary matrix.What are 
the possible values of det(Q)? 
12.17 
Let u .Rn,1 \{0}and let 
1 

T . 
Rn,n

H(u). 
In .2 uu. 

uTu 

Show that the n columns of H(u)form an orthonormal basis of Rn,1 with 
respecttothestandardscalar product.(Matricesofthisformare called Householder 
matrices.9We will study themin more detailinExample18.15.) 

12.18 
Prove Lemma 12.21. 
9Alston ScottHouseholder(1904–1993),pioneerof Numerical Linear Algebra. 


186 
12 Euclidean andUnitaryVector Spaces 

12.19 
Let 
.. 
v10 v1
2 
2 

.. 
3,3

[v1,v2,v3 . 
. 
.v10 v1. 
. 
R. 

22 

000 

Analyze whetherthevectors v1,v2,v3areorthonormalwith respecttothestandardscalarproductandcomputetheorthogonal 
complementofspan{v1,v2,v3}. 

12.20 
Let V beaEuclidean or unitaryvector space with thescalar product ·,·,let 
u1,...,uk . 
V and let U . 
span{u1,...,uk}. Showthat for v . 
V we have 
v. 
U. 
if and onlyif v,uj. 
0for j. 
1,...,k. 
12.21 
In the unitary vector space C4,1 with the standard scalar product let v1 . 
[.1, i,0, 1 T and v2 =[i, 0, 2, 0 T be given. Determine an orthonormal 
basis of span{v1,v2}. 
. 
12.22 
Prove Lemma 12.24. 

Chapter13 
Adjointsof LinearMaps 

In this chapter we introduce adjointsoflinear maps.Insomesensetheserepresent 
generalizationsof the(Hermitian) transposesofamatrices.Amatrixis symmetric 
(orHermitian)ifitis equaltoits (Hermitian) transpose.Inan analogousway,an 
endomorphism is selfadjoint if it is equal to its adjoint endomorphism.The sets of 
symmetric (orHermitian) matrices and of selfadjoint endomorphismsform certain 
vector spaceswhichwillplayakeyroleinourproofofthe Fundamental Theoremof 
AlgebrainChap. 15. Special propertiesofselfadjoint endomorphismswillbe studied 
in Chap.18. 

13.1 BasicDefinitionsandProperties 
In Chap.12 we have considered Euclidean and unitary vector spaces, and hence 
vector spaces over the fields R and C.Nowlet V and W be vector spaces over a 
general field K, and let . 
beabilinear form on V . 
W. 

For every fixed vector v . 
V,the map 

.v : 
W . 
K,w 
. 
.(v,w), 

isa linear form on W.Thus,we can assigntoevery v . 
V avector .v . 
W.,which 
defines the map 

.(1) : 
V . 
W . 
,v 
. 
.v. (13.1) 

Analogously, we define the map 

.(2) : 
W . 
V . 
,w 
. 
.w, (13.2) 

where .w : 
V . 
K is defined by v 
. 
.(v,w) for every w . 
W. 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_13 

188 13 Adjoints of Linear Maps 

Lemma 13.1 The maps .(1) and .(2) definedin(13.1)and(13.2),respectively, are 
linear, i.e., .(1) . 
L(V, W. 
)and .(2) . 
L(W, V. 
).Ifdim(V)= 
dim(W). 
Nand 
. 
is non-degenerate(cp. Definition 11.9), then .(1) and .(2) arebijective and thus 
isomorphisms. 

Proof We prove theassertion onlyfor themap.(1);the proof for.(2) is analogous. 

We first show the linearity.Letv1,v2 . 
V and .1, .2 . 
K.For every w . 
W we 
then have 

.(1)

(.1v1 + 
.2v2)(w)= 
.(.1v1 + 
.2v2,w) 

= 
.1.(v1,w)+ 
.2.(v2,w) 

= 
.1.(1)(v1)(w)+ 
.2.(1)(v2)(w) 

= 
.1.(1)(v1)+ 
.2.(1)(v2) (w), 

and hence .(1)(.1v1+.2v2)= 
.1.(1)(v1)+.2.(1)(v2).Therefore, .(1) . 
L(V, W. 
). 

Letnowdim(V)= 
dim(W). 
Nand let . 
be non-degenerate.Weshowthat .(1) . 
L(V, W. 
)is injective.By(5)inLemma 10.7,thisholdsifandonlyifker(.(1))={0}. 
If v . 
ker(.(1)),then .(1)(v)= 
.v = 
0. 
W., and thus 

.v(w)= 
.(v,w)= 
0 for allw . 
W. 

Since . 
is non-degenerate, we have v = 
0. Finally,dim(V)= 
dim(W)and dim(W) 
= 
dim(W. 
)imply that dim(V) = 
dim(W. 
)so that .(1) is bijective(cp. Corollary 
10.11). . 


We next discusstheexistenceof the adjoint map. 

Theorem 13.2 If V and W areK-vector spaces with dim(V)= 
dim(W). 
Nand 
. 
isa non-degeneratebilinear form on V . 
W,then thefollowing assertions hold: 

(1) For every f . 
L(V, V)thereexistsa uniquely determinedg . 
L(W,W)with 
.(f(v),w)= 
.(v, g(w)) for all v . 
V and w . 
W. 

The mapgis calledthe right adjoint of f with respect to .. 

(2) Foreveryh . 
L(W, W)thereexistsa uniquely determinedk . 
L(V,V)with 
.(v, h(w)) = 
.(k(v),w) for all v . 
V and w . 
W. 

The mapkis calledthe left adjoint ofhwithrespect to .. 

Proof Weonlyshow(1);theproofof(2)is analogous. 

.

Let V. 
be the dual space of V,let f. 
L(V. 
, V. 
)be the dual map of f, and 
let .(2) . 
L(W, V. 
)be as in(13.2). Since . 
is non-degenerate, .(2) is bijectiveby 
Lemma 13.1.Define 

g := 
(.(2)).1 . 
f.. 
.(2) . 
L(W, W). 


13.1 Basic Definitions andProperties 189 
Then, for all v . 
V and w . 
W, 

.(v, g(w)) = 
.(v,((.(2)).1 . 
f.. 
.(2))(w)) 

= 
.(2) ((.(2)).1 . 
f.. 
.(2))(w) (v) 

.1. 


= 
.(2)(.(2))(f(.(2)(w))) (v) 

= 
.(2) . 
(.(2)).1 . 
.(2)(w). 
f (v) 
= 
.(2)

(w)( f(v)) 
= 
.(f(v),w). 

(Recallthat the dual mapsatisfies f. 
(.(2)(w)) = 
.(2)(w). 
f.) 

It remains to show the uniqueness of g.Let 

g . 
L(W, W)with .(v,
g(w)) = 
.(f(v),w)for all v . 
Vand w . 
W.Then .(v,
g(w)) = 
.(v, g(w)), and hence 

.(v,(
g. 
g)(w)) = 
0 for allv . 
V and w . 
W. 

Since . 
is non-degenerateinthe secondvariable, wehave (
g. 
g)(w) = 
0for all 
w . 
W, sothat g = 
g. . 


Example 13.3 Let V = 
W = 
Kn,1 and .(v,w) = 
w TBv with a matrix B . 
GLn(K), so that . 
is non-degenerate(cp. (1)inExample 11.10).We consider the 
linear map f : 
V . 
V, v 
. 
Fv, with a matrix F . 
Kn,n, and the linear map 

h : 
W . 
W, w 
. 
Hw, with a matrix H . 
Kn,n.Then 
.v : 
W . 
K,w 
. 
w T(Bv), 
.(1) : 
V . 
W . 
T 

,v 
. 
(Bv), 

.(2) : 
W . 
V . 
,w 
. 
w TB, 

wherewehaveidentified theisomorphicvector spaces W. 
and K1,n, respectively 
V. 
and K1,n,with each other.Ifg . 
L(W,W)is the right adjoint of f with respect 
to .,then 

.(f(v),w)= 
w TBf (v)= 
w TBFv = 
.(v, g(w)) = 
g(w)TBv 

for all v . 
V and w . 
W.Ifwe represent the linear map g via the multiplication 
withamatrix G . 
Kn,n, i.e., g(w) = 
Gw, then w TBFv = 
w TGTBv for all 
v,w . 
Kn,1.Hence BF = 
GTB.Since B is invertible, the unique right adjoint is 
givenbyG = 
(BFB.1)T = 
B.TFTBT . 

Analogously,for theleft adjoint k . 
L(V, V)of hwith respect to . 
we obtainthe 
equation 

.(v, h(w)) = 
(h(w))TBv = 
w THTBv = 
.(k(v),w)= 
w T Bk(v) 

for all v . 
V and w . 
W. With k(v) = 
Lv for a matrix L . 
Kn,n, we obtain 
HTB = 
BL and hence L = 
B.1HTB. 


190 13 Adjoints of Linear Maps 

If V is finite dimensional and . 
isa non-degeneratebilinear form on V,then by 
Theorem 13.2 every f . 
L(V, V) has a unique right adjoint g and a unique left 
adjoint k, such that 

.(f(v),w) =.(v, g(w)) and .(v, f(w)) =.(k(v),w) (13.3) 

forall v,w .V.If . 
is symmetric, i.e., if .(v,w) =.(w,v)holds forall v,w .V, 
then(13.3)yields 

.(v, g(w)) =.(f(v),w) =.(w, f(v)) =.(k(w),v) =.(v,k(w)). 

Therefore, .(v,(g .k)(w)) = 
0for all v,w . 
V, and hence g = 
k, since . 
is 
non-degenerate. Thus,wehaveprovedthe following result. 

Corollary 13.4 If . 
isasymmetric and non-degeneratebilinear form ona finite 
dimensional K -vector space V, then for every f . 
L(V, V)there exists a unique 
g .L(V,V)with 

.(f(v),w) =.(v, g(w)) and .(v, f(w)) =.(g(v),w) 

for all v,w .V. 

Bydefinition,ascalar productonaEuclideanvector spaceisasymmetricand nondegeneratebilinear 
form (cp. Definition 12.1). This leads to thefollowing corollary. 

Corollary 13.5 If V is a finite dimensional Euclidean vector space with the scalar 
product 	·, ·
,then for every f .L(V,V)there exists a unique f ad .L(V,V)with 

	f(v),w
=	v, fad (w). 
and 	v, f(w)
=	fad (v),w. 
(13.4) 

for all v,w .V.Themapf adis calledthe adjoint of f (with respect to 	·,·
). 

In orderto determine whetheragivenmap g .L(V, V)is the unique adjoint of 
f . 
L(V, V), only oneof thetwo conditionsin(13.4)havetobeverified:If for 
f, g .L(V, V)the equation 

	f(v),w
=	v, g(w). 


holds for all v,w .V,then also 

	v, f(w)
=	f(w),v
=	w, g(v)
=	g(v),w. 


for all v,w .V,wherewehaveusedthe symmetryofthe scalar product.Similarly, 
if 	v, f(w)
=	g(v),w
holds forall v,w .V,then also 	f(v),w
=	v, g(w)
for 
all v,w .V. 


13.1 Basic Definitions andProperties 191 
Example 13.6 Consider theEuclideanvector space R3,1 with the scalar product 
.. 


100 
	v,w
=w TDv, where D = 
020 ,

.. 


001 

and the linear map 

.. 


122 
f :R3,1 >R3,1 ,v >Fv, where F = 
101 .

.. 


200 

For all v,w .R3,1 we then have 

	f(v),w
=w TDFv =w T DFD.1Dv =(D.TFTDTw)TDv =	v, fad (w)
, 

and thus 
.. 
122 

fad :R3,1 >R3,1 >D.1FTDv = 
.

,v 100. 
v, 
220 

where wehave usedthat Dis symmetric. 

Wenowshowthat uniquely determined adjoint maps alsoexistinthe unitary case. 
However,wecannot concludethisdirectlyfromCorollary 13.4,sinceascalarproduct 
on a C-vector spaceis notasymmetric bilinear form,butaHermitiansesquilinear 
form.Inordertoshowtheexistenceofthe adjointmapintheunitary caseweconstruct 
itexplicitly.This constructionworks alsoin theEuclidean case. 

Let V be a unitary vector space with the scalar product 	·,·
and let {u1,..., un}be an orthonormal basis of V.For agiven f .L(V,V)we define the map 

n 

g : 
V >V,v 
. 
	v, f(ui)
ui. 
i=1 
If v,w .V and .,. 
.C,then 

nn 

g(.v +.w) =	.v +.w, f(ui)
ui = 
.	v, f(ui)
ui +.	v, f(ui)
ui 
i=1 i=1 

=.g(v) +.g(w), 


192 13 Adjoints of Linear Maps 

n

and hence g .L(V,V).Let now v = 
1 .iui .V and w .V,then 

i=

. 
nn 
. 
nn 

	v, g(w)
= 
.iui, 	w, f(uj)
uj = 
.i	w, f(ui)
= 
.i	f(ui),w. 
i=1 j=1 i=1 i=1 

=	f(v),w
. 

Furthermore, 

	v, f(w)
=	f(w),v
=	w, g(v)
=	g(v),w. 


for all v,w .V.If 

g .L(V,V)satisfies 	f(v),w
=	v,
g(w)
for all v,w .V, 
then g =g,since thescalar productis positive definite.We can thereforeformulate 
thefollowing result analogously to Corollary 13.5. 

Corollary 13.7 If V is a finite dimensional unitary vector space with the scalar 
product 	·,·
,then for every f .L(V,V)there exists a unique f ad .L(V, V)with 

	f(v),w
=	v, fad (w). 
and 	v, f(w)
=	fad (v),w. 
(13.5) 

for all v,w .V.Themapf adis calledthe adjoint of f (with respect to 	·,·
). 

AsintheEuclidean case,againthevalidityof oneofthetwo equationsin(13.5) 
for all v,w .V impliesthevalidityof theother forall v,w .V. 

Example 13.8 Consider the unitary vector space C3,1 with the scalar product 

.. 


100 
	v,w
=w HDv, where D = 
020 ,

.. 


001 

and the linear map 

.. 


12i 2 

3,1 >C3,1 

..

f :C,v >Fv, where F = 
i 0 .i . 

203i 

For all v,w .C3,1 we then have 

	f(v),w
=w HDFv =w HDFD.1Dv =(D.HFHDHw)HDv 
=	v, fad (w)
, 

and thus 

.. 


1.2i 2 

fad :C3,1

3,1 >C,v >D.1FHDv = 
. 
.i 00 
. 
v, 
22i.3i 


13.1 Basic Definitions andProperties 193 
where wehave usedthat Dis real and symmetric. 

We nextinvestigatethe propertiesofthe adjoint map. 

Lemma 13.9 Let V be a finite dimensional Euclidean or unitary vector space. 

(1) Iff1, f2 .L(V, V)and .1, .2 .K(whereK =Rin theEuclidean andK =C 
in the unitary case),then 
ad =.1 fad +.2 fad

(.1 f1 +.2 f2)12 . 

IntheEuclidean casethemapf 
. 
fad is thereforelinear, and in the unity case 
semilinear. 

(2) We have (IdV)ad =IdV. 
(3) For every f .L(V, V)we have (fad )ad = 
f. 
= 
fad . 
fad
(4) Iff1, f2 .L(V, V),then (f2 . 
f1)ad .
12 

Proof 

(1) Ifv,w .V and .1, .2 .K,then 
	(.1 f1 +.2 f2)(v),w
=.1	f1(v),w
+.2	f2(v),w. 


=.1 v, f1 
ad(w) +.2 v, f2 
ad (w) 

= 
v, .1 f1 
ad(w)+.2 f2 
ad(w) 

.1 fad +.2 fad

= 
v, (w) ,

12 

=.1 fad +.2 fad

and thus (.1 f1 +.2 f2)ad 
2.

1 

(2) For all v,w . 
V we have 	IdV(v),w
=	v,w
=	v, IdV(w)
, and thus 
(IdV)ad =IdV. 
(3) For all v,w .V we have 	fad (v),w
=	v, f(w)
,and thus (fad )ad = 
f. 
(4) For all v,w .V we have 
. 
. 
. 


v, fad fad

	(f2 . 
f1)(v),w
=	f2(f1(v)),w
= 
f1(v), f2 
ad (w) = 
2 (w)

1 

fad . 
fad

= 
v, (w) ,

12 

= 
fad . 
fad

and thus (f2 . 
f1)ad . . 


12 

Thefollowing resultshowsrelations betweentheimageandkernelofan endomorphism 
and of its adjoint. 

Theorem 13.10 If V is a finite dimensional Euclidean or unitary vector space and 
f .L(V, V),then thefollowing assertions hold: 


194 13 Adjoints of Linear Maps 

(1) ker(fad)=im(f). 
. 
(2) ker(f)=im(fad). 
. 
Proof 

(1) Ifw .ker(fad),then fad(w)=0and 
0=	v, fad(w)
=	f(v),w. 


for all v .V, hence w .im(f).. If, on the other hand, w .im(f).,then 

0=	f(v),w
=	v, fad(w). 


for all v .V.Since 	·,·
is non-degenerate, we have fad(w) =0and, hence, 
w .ker(fad). 

(2) Using (fad)ad = 
f and (1)we getker(f)=ker(( fad)ad)=im(fad). 
. . 

Example 13.11 Considerthe unitaryvector space C3,1 withthestandardscalarproduct 
and the linear map 

.. 


1 ii 
f :C3,1 >C3,1 ,v >Fv, with F = 
i00 .

.. 


100 

Then 
.. 
1.i1 

fad :C3,1

3,1 >C,v >FH v, with FH = 
. 
.i 00 
. 
. 
.i 00 

Thematrices Fand FH haverank2. Therefore, dim(ker(f)) =dim(ker(fad)) =1. 
Asimple calculation shows that 

.. 
.. 


.. 
.. 
. 
0 
.. 
0 
. 
.. 
..


ker(f)=span 1 and ker(fad)=span 1 . 

.. 
..

.1 i 

Thedimensionformulafor linearmaps impliesthatdim(im(f)) =dim(im(fad)) =2. 
From the matrices Fand FH we can seethat 

.. 
.. 


.... 
. 
... 


. 
11 
.. 
11 
. 
im(f)=span 
. 
i. 
, .
0. 
and im(fad)=span 
. 
.i. 
, .
0 .

. 
.. 
..

10 .i 0 

The equationsker(fad)=im(f).andker(f)=im(fad).canbeverifiedby direct 
computation. 


13.2 Adjoint Endomorphisms andMatrices 195 
13.2 AdjointEndomorphismsand Matrices 
We now study the relation between the matrix representations of an endomorphism 
and its adjoint.Let V be a finite dimensional unitary vector space with the scalar 
product 	·,·
and let f .L(V,V).For an orthonormal basis B ={u1,...,un}of V let [fB,B =[aij ].Cn,n, i.e., 

n 

f(uj)= 
akjuk, j=1,...,n, 
k=1 

and hence 

. 
n 
. 


	f(uj),ui
= 
akjuk, ui =aij, i,j=1,...,n. 
k=1 

If [fad 
B,B =[bij ].Cn,n, i.e., 

n 

fad 

(uj)= 
bkjuk, j=1,...,n, 
k=1 

then 

bij =	fad(uj),ui
=	uj, f(ui)
=	f(ui),uj
=aji. 

Thus, [fad 
B,B = 
([fB,B)H. The same holds for a finite dimensional Euclidean 
vector space, but then we can omit the complex conjugation. Therefore, we have 
shownthe following result. 

Theorem 13.12 If V is a finite dimensional Euclidean or unitary vector space with 
theorthonormal basisB andf .L(V,V),then 

[fad 
B,B = 
([fB,B)H . 

(In theEuclidean case ([fB,B)H =([fB,B)T.) 

An important special class are the selfadjoint endomorphisms. 

Definition 13.13 Let V be a finite dimensional Euclidean or unitary vector space. 

= 
fad

An endomorphism f .L(V,V)is called selfadjoint when f . 

Trivial examples of selfadjoint endomorphism inL(V,V)are f =0and IdV. 

Corollary 13.14 

(1) If V is a finite dimensional Euclidean vector space, f .L(V,V)is selfadjoint 
andBisanorthonormal basisof V,then [fB,B is a symmetric matrix. 

196 
13 Adjoints of Linear Maps 

(2) IfVis a finite dimensional unitary vector space, f .L(V, V)is selfadjoint and 
Bis an orthonormal basis ofV,then [fB,B is an Hermitian matrix. 
The selfadjoint endomorphisms again form a vector space. However, one has to 
be careful to use the appropriate field over which this vector space is defined. In 
particular,the setofselfadjoint endomorphismsonaunitaryvector space V does not 
formaC-vector space. If f = 
fad .L(V, V) \{0},then (if)ad =.ifad =.if 
= 
if (cp.(1)inLemma 13.9). Similarly, theHermitianmatricesin Cn,n do not form 
a C-vector space. If A = 
AH . 
Cn,n \{0}is Hermitian, then (iA)H =.iAH = 
.iA=iA. 
Lemma 13.15 

(1) If V is ann-dimensional Euclidean vector space,then thesetof selfadjoint 
endomorphisms {f .L(V, V)|f = 
fad}formsanR-vector spaceof dimension 
n(n +1)/2. 
(2) IfV isann-dimensional unitary vector space,thenthesetof selfadjoint endomorphisms 
{f . 
L(V,V)|f = 
fad }forms an R-vector space of dimension 
2

n. 
Proof Exercise. . 

Amatrix A .Cn,n with A = 
AT is called complex symmetric.Unlike the Hermitianmatrices,
the complex symmetric matrices forma C-vector space. 

Lemma 13.16 The set of complex symmetric matrices in Cn,n forms a C-vector 
space of dimension n(n +1)/2. 

Proof 
Exercise. . 


Lemmas13.15and13.16willbeusedinChap. 15inourproofofthe Fundamental 
Theorem of Algebra. 

Exercises 

13.1. Let 
.(v,w) = 
w TBv with B = 
diag(1, .1)be defined for v,w . 
R2,1. 
Consider the linear maps f :R2,1 >R2,1, v 
. 
Fv, and h :R2,1 >R2,1, 
w >Hw, where 
12 
10

2,2 
2,2

F =.R, H =.R.

01 
11 

Determine .v, .(1) and .(2) asin(13.1)–(13.2)aswellasthe right adjointof 
f and theleft adjointof hwith respect to .. 

13.2. Let 
(V, 	·, ·
V) and (W, 	·, ·
W)be two finite dimensional Euclidean vector 
spaces and let f . 
L(V, W). Show that there exists a unique g . 
L(W, V)with 	f(v),w
W =	v, g(w)
V for all v .V and w .W. 
13.3. Let 	v,w
=w TBv for all v,w .R2,1with 
21 

2,2

B =.R.

11 


13.2 Adjoint Endomorphisms andMatrices 
197 
(a) Showthat 	v,w
=w TBvis a scalar product on R2,1. 
(b) Using this scalar product, determine the adjoint map fad of f :R2,1 . 
R2,1 

, v>Fv, with F .R2,2. 

(c) Investigate which properties Fneeds to satisfy so that f is selfadjoint. 
13.4. Let n .2and 
f : 
Rn,1 >Rn,1 T 
T 
, [x1,...,xn >[0,x1,...,xn.1 . 

Determinethe adjoint fad of f with respecttothestandardscalar productof 
Rn,1 

. 

13.5. Let V be a finite dimensional Euclidean or unitary vector space and let f . 
L(V,V). Showthatker(fad . 
f)=ker(f)andim(fad . 
f)=im(fad). 
13.6. Let V beafinite dimensional Euclidean or unitaryvector space, let U .V be 
a subspace and let f .L(V,V)with f(U).U. Showthat then fad(U.). 
U. 


. 

13.7. Let 
V be a finite dimensional Euclidean or unitary vector space, let f . 
L(V,V)and v.V. Showthat v.im(f)if and onlyif v.ker(fad). 
. 
“Matrix version”:For A.Cn,n and b.Cn,1 the linear system of equations 
Ax =bhasasolutionifand onlyif b.L(AH ,0). 
. 
13.8. Let V beafinite dimensional Euclidean or unitaryvector space and let f,g. 
L(V,V)be selfadjoint. Showthat f .gis selfadjoint if and onlyif f and g 
commute, i.e., f .g=g. 
f. 
13.9. Let Vbeafinite dimensional unitaryvector space and let f .L(V,V). Show 
that f is selfadjointif and onlyif 	f(v),v
.R holds for all v.V. 
13.10. 
Let V be a finite dimensional Euclidean or unitary vector space and let f . 
L(V,V)be a projection, i.e., f satisfies f2 = 
f. Showthat f is selfadjoint 
if and onlyifker(f).im(f), i.e., 	v,w
=0holds for allv .ker(f)and 
w.im(f). 
13.11. 
Let V beafinite dimensional Euclidean or unitaryvector space and let f,g. 
ad . 
f
L(V,V). Showthatif g=0.L(V,V),then 	v,w
=0holds for all 
v.im(f)and w.im(g). 

13.12. 
Fortwo polynomials p,q .R[t .n let 
1 

	p,q
:= 
p(t)q(t)dt. 

.1 

(a) Showthat this definesascalar product on R[t .n. 
(b) Consider the map 
nn 

f :R[t .n >R[t .n, p= 
.iti 
. 
i.iti.1 , 
i=0 i=1 

and determine fad,ker(fad),im(f),ker(fad). 
and im(f). 
. 

13.13. 
Prove Lemma 13.15. 
13.14. 
Prove Lemma 13.16. 

Chapter14 
Eigenvalues of Endomorphisms 

In previous chapterswehavealready studied eigenvalues and eigenvectorsofmatrices.
Inthis chapter we generalizethese conceptsto endomorphisms, and we investigate 
when endomorphisms on finite dimensional vector spaces can be represented 
by diagonal matrices or (upper)triangular matrices.Fromsuch representations we 
easily can read offimportant information about the endomorphism,in particular its 
eigenvalues. 

14.1 BasicDefinitionsandProperties 
We first consider an arbitraryvector space and then concentrate on the finite dimensional 
case. 

Definition 14.1 Let V be a K-vector space and f . 
L(VV .If . 
. 
K and v 
. 
V \{0}satisfy 

f(v 
=.v 


then . 
is calledan eigenvalue of f,andv 
is calledan eigenvector of f corresponding 
to .. 

Bydefinition,v 
=0cannotbeaneigenvector,butaneigenvalue. 
=0may occur 
(cp. theexamplefollowing Definition 8.7). 

The equation f(v 
=.v 
can be written as 

0=.v 
. 
f(v 
=(.IdV . 
f )(v 


Hence, . 
. 
K is an eigenvalue of f if and onlyif 

ker(.IdV . 
f ={0} 


©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_14 

200 14 EigenvaluesofEndomorphisms 

We already knowthat thekernel of an endomorphism onV forms a subspace of V (cp. Lemma 10.7). This holds,in particular,forker(.IdV . 
f . 

Definition 14.2 If Visa K-vector space and . 
. 
Kisan eigenvalueof f . 
L(VV , 
then the subspace 

Vf(. 
:= 
ker(.IdV . 
f 

is calledthe eigenspace of f corresponding to . 
and 

g(. 
f := 
dim(Vf(. 


is calledthe geometric multiplicityof the eigenvalue .. 

By definition, the eigenspace Vf(. 
is spanned by all eigenvectors of f corresponding 
to the eigenvalue ..If Vf(. 
is finite dimensional, then g(. 
f = 
dim(Vf(. 
is equal to the maximal number of linearly independent eigenvectors 
of f corresponding to .. 

Definition 14.3 Let V be a K-vector space, let U . 
V be a subspace, and let 

f . 
L(VV .If f(U . 
U, i.e., if f(u . 
U holds forall u . 
U,then U is calledan 

f-invariant subspace of V. 

An important example of f-invariant subspaces arethe eigenspacesof f. 

Lemma 14.4 If V isaK-vector space and . 
. 
K is aneigenvalue of f . 
L(VV , 
then Vf(. 
is an f-invariant subspace of V. 

Proof For everyv 
. 
Vf(. 
we have f(v 
= 
.v 
. 
Vf(. 
. . 


We now consider finite dimensional vector spaces and discuss the relationship 
between the eigenvalues of f and the eigenvalues of a matrix representation of f 
with respecttoagiven basis. 

Lemma 14.5 If V is a finite dimensional K-vector space and f . 
L(VV , then 
thefollowing statementsare equivalent: 

(1) . 
. 
K is an eigenvalue of f. 
(2) . 
. 
K is an eigenvalue of the matrix[ 
f]BB forevery basisB ofV. 
Proof Let . 
. 
K be an eigenvalue of f and let B ={v1 ,...,vn} 
be an arbitrary 
basis of V.Ifv 
. 
V is an eigenvector of f corresponding to the eigenvalue .,then 
f(v 
= 
.v 
and there exist (unique) coordinates .1 .n . 
K, not all equal to 

n 

zero, with v 
==1 . 
jv 
j.Using(10.4)we obtain

j

.. 
.. 


.1 .1 

.. 
..

[ 
f]BB 
.. 
= 
B( 
f(v 
= 
B(.v 
= 
.B(v 
= 
. 
.. 


.n .n 


14.1 Basic Definitions andProperties 201 
and thus . 
is an eigenvalue of[f]BB. 

If, on the other hand, [f]BB[.1 .n]T = 
.[.1 .n]T with [.1 
.n]T 
= 
0 for a given (arbitrary) basis B ={v1 ,...,vn} 
of V, then we set 

n 

v 
:= 
1 .jv 
j.Then v 
=0and

j=

.. 
.. 


.1 .1

n 

.. 
..

f(v 
= 
.jf(v 
j =( 
f(v1 f(vn 
.. 
= 
(v1 ,...,vn [f]BB 
.. 
j=1 

.n .n 

.. 
.. 


.1 

.. 
.. 


=(v1 ,...,vn 
. 
. 
. 
.. 
=.v 


.n 

i.e., . 
is an eigenvalue of f. . 


Lemma 14.5 impliesthattheeigenvaluesof f arethe rootsofthe characteristic 
polynomial of the matrix [f]BB (cp. Theorem 8.8). This,however, does not hold 
in general for a matrix representation of the form [f]BB
, where B and B are two



different bases of V.In general,thetwomatrices 

[f]BB =[IdV]B and


B [f]BB [f]BB 

do not have the same eigenvalues. 

Example 14.6 Consider the vector space R21 with the bases 

. 
. 
. 
. 


10 11 



B = 
B = 


01 .11 

Then the endomorphism 

. 


21 >R21 01

f :Rv 

> 
Fv 
where F = 


10 

has the matrix representations 

. 
. 


01 1 .11

[f]BB =[f]BB = 




10 211 

We have det(tI2 .[f]BB =t2 .1, and thus f has the eigenvalues .1and 1. On 
the other hand, the characteristic polynomial of[f]B 

B is t2 . 
1,sothat this matrix

2

vv 


has the eigenvalues .1 2and1 2. 



Fortwo different bases B and B of V the matrices [f]BB and [f]B are similar



B 

(cp.thediscussion following Corollary 10.20).In Theorem 8.12wehaveshownthat 


202 14 EigenvaluesofEndomorphisms 

similarmatriceshavethesame characteristic polynomial.This justifiesthefollowing 
definition. 

Definition 14.7 If n . 
N, V is an n-dimensional K-vector space with the basis B, 
and f . 
L(VV ,then 

Pf := 
det(tIn .[ 
f]BB . 
K[t] 


is calledthe characteristic polynomial of f. 

The characteristic polynomial Pf is always a monic polynomial with 

deg(Pf = 
n = 
dim(V 

As we have discussed before, Pf is independent of the choice of the basis of V.A 
scalar . 
. 
K is an eigenvalue of f if and onlyif . 
isaroot of Pf, i.e., Pf(. 
= 
0. 
AsshowninExample 8.9,inrealvector spaceswith dimensionsat leasttwo,there 
exist endomorphismsthatdo nothave eigenvalues. 

If . 
isaroot of Pf, then Pf = 
(t . 
. 
· 
q for a monic polynomial q . 
K[t], 
i.e., the linear factort . 
. 
divides the polynomial Pf;wewill show this formallyin 
Corollary 15.5 below.Ifalso q(. 
= 
0, thenq = 
(t . 
. 
· 

q fora monic polynomial 

q . 
K[t], and thus Pf = 
(t . 
. 
2 · 
q.We can continue until Pf = 
(t . 
. 
d · 
gfora 
g . 
K[t] 
with g(. 

= 
0. This leads to thefollowing definition. 

Definition 14.8 Let V bea finite dimensional K-vector space, and let f . 
L(VV have the eigenvalue . 
. 
K.Ifthe characteristic polynomialof f has theform 

Pf = 
(t . 
. 
d · 
g 

for some g . 
K[t] 
with g(. 

= 
0, thend is calledthe algebraic multiplicity of the 
eigenvalue . 
of f.Itis denotedby a(. 
f . 

If .1 .k arethe pairwisedistinct eigenvalues of f with corresponding algebraic 
multiplicities a(.1 fa(.kf , and if dim(V = 
n,then 

a(.1 f ++ 
a(.kf . 
n 

since deg(Pf = 
dim(V = 
n. 

Example 14.9 The endomorphism f : 
R41 > 
R41 , v 

> 
Fv 
with 

.. 


12 34 

..

01 23 

.. 
44 

F =. 
R

..

00 01 

00 .10 

has the characteristic polynomial Pf = 
(t.12(t2+1.Theonlyreal rootof Pf is1, 
and a(.1 f = 
2< 
4= 
dim(R41 . 


14.1 Basic Definitions andProperties 203 
Lemma 14.10 If V isa finite dimensionalK-vector spaceandf .L(VV ,then 

g(. 
f . 
a(. 
f 

for every eigenvalue . 
of f. 

Proof Let . 
. 
K be an eigenvalue of f with geometric multiplicity m = 
g(. 
f . 
Then there exist m linear independent eigenvectors v1 ,...,vm . 
V of f corresponding 
to the eigenvalue ..If m = 
dim(V , then these m eigenvectors form a 
basis Bof V.Ifm < 
dim(V =n,then we can extend the m eigenvectorstoa basis 
B ={v1 ,...,vm ,vm+1 ,...,vn}of V. 

Wehave f(v 
j =.v 
j for j=1 m and, therefore, 

. 


.Im Z1

[f]BB = 


0 Z2 

for two matrices Z1 . 
Km n.m and Z2 . 
Kn.mn.m.Using (1)inLemma 7.10 we 
obtain 

Pf =det(tIn .[f]BB =(t .. 
m ·det(tIn.m .Z2 

whichimplies a(. 
f .m =g(. 
f . . 


In the following we will try to find a basis of V, so that the eigenvalues of a 
given endomorphism f can be read off easily from its matrix representation. The 
easiest forms of matrices in this sense are diagonal and triangular matrices, since 
theireigenvalues arejusttheirdiagonal entries. 

14.2 Diagonalizability 
In this sectionwewill analyze when foragivenendomorphism hasadiagonal matrix 
representation.We formally define this property as follows. 

Definition 14.11 Let V be a finite dimensional K-vector space. An endomorphism 

f .L(VV is called diagonalizable,ifthereexistsabasis Bof V,such that [f]BB 
isadiagonal matrix. 

Accordingly, a matrix A . 
Kn n is diagonalizable when there exists a matrix 
S .GLn(K with A=SDS.1 for a diagonal matrix D . 
Kn n . 

In order to analyze thediagonalizablility,webegin withasufficient conditionfor 
the linear independenceof eigenvectors. This conditionalso holds when V is infinite 
dimensional. 

Lemma 14.12 Let V be a K-vector space and f . 
L(VV .If .1 .k . 
K, 
k . 
2, are pairwise distinct eigenvalues of f with corresponding eigenvectors 
v1 ,...,vk .V,then v1 ,...,vk are linearly independent. 


204 14 EigenvaluesofEndomorphisms 

Proof We prove theassertionbyinductiononk.Let k = 
2and letv1 ,v2 be eigenvectors 
of f corresponding to the eigenvalues .1 
= 
.2.Let .1 .2 . 
K with 
.1v1 +.2v2 = 
0. Applying f on both sides of this equation as well as multiplying 
the equation with .2 yields thetwo equations 

.1.1v1 +.2.2v2 =0 

.1.2v1 +.2.2v2 =0 

Subtracting the second equation from the first, we get .1(.1 ..2 )v1 = 
0. Since 
.1 
= 
.2 and v1 
= 
0, wehave .1 = 
0. Then from.1v1 +.2v2 = 
0we also obtain 
.2 =0, sincev2 =0. Thus,v1 and v2 are linearly independent. 

Theproofoftheinductivestepis analogous.Weassumethattheassertion holds 
for some k . 
2.Let .1 .k+1 be pairwisedistinct eigenvalues of f with corresponding 
eigenvectors v1 ,...,vk+1, and let .1 .k+1 . 
K satisfy 

.1v1 ++.kvk +.k+1vk+1 =0 

Applying f to this equationyields 

.1.1v1 ++.k.kvk +.k+1.k+1vk+1 =0 

while a multiplication with .k+1 gives 

.1.k+1v1 ++.k.k+1vk +.k+1.k+1vk+1 = 
0 

Subtractingthis equationfromthe previous one we get 

.1(.1 ..k+1 )v1 ++.k(.k ..k+1 )vk =0 

Since .1 .k+1 are pairwisedistinct and v1 ,...,vk are linearly independent by 
theinductionhypothesis,we obtain .1 = 
··· 
= 
.k = 
0. But then .k+1vk+1 = 
0 
impliesthat also .k+1 =0, so thatv1 ,...,vk+1 are linearly independent. . 


Using this result we next show that the sum of eigenspaces corresponding to 
pairwisedistinct eigenvaluesis direct (cp. Theorem 9.31). 

Lemma 14.13 Let V be a K-vector space and f . 
L(VV .If .1 .k . 
K, 
k . 
2,are pairwisedistinct eigenvalues of f, then the corresponding eigenspaces 
satisfy 

k 

Vf(.i . 
Vf(.j ={0}

j=1 
j=i 

for alli =1 k. 


14.2 Diagonalizability 
205 
Proof Let i be fixed and let 

k 

v 
.Vf(.i . 
Vf(. 
j 

j=1 
j=i 

In particular, we have v 
= 
v 
j for some v 
j .Vf(. 
j , j =i. Then .v 
+ 


. 
j=i 
v 
j =0,and the linear independence of eigenvectors corresponding to pairwise

j=i 

distinct eigenvalues (cp. Lemma 14.12)impliesv 
=0. 
. 


Thefollowing theoremgives necessary and sufficient conditions forthe diagonalizability 
of an endomorphism on a finite dimensional vector space. 

Theorem 14.14 If V isa finite dimensionalK-vector space andf .L(VV ,then 
thefollowing statementsare equivalent: 

(1) f is diagonalizable. 
(2) There exists a basis of V consisting ofeigenvectors of f. 
(3) The characteristic polynomial Pf decomposes into n =dim(V linear factors 
overK, i.e., 
Pf =(t ..1 ··(t ..n 


with the eigenvalues .1 .n .Koff, and foreveryeigenvalue . 
j we have 
g(.jf =a(. 
jf . 

Proof 

(1) 
. 
(2 :If f . 
L(VV is diagonalizable, then there exists a basis B = 
{v1 ,...,vn}of V and scalars .1 .n .K with 
.. 


.1 

..

[f]BB = 
.. 
(14.1) 

.n 

and hence f(v 
j =. 
jv 
j, j =1 n.The scalars .1 .n arethus eigenvalues 
of f, and the corresponding eigenvectors are v1 ,...,vn. 
If, on the other hand, there exists a basis B ={v1 ,...,vn}of V consisting of 
eigenvectorsof f,then f(v 
j =. 
jv 
j, j=1 n,for scalars.1 .n .K 
(the corresponding eigenvalues), and hence [f]BB has theform(14.1). 

(2) 
.(3 :Let B ={v1 ,...,vn}be a basis of V consisting ofeigenvectors of f, 
and let .1 .n .K be the corresponding eigenvalues.Then [f]BB has the 
form(14.1)and hence 
Pf =(t ..1 ··(t ..n 

so that Pf decomposes into linearfactorsover K. 


206 14 EigenvaluesofEndomorphisms 

We still have to show that g(.jf = 
a(.jf for every eigenvalue . 
j.The 
eigenvalue . 
j has the algebraic multiplicity mj := 
a(. 
jf if and onlyif .j 
occurs mj timesonthediagonalof the(diagonal)matrix [ 
f]BB.This holds if 
and onlyifexactly mj vectorsofthe basis Bareeigenvectorsof f corresponding 
to theeigenvalue . 
j.Each of these mj linearly independentvectorsisa element 
of the eigenspace Vf(. 
j and, hence, 

dim(Vf(.j = 
g(.jf . 
mj = 
a(.jf 

From Lemma 14.10 we know that g(.jf . 
a(. 
jf , and thus g(.jf = 
a(. 
jf . 



(3) L 
(2 : 
Let.1 .k be the pairwisedistinct eigenvaluesof f with correspond


ing geometric and algebraicmultiplicities g(. 
jf and a(. 
jf , j= 
1 k, 
respectively. Since Pf decomposes into linearfactors, wehave 

k 



a(. 
jf = 
n = 
dim(V 
j=1 



Now g(.jf = 
a(.jf , j= 
1 k,impliesthat 

k 



g(. 
jf = 
n = 
dim(V 
j=1 

By Lemma 14.13 we obtain(cp. also Theorem 9.31) 



Vf(.1 .. 
Vf(.k = 
V 



If we select bases of the respective eigenspaces Vf(. 
j , j = 
1 k,then we 
get a basis of V that consists of eigenvectors of f. 

Theorem 14.14 and Lemma 14.12 imply an important sufficient condition for 
diagonalizability. 

Corollary 14.15 If V isann-dimensionalK-vector spaceandf . 
L(VV has n 
pairwisedistinct eigenvalues,thenfisdiagonalizable. 

The conditionofhaving n = 
dim(V pairwisedistinct eigenvalues is,however,not 
necessary forthe diagonalizability of an endomorphism.Asimple counterexample 
is theidentity IdV, which has the n-fold eigenvalue1, while [IdV]BB = 
In holds 
for every basis Bof V.Onthe other hand, thereexist endomorphismswith multiple 
eigenvalues that are not diagonalizable. 


14.2 Diagonalizability 
207 
Example 14.16 The endomorphism 

. 
. 


21 >R21 
11

f :Rv 

> 
Fv 
with F = 


01 

has the characteristic polynomial (t .12 and thus only has theeigenvalue1.We 
haveker(Vf(1 = 
span{[10]T} 
and thus g(1 f = 
1 < 
a(1 f = 
2. By Theorem 
14.14, f is not diagonalizable. 

14.3 Triangulation and Schur’s Theorem 
If the property g(. 
jf = 
a(. 
jf does not holdforeveryeigenvalue . 
j of f, 
then f is not diagonalizable. However, as long as the characteristic polynomial Pf 
decomposes into linearfactors, we can findaspecial basis B such that [f]BB is a 
triangular matrix. 

Theorem 14.17 If V isa finite dimensionalK-vector space andf . 
L(VV ,then 
thefollowing statementsare equivalent: 

(1) Thecharacteristic polynomialPf decomposes into linear factorsoverK. 
(2) There exists a basis B of V suchthat [f]B B is upper triangular, i.e., f can be 
triangulated. 
Proof 

(2) 
L 
(1 : 
If n = 
dim(V and [f]BB =[rij]. 
Kn n is upper triangular, then 
Pf =(t .r11 ··(t .rnn . 
(1) 
L 
(2 :We show theassertionbyinductiononn = 
dim(V .The case n = 
1is 
trivial, since then [f]BB . 
K1 1 . 
Suppose that the assertion holds for an n . 
1, and letdim(V = 
n + 
1. By 
assumption, 
Pf =(t ..1 ··(t ..n+1 

where .1 .n+1 . 
K are the eigenvalues of f.Let v1 . 
V be an eigenvector 
corresponding to the eigenvalue .1. We extend this vector to a basis 
B ={v1 ,w2 ,...,wn+1}of V.With BW :={w2 ,...,wn+1}and W := 
span BW 
we have V =span{v1}.W and 

. 


.1 

. 


0 

.

[f]BB = 
. 


. 


0 

. 


a12 ··· 
a1 n+1 

.

a22 a2 n+1 

. 


. 


. 


an+12 ··· 
an+1 n+1 


208 14 EigenvaluesofEndomorphisms 

We defineh .L(W span{v1} 
and g .L(WW by 

n+1 

h(w 
j :=a1jv1 and g(w 
j := 
akjwkj=2 n +1 
k=2 

Then f(w 
=h(w 
+g(w 
for all w 
.W, and 

. 


.1 [h]BW {v1}

[f]BB = 


0 [g]BW BW 

Consequently, 

(t ..1 Pg =Pf =(t ..1 ··(t ..n+1 

and hence Pg = 
(t ..2 ··(t ..n+1 .Nowdim(W = 
n and the characteristic 
polynomial of g .L(WW decomposes into linearfactors.By the 



inductionhypothesis thereexistsa basis BW ={w

2 w

n+1}of W such that 
[g]
upper triangular.Thus,for the basis B1 := 
{v1 w

2 w

n+1}the

BW BW matrix [f]B1 B1 is upper triangular. . 


A“matrixversion”ofthis theoremreadsasfollows:The characteristic polynomial 
PA of A . 
Kn n decomposes into linear factors over K if and only if A can be 
triangulated, i.e., there exists a matrix S .GLn(K with A =SRS.1 for an upper 
triangular matrix R .Kn n . 

Corollary 14.18 Let V be a finite dimensional Euclidian or unitary vector space 
and f .L(VV .If Pf decomposes over R (in the Euclidian case case) or C (in 
the unitary case) into linear factors, then thereexistsanorthonormal basisB ofV, 
suchthat [f]B B is upper triangular. 

Proof If Pf decomposes into linearfactors, thenby Theorem 14.17 thereexistsa 
basis B1 of V, such that [f]B1 B1 is upper triangular.Applying theGram-Schmidt 
method to the basis B1,weobtainanorthonormal basis B2 of V,such that [IdV]B1 B2 
is upper triangular (cp. Theorem 12.11). Then 

.1

[f]B2 B2 =[IdV]B1 B2[f]B1 B1[IdV]B2 B1 =[IdV][f]B1 B1[IdV]B2 B1

B2 B1

The invertible upper triangular matrices form a group with respect to the matrix 
multiplication(cp. Theorem 4.13). Thus,all matricesin theproduct on theright 
hand side are upper triangular, and hence [f]B2 B2 is upper triangular. . 


Example 14.19 Consider theEuclidianvector space R[t].1 with the scalar product 

. 
1

pq= 
0 p(tq(t dt, and the endomorphism 


14.3 Triangulation and Schur’s Theorem 209 
f :R[t].1 >R[t].1 .1t +.0 
> 
2.1t +.0 

Wehave f(1 =1and f(t =2t, i.e., the polynomials1and t are eigenvectors of 
f corresponding to the(distinct)eigenvalues1 and2. Thus, B ={1 t}is a basis 





of R[t].1, and [f]
B
isadiagonal matrix.Notethat B is not an orthonormal basis, 

B 

since in particular 1 t=0. 

Since Pf decomposes into linearfactors, Corollary 14.18 guarantees theexistence 
of an orthonormal basis B for which [f]BB is upper triangular.Inthe proof of the 
implication (1 L 
(2 of Theorem 14.17 one chooses any eigenvector of f, and 
then proceeds inductivelyinorderto obtainthe triangulationof f.Inthis example, 
let us use q1 =1asthe firstvector.Thisvectoris an eigenvectorof f with norm 
1 corresponding to the eigenvalue 1. If q2 . 
R[t].1 is a vector with norm 1 and 
q1 q2=0, thenB ={q1 q2}is an orthonormal basis forwhich [f]BB is an upper 
triangular matrix.We construct thevector q2 by orthogonalizing t against q1 using 
theGram-Schmidt method: 

1 

q2 =t .tq1q1 =t . 


2 

. 


1112 1 

q2= 
t . 
t . 
=v 


22 12

vv 
q2 =q2.1 
q2 = 
12t . 
3 

This leads to thetriangulation 

. 
v 
. 


13 22 

[f]BB =.R

02 

v 
We could also choose q1 = 
3t, which is an eigenvector of f with norm 1 
correspondingto theeigenvalue2.Orthogonalizingthevector1 against q1 leads to 
the second basis vector q2 =.3t +2.With the corresponding basis B1 we obtain 
thetriangulation 

. 
v 
. 


2. 
3 22 

[f]B1 B1 =.R

01 

Thisexampleshows thatinthetriangulationof f theelementsabovethediagonal can 
be different fordifferent orthonormal bases.Onlythe diagonal elementsare(except 
for their order) uniquely determined, since they are the eigenvalues of f.Amore 
detailedstatement about the uniquenessisgivenin Lemma 14.22. 

In the next chapter we will prove the Fundamental Theorem of Algebra, which 
states that every non-constant polynomial over C decomposes into linear factors. 
This result has thefollowing corollary,whichis known as Schur’s theorem.1 

1IssaiSchur (1875–1941). 


210 14 EigenvaluesofEndomorphisms 

Corollary 14.20 If V isa finite dimensional unitary vector space,thenevery endomorphism 
on V can be unitarily triangulated, i.e., for each f .L(VV there exists 
an orthonormal basisBofV,suchthat[f]B B is upper triangular. The matrix [f]BB 
is called a Schur form of f. 

If V is the unitary vector space Cn 1 with the standard scalar product, then we 
obtainthe following “matrixversion”of Corollary 14.20. 

Corollary 14.21 If A . 
Cn n, then there exists a unitary matrix Q . 
Cn n with 
A = 
QRQH for an upper triangular matrix R . 
Cnn. The matrixRis calleda 
Schur form of A. 

Thefollowing resultshowsthataSchur formofamatrix A.Cnn with n pairwise 
distinct eigenvalues is “almost unique”. 

Lemma 14.22 LetA.Cnn haven pairwisedistincteigenvalues,andletR1 R2 . 
Cnn be twoSchur formsofA.Ifthe diagonalsofR1 and R2 are equal then R1 = 
UR2UH fora unitary diagonal matrixU. 

Proof Exercise. . 


A survey of the results on unitary similarity of matrices can be found in the 
article [Sha91]. 

MATLAB-Minute. 

Consider for n .2the matrix 

12 . 
3 ··· 
. 
n 
.
13 4 ··· 
n +1. 
. 
. 
A= 
. 
. 
14 5 ··· 
n +2. 
. 
.Cn n 
. 
. 
. 
. 


1n +1n +22n .1 

Compute a Schur form of Ausingthe command [U,R] = schur(A) for n = 
234 10.Whatare theeigenvaluesof A?Formulate a conjecture about the rank 
of Aforgeneral n.Can you prove your conjecture? 

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

14.1. Let V be a vector space and let f . 
L(VV have the eigenvalue .. Show 
that im(.IdV . 
f is an f-invariant subspace. 
14.2. Let V be a finite dimensional vector space and let f .L(VV be bijective. 
Showthat f and f.1have thesameinvariant subspaces. 

14.3 Triangulation and Schur’s Theorem 211 
14.3. Let V be an n-dimensional K-vector space, let f .L(VV , and let U be an 
m-dimensional f-invariant subspace of V. Show that a basis Bof V exists 
such that 
. 


A1 A2

[f]BB = 


0 A3 

, A2 .Km n.mn.m

for some matrices A1 .Kmm m and A3 .Kn.. 

14.4. Let K .{RC}and f :K41 >K4 1 , v 
>Fv 
with 
.. 


12 34 

..

01 23 

..

F= 


..

00 11 

00 .10 

Compute Pf and determine for K = 
R and K = 
C the eigenvalues of f 
with their algebraic and geometric multiplicities, as well as the associated 
eigenspaces. 

14.5. Consider the vector space R[t].n with the standard basis {1 t tn}and 
the endomorphism 
nn 

	. 
d2 
f :R[t].n >R[t].n .iti 
> 
i(i.1 .iti.2 = 
p

dt2 

i=0 i=2 

Compute Pf, the eigenvalues of f with their algebraic and geometric multiplicities, 
and examine whether f is diagonalizableor not.What changesif 
one considers as map the kth derivative(for k=34 n)? 

14.6. Examinewhether thefollowing matrices 
.. 


.. 
310 .2 

. 


100 

..

01 0200

22 33 
.. 
44 

..

A=. 
QB=.120 . 
QC=. 
Q

..

.10 222 .4

.111 

0002 

arediagonalizable. 

14.7. Isthesetofalldiagonalizableandinvertible matricesasubgroupof GLn(K ? 
14.8. Let n .N0.Consider the R-vector space R[t].n and the map 
f :R[t].n >R[t].np(t >p(t+1 .p(t 

Showthat f is linear.For which n is f diagonalizable? 

14.9. Let V be an R-vector space with the basis {v1 ,...,vn}.Examinewhether the 
following endomorphisms are diagonalizable or not: 
(a) f(v 
j =v 
j +v 
j+1, j=1 n.1, and f(vn =vn, 

212 
14 EigenvaluesofEndomorphisms 

(b) f(v 
j = 
jv 
j +v 
j+1, j=1 n .1, and f(vn =nvn. 
14.10. 
Let V be a finite dimensional Euclidian vector space and let f . 
L(VV f + 
fad
with = 
0 . 
L(VV . Show that f 
= 
0 if and only if f is not 
diagonalizable. 

14.11. 
Let V bea C-vector space and let f .L(VV with f2 =.IdV.Determine 
all possible eigenvalues of f. 
14.12. 
Let V be a finite dimensional vector space and f . 
L(VV . Show that 
Pf( 
f =0.L(VV . 
14.13. 
Let V be a finite dimensional K-vector space, let f .L(VV and 
p=(t..1 ··(t ..m .K[t].m 

Showthat p( 
f is bijectiveifand onlyif .1 .m are not eigenvalues of 

f. 
14.14. 
Determine conditions forthe entriesof thematrices 
. 


.. 


22 

A=.R

.. 


such that Ais diagonalizable or can be triangulated. 

14.15. 
Determine an endomorphism on R[t].3 that is not diagonalizable and that 
cannot be triangulated. 
14.16. 
Let V be a vector space with dim(V =n. Show that f .L(VV can be 
triangulatedif and onlyifthereexist subspaces V0 V1 Vn of V with 
(a) Vj .Vj+1 for j=01 n .1, 
(b) dim(Vj 
= 
jfor j=01 n, and 
(c) Vj is f-invariant for j=01 n. 
14.17. 
Prove Lemma 14.22. 

Chapter15 
Polynomials and the FundamentalTheorem 
of Algebra 

In this chapter we discuss polynomials in more detail. We consider the division 
of polynomials and derive classical results from polynomial algebra, including the 
factorization into irreducible factors.We also prove the Fundamental Theorem of 
Algebra, which states that every non-constant polynomial over the complex numbers 
hasaleast one complex root.Thisimpliesthatevery complex matrix andevery 
endomorphismona(finite dimensional)complexvectorspacehasatleastoneeigenvalue. 


15.1 Polynomials 
Letusrecallsomeofthe most important termsinthe context ofpolynomials.If K 
is a field, then 

p . 
.0 . 
.1t +. 
.ntn with n . 
N0 and .0 .1 .n . 
K 

isapolynomialover K in thevariable t.ThesetK[t ofallthese polynomials formsa 
commutativering with unit(cp. Example 3.17). If .n 
. 
0,then deg( 
p . 
n is called 
the degree of p.If.n . 
1, then p is called monic.If p . 
0, then deg( 
p :. 
.., 
and if deg( 
p < 
1, then pis called constant. 

Lemma 15.1 Fortwo polynomialspq . 
K[t thefollowing assertions hold: 

(1) deg( 
p. 
q L 
max{deg( 
p deg(q }. 
(2) deg( 
p. 
q . 
deg( 
p . 
deg(q. 
Proof Exercise. . 


We nowintroduce some conceptsassociated with thedivisionofpolynomials. 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_15 

214 
15 Polynomials andthe Fundamental TheoremofAlgebra 

Definition 15.2 Let K be a field. 

(1) Iffor two polynomials ps . 
K[t there exists a polynomial q . 
K[t with 
p . 
s . 
q,then s is calleda divisor of pand we write s. 
p(read this as “s divides 
p”). 
(2) Two polynomials 
ps . 
K[t are called coprime,if q|p and q|s for some 
q . 
K[t always implythat q is constant. 
(3) Anon-constant polynomial p . 
K[t is called irreducible (overK), if p . 
s . 
q 
for two polynomials sq . 
K[t impliesthat s or q are constant.Ifthereexist 
two non-constant polynomials sq . 
K[t with p . 
s . 
q, then p is called 
reducible (overK). 
Note that thepropertyof irreducibilityis only defined for polynomialsofdegree 
at least1.Apolynomialofdegree1isalways irreducible. Whethera polynomialof 
degreeat least2is irreduciblemay dependonthe underlying field. 

Example 15.3 The polynomial2 . 
t2 . 
Q[t is irreducible,but the factorization 
. 
v 
. 
v 
. 


2. 
t2 . 
2. 
t . 
2. 
t 

showsthat2 . 
t2 . 
R[t is reducible. The polynomial1 . 
t2 . 
R[t is irreducible, 
butusing theimaginary unit iwe have 

1. 
t2 . 
(.i. 
t . 
(i. 
t 

so that1 . 
t2 . 
C[t is reducible. 

The next result concerns the division with remainder of polynomials. 

Theorem 15.4 If p . 
K[t and s . 
K[t ]\{0}, then there exist uniquely defined 
polynomialsq r . 
K[t with 

p . 
s . 
q. 
r and deg(r < 
deg(s 
(15.1) 

Proof Weshowfirsttheexistenceof polynomialsqr . 
K[t such that(15.1)holds. 

.1

If deg(s . 
0,thens . 
s0 foran s0 . 
K \{0. 
and(15.1)follows withq :. 
s0 . 
p 
and r :. 
0, wheredeg(r < 
deg(s . 

We now assume that deg(s . 
1. If deg( 
p < 
deg(s , then we set q :. 
0 and 
r :. 
p.Then p . 
s . 
q . 
r with deg(r < 
deg(s . 

Let n :. 
deg( 
p . 
m :. 
deg(s . 
1.We prove(15.1)byinductionon n.If 
n . 
1, then m . 
1. Hence p . 
p1 . 
t . 
p0 with p1 
. 
0ands . 
s1 . 
t . 
s0 with 
s1 
. 
0. Therefore, 

.1 .1

p . 
s . 
q. 
r for q :. 
p1sr :. 
p0 . 
p1s

1 
1 s0 

wheredeg(r < 
deg(s . 


15.1 Polynomials 215 
Supposethat theassertion holds foran n . 
1. Lettwo polynomials pand s with 
n . 
1 . 
deg( 
p . 
deg(s . 
m be given, and let pn+1(
. 
0 and sm(
. 
0 be the 
highest coefficients of pand s.If 

.1

h :. 
p. 
pn+1ss . 
tn+1.m . 
K[t

m 

then deg(h < 
deg( 
p . 
n. 
1.By theinductionhypothesis thereexist polynomials 

qr . 
K[t with 
h . 
s . 
q. 
r and deg(r < 
deg(s 
It then follows that 

tn+1.m

p . 
s . 
q . 
r with q :. 
q. 
pn+1s.1 

m 

wheredeg(r < 
deg(s . 
It remainstoshowthe uniqueness. Supposethat(15.1)holds and that thereexist 
polynomials 

q 

r . 
K[t with p . 
s . 

q +
r and deg(

r < 
deg(s .Then 
r .
r . 
s . 
(
q. 
q 
If

r . 
r 
. 
0, then

q . 
q 
. 
0and thus 
deg(r .
r . 
deg(s . 
(

q . 
q . 
deg(s . 
deg(
q. 
q . 
deg(s 
On the other hand, we also have 
deg(r .
r L 
max{deg(r deg(

r . 
< 
deg(s 
This is a contradiction, which shows that indeed r . 

r and q . 
q. . 

This theorem has some important consequences forthe rootsofpolynomials.The 

1

firstof theseis known as the Theorem of Ruffini. 

Corollary 15.5 If . 
. 
Kisaroot ofp . 
K[t , i.e., p(. 
. 
0,then there exists a 
uniquely determined polynomialq . 
K[t with p . 
(t . 
. 
. 
q. 
Proof When we applyTheorem 15.4 to the polynomials pand s . 
t . 
. 

. 
0, then 

we get uniquely determined polynomials q and r with deg(r < 
deg(s . 
1and 
p . 
(t . 
. 
. 
q . 
r 
The polynomial r is constant andevaluatingitat. 
gives 
0. 
p(. 
. 
(. 
. 
. 
. 
q(. 
. 
r(. 
. 
r(. 


1PaoloRuffini(1765–1822). 


216 15 Polynomials andthe Fundamental TheoremofAlgebra 

whichyields r . 
0and p . 
(t . 
. 
. 
q. . 


If a polynomial p . 
K[t hasat leastdegree2andaroot . 
. 
K,then the linear 
factort . 
. 
isadivisor of p and, in particular, p is reducible. The converse of this 
statement does not hold.For instance the polynomial4.4t2+t4 . 
(2.t2 ·(2.t2 . 
Q[t is reducible,butitdoes nothavearootin Q. 

Corollary 15.5 motivatesthe following definition. 

Definition 15.6 If . 
. 
K isaroot of p . 
K[t ]\{0}, then its multiplicity is the 
uniquely determined nonnegative integer m, such that p . 
(t . 
. 
m . 
q for a polynomial 
q . 
K[t with q(. 

. 
0. 

Recursive applicationofCorollary 15.5toagiven polynomial p . 
K[t leads to 
thefollowing result. 

Corollary 15.7 If .1 .k . 
Kare pairwisedistinctrootsof p . 
K[t ]\{0. 
with 
the corresponding multiplicities m1 mk,then thereexistsa unique polynomial 
q . 
K[t with 

m1 ·

p . 
(t . 
.1 . 
(t . 
.k 
mk . 
q 

andq(. 
j 
. 
0for j . 
1 k. In particular, the sum of the multiplicities ofall 
pairwisedistinctrootsofpisat most deg( 
p. 

2

Thenextresultis known as the Lemma of Bezout. 

Lemma 15.8 Ifp s . 
K[t ]\{0. 
are coprime, then thereexist polynomialsq1 q2 . 
K[t with 
p. 
q1 . 
s . 
q2 . 
1 

Proof We may assume without loss of generality that deg( 
p . 
deg(s )(. 
0 , and 
we proceedby inductionondeg(s . 

If deg(s . 
0, thens . 
s0 for an s0 . 
K \{0}, and thus 

.1

p. 
q1 . 
s . 
q2 . 
1 with q1 :. 
0 q2 :. 
s0 

Suppose that the assertion holds for all polynomials ps . 
K[t ]\{0. 
with 
deg(s . 
n for an n . 
0.Let ps . 
K[t ]\{0. 
with deg( 
p . 
deg(s . 
n . 
1be 
given. By Theorem 15.4 there exist polynomials q and r with 

p . 
s . 
q . 
r and deg(r < 
deg(s 

Here we have r 
. 
0, since by assumption pand s are coprime. 

Suppose that there exists a non-constant polynomial h . 
K[t that divides both 
s and r.Then h also divides p,in contradictiontothe assumptionthat p and s are 
coprime. Thus,the polynomials s and r are coprime. Since deg(r < 
deg(s ,we can 

2Etienne Bezout (1730–1783). 


15.1 Polynomials 217 
applythe inductionhypothesis to the polynomials sr . 
K[t ]\{0}.Hence there 
exist polynomials
q1 
q2 . 
K[t with 

s . 
q1 . 
r . 
q2 . 
1 

From r . 
p. 
s . 
q we then get 

1. 
s . 
q1 . 
( 
p. 
s . 
q . 
q2 . 
p. 
q2 . 
s . 
(
q1 . 
q . 
q2 

which completes theproof. . 

Usingthe Lemmaof Bezout we can easily prove thefollowing result. 

Lemma 15.9 If p . 
K[t is irreducible andadivisorof theproducts . 
hof two 
polynomials s h . 
K[t,thenpdividesat least oneofthefactors,i.e.,p|sorp|h. 

Proof If s . 
0, then p|s, becauseevery polynomialisadivisorofthe zero polynomial. 


If s 
. 
0andpis notadivisorof s,then pand s are coprime, since pis irreducible. 
By Lemma 15.8 there exist polynomials q1 q2 . 
K[t with p. 
q1 . 
s . 
q2 . 
1, and 
hence 

h . 
h. 
1. 
(q1 . 
h . 
p. 
q2 . 
(s . 
h 

The polynomial pdivides bothterms on theright hand side, and thus also p|h. . 


By recursive applicationofLemma 15.9 we obtainthe Euclidean theorem,which 
describesaprimefactor decompositioninthe ringof polynomials. 

Theorem 15.10 Every polynomial p . 
.0 . 
.1t +. 
.ntn . 
K[t ]\{0. 
has a 
unique(uptotheorderingofthe factors) decomposition 

p . 
. 
. 
p1 ·. 
pk 

with . 
. 
K and monicirreducible polynomialsp1 pk . 
K[t. 

Proof If deg( 
p . 
0, and thus p . 
.0, then the assertion holds with k . 
0 and 
. 
. 
.0. 

Letdeg( 
p . 
1. If p is irreducible, then the assertion holds with p1 . 
..1p 
and . 
. 
.n.If p is reducible, then p . 
p1 . 
p2 for two non-constant polynomials 
p1 and p2.Theseare either irreducible, or we can decomposethem further.Every 
multiplicativedecompositionofpthatis obtainedinthiswayhasatmostdeg( 
p . 
n 
non-constantfactors. Supposethat 

p . 
. 
. 
p1 ·. 
pk . 
. 
. 
q1 ·. 
q. 
(15.2) 

for some k ,, where 1 L 
. 
L 
k L 
n, .. 
. 
K, as well as monic irreducible 
polynomials p1 pk q1 q. 
. 
K[t .Then p1|pand hence p1|qj forsome j. 
Since the polynomials p1 and qj areirreducible, we musthave p1 . 
qj. 


218 
15 Polynomials andthe Fundamental TheoremofAlgebra 

We may assume without loss of generality that j=1and cancel the polynomial 

p1 =q1 in the identity (15.2), which gives 
. 
·p2 . 
·pk =. 
·q2 . 
·q. 
Proceeding analogously for the polynomials 
. 
=. 
and pj =qj for j=1 k. 
p2 pk, we finally obtain k =, 
. 



15.2 The Fundamental TheoremofAlgebra 
We have seen above that the existence of roots of a polynomial depends on the 
field over which it is considered. The field C is special in this sense, since herethe 
Fundamental Theorem of Algebra3 guarantees that every non-constant polynomial 
hasaroot.Inorderto usethistheoremin our context,we first presentanequivalent 
formulationinthe languageof Linear Algebra. 

Theorem 15.11 The following statements are equivalent: 

(1) Every non-constant polynomial p .C[t has a root inC. 
(2) If V ={0}is a finite dimensional C-vector space, then every endomorphism 
f .L(VV has an eigenvector. 
Proof 

(1) 
.(2 :If V ={0}and f .L(VV , then the characteristic polynomial Pf . 
C[t is non-constant, since deg(Pf =dim(V > 
0. Thus, Pf has a root in C, 
whichis an eigenvalue of f, sothat f indeed has an eigenvector. 
(2) 
.(1 :Let p =.0 +.1t ++.ntn .C[t be a non-constant polynomial 
with .n =0. The roots of p are equal to the roots of the monic polynomial 

p :=..1p.Let A .Cnn be the companion matrix of 
p, then PA . 

p (cp.
n 

Lemma 8.4). 
If V is an n-dimensional C-vector space and B is an arbitrary basis of V, then 
there exists a uniquely determined f .L(VV with [f BB . 
A (cp. Theorem 
10.16). By assumption, f has an eigenvector and hence also an eigenvalue, 
so that 

p =PA has a root. . 


The Fundamental TheoremofAlgebra cannotbeprovenwithouttoolsfromAnalysis. 
In particular, one needs that polynomials are continuous.Wewill usethe followingstandardresult, 
whichis based on the continuityof polynomials. 

Lemma 15.12 Every polynomial p .R[t with odddegree hasa(real)root. 

3Numerous proofsofthisimportant resultexist. Carl FriedrichGau.(1777–1855) alonegave four 
differentproofs, starting with the onein hisdissertation from 1799, whichcontainedhowevera 
gap. Thehistoryof this resultis describedindetailinthe book [Ebb91]. 


15.2 The Fundamental TheoremofAlgebra 
219 
Proof Letthe highest coefficientof pbe positive.Then 

lim 
p(t =+. 
lim p(t =.. 


t>. 
t>.. 


Since the real function p(t is continuous, the Intermediate Value Theorem from 
Analysis impliestheexistenceofarootof p.Theargumentinthe caseofanegative 
leading coefficient is analogous. . 


Ourproofofthe Fundamental TheoremofAlgebrabelowfollowsthepresentation 
in thearticle [Der03].The proofisby inductiononthe dimensionofV.However, 
wedo not usethe usual consecutiveorder, i.e.,dim(V =123 ,but an order 
that is based on the sets 

Mj :={2m ·. 
|0.m L 
j.1 . 
odd}.N j=123 

Forinstance, 

M1 ={. 
|. 
odd}={1357 . 
M2 =M1 .{26 10 14 . 


Lemma 15.13 

(1) IfV is an R-vector space and if dim(V is odd, i.e., dim(V . 
M1, then every 
f .L(VV has an eigenvector. 
(2) LetKbea field andj .N.IfforeveryK-vector space V with dim(V . 
Mj 
every f .L(VV has an eigenvector, then two commuting f1 f2 .L(VV haveacommoneigenvector. Thatis,iff1.f2 . 
f2.f1,then thereexistsavector 
v 
.V \{0}and two scalars .1 .2 .K with f1(v 
=.1v 
and f2(v 
=.2v. 
(3) If V is an R-vector space and if dim(V is odd, then two commuting f1 f2 . 
L(VV have a common eigenvector. 
Proof 

(1) For every 
f .L(VV thedegree of Pf .R[t is odd. Hence Lemma 15.12 
impliesthat Pf has a root, and therefore f has an eigenvector. 
(2)Weproceedbyinductionondim(V ,wheredim(V runs through theelementsof 
Mj in increasingorder.The set Mj isaproper subsetof N consisting ofnatural 
numbersthat are notdivisibleby2j and,in particular,1is thesmallest element 
of Mj. 
If dim(V =1 .Mj,thenby assumptiontwo arbitrary f1 f2 .L(VV each 
have an eigenvector, i.e., 
f1(v1 =.1v1 f2(v2 =.2v2 

Since dim(V =1, we have v1 =.v2 for an . 
.K \{0}.Thus, 

f2(v1 . 
f2(.v2 =. 
f2(v2 =.2(.v2 =.2v1 


220 
15 Polynomials andthe Fundamental TheoremofAlgebra 

i.e., v1 is a common eigenvector of f1 and f2. 
Let now dim(V . 
Mj, and letthe assertionbeprovenfor all K-vector spaces 
whosedimensionsisan elementof Mj thatis smallerthandim(V .Let f1 f2 . 
L(VV with f1 . 
f2 . 
f2 . 
f1.Byassumption, f1 has an eigenvector v1 with 
corresponding eigenvalue .1.Let 

U :. 
im(.1IdV . 
f1 W :. 
Vf1(.1 . 
ker(.1IdV . 
f1 

Thesubspaces U and Wof Vare f1-invariant, i.e., f1(U . 
U and f1(W . 
W. 
Forthe spaceW wehaveshownthisinLemma 14.4andforthe space U this can 
be easily shownaswell(cp.Exercise 14.1). Thesubspaces U and W are also 

f2-invariant: 
If u . 
U,then u . 
(.1IdV . 
f1 )(v 
for a v 
. 
V.Since f1 and f2 commute, we 
have 

f2(u 
. 
( 
f2 . 
(.1IdV . 
f1 ))(v 
. 
((.1IdV . 
f1 . 
f2 )(v 
. 
(.1IdV . 
f1 ( 
f2(v 
. 
U 

If w 
. 
W,then 

(.1IdV . 
f1 ( 
f2(w 
. 
((.1IdV . 
f1 . 
f2 )(w 
. 
( 
f2 . 
(.1IdV . 
f1 ))(w 
. 
f2((.1IdV . 
f1 )(w 
. 
f2(0 . 
0 

hence f2(w 
. 
W. 
We have dim(V . 
dim(U . 
dim(W and since dim(V is notdivisibleby2j, 
either dim(U or dim(W is not divisibleby2j.Hence either dim(U . 
Mj or 
dim(W . 
Mj. 
If the corresponding subspaceisaproper subspaceof V,then its dimensionis 
an element of Mj thatis smallerthan dim(V .Bythe inductionhypothesis then 


f1 and f2 have a common eigenvector in this subspace. Thus, f1 and f2 have a 
common eigenvector in V. 
If the corresponding subspace is equal to V,then this must be the subspace W, 
since dim(W . 
1. ButifV . 
W,theneveryvector in V \{0. 
is an eigenvector 
of f1.Byassumption also f2 hasan eigenvector,sothat thereexistsatleast one 
common eigenvector of f1 and f2. 


(3)By(1)it follows that theassumptionof(2) holds for K . 
R and j . 
1, which 
means that (3) holds as well. . 

We will nowprove the Fundamental Theorem of Algebrainthe formulation(2) 
of Theorem 15.11. 

Theorem 15.14 If V ={0. 
is a finite dimensional C-vector space,theneveryf . 
L(VV has an eigenvector. 


15.2 The Fundamental TheoremofAlgebra 221 
Proof We prove theassertionbyinductionon j. 
123 and dim(V . 
Mj. 

We start with j. 
1and thusby showingthe assertionfor allC-vector spaces of 
odd dimension. Let V be an arbitrary C-vector space with n :. 
dim(V . 
M1.Let 

f . 
L(VV and consider an arbitrary scalar product on V (such a scalar product 
alwaysexists;cp.Exercise 12.1),aswellasthesetof self-adjointmapswith respect 
to this scalar product, 

ad }

H :. 
{g . 
L(VV . 
g . 
g

By Lemma 13.15 the set H forms an R-vector space of dimension n2.If we define 

h1 h2 . 
L(H H by 
h1(g :. 
1 
2( 
f . 
g. 
g. 
fad h2(g :. 
1 
2i( 
f . 
g. 
g. 
fad 

for all g . 
H,then h1 . 
h2 . 
h2 . 
h1 (cp.Exercise 15.8).Since n is odd, also n2 is 
odd.By(3)inLemma 15.13, h1 and h2 havea commoneigenvector.Hence, there 
exists a

g . 
H \{0. 
with 

h1(

g . 
.1
gh2(

g . 
.2

g for some .1 .2 . 
R 

Wehave (h1 . 
ih2 )(g . 
f . 
gfor all g . 
Hand therefore, in particular, 

f . 

g . 
(h1 . 
ih2 )(

g . 
(.1 . 
i.2 

g 

Since 

g 
. 
0, there exists av 
. 
V with 
g(v 

. 
0. Then 

f(
g(v 
. 
(.1 . 
i.2 )(
g(v 


which shows that 
g(v 
. 
V is an eigenvector of f,sothat theproof for j . 
1is 
complete. 

Assume nowthat forsome j. 
1andeveryC-vector space V with dim(V . 
Mj, 
every f . 
L(VV hasaneigenvector.Then(2)inLemma 15.13 impliesthatevery 
two commuting f1 f2 . 
L(VV have a common eigenvector. 

We have to show that for every C-vector space V with dim(V . 
Mj+1,every 
f . 
L(VV has an eigenvector.Since 

Mj+1 . 
Mj .{2jq. 
q odd. 


we onlyhavetoprovethis for C-vector spaces V with n :. 
dim(V . 
2jqfor odd q. 
Let V be such a vector space and let f . 
L(VV begiven.We chooseanarbitrary 
basis of V and denote the matrix representation of f with respect to this basis by 
A. 
Cnn.Let 


222 15 Polynomials andthe Fundamental TheoremofAlgebra 

S :. 
{B. 
Cnn. 
B. 
BT. 


be the set of complex symmetric n. 
nmatrices.Ifwe define h1 h2 . 
L(SS by 

h1(B :. 
AB. 
BAT h2(B :. 
ABAT 

for all B. 
S,then h1 . 
h2 . 
h2 . 
h1(cp.Exercise 15.9).ByLemma 13.16theset 
S forms a C-vector space of dimension n(n. 
1 2.Wehaven . 
2jqfor an odd 
natural number q.Thus, 

n(n. 
12jq(2jq. 
1 

=. 
2j.1q. 
(2jq. 
1 . 
Mj

22 

By theinductionhypothesis,the commuting endomorphisms h1 and h2 havea commoneigenvector.
Hence thereexistsa B. 
S \{0. 
with 

h1(B . 
.1Bh2(B . 
.2B for some .1 .2 . 
C 

In particular,wehave .1B. 
AB. 
BAT.Multiplying this equation from the left 

with Ayields 

B. 
A2 


B. 
ABAT . 
A2 

B . 
A2 
B

.1AB. 
h2(B. 
.2

so that 

A2 . 
.1A. 
.2In B. 
0 

We nowfactorizet2 . 
.1t. 
.2 . 
(t. 
. 
)(t. 
. 
with 

.2 .2

.1 . 
1 . 
4.2 .1 . 
1 . 
4.2 
. 
. 
. 
. 


22 

wherewehaveusedthatevery complex number hasasquareroot.Then 

(A. 
.In ( 
A. 
. 
In B. 
0 

Since B
. 
0, there exists av 
. 
Cn 1 with Bv 

. 
0.If(A. 
. 
In Bv 
. 
0, then Bv 
is 

an eigenvector of Acorresponding to the eigenvalue ..If(A. 
. 
In Bv 

. 
0, then 

( 
A. 
. 
In Bv 
is an eigenvector of Acorresponding to theeigenvalue ..Since Ahas 

an eigenvector, also fhas an eigenvector. . 



15.2 The Fundamental TheoremofAlgebra 223 
MATLAB-Minute. 

Computethe eigenvaluesof thematrix 

.. 


12345 

..

12435 

.. 


.. 
55

A. 
23 415 .R

.. 


..

51423 
42315 

usingthe command eig(A). 
Bydefinitionareal matrix Acan onlyhavereal eigenvalues.The reason forthe 
occurrence of complex eigenvalues is that MATLABinterprets every matrix 
asa complex matrix.Thismeans that withinMATLAB every matrix can be 
unitarily triangulated, sinceevery complex polynomial(ofdegreeat least1) 
decomposes into linearfactors. 


As a direct corollary of the Fundamental Theorem of Algebra and (2) in 
Lemma 15.13 wehave thefollowing result. 

Corollary 15.15 If V ={0}is a finite dimensional C-vector space,then two commuting 
f1 f2 .L(VV have a common eigenvector. 

Example 15.16 Thetwo complex2.2matrices 

i 12i 1

A. 
and B =

1 i 12i 

commute.The eigenvalues of A are ±1+i and those of B are ±2+i.Hence A 

T

and B do not have a common eigenvalue, while [11 T and [.1 1 are common 
eigenvectors of Aand B. 

UsingCorollary 15.15, Schur’s theorem (Corollary 14.20)canbe generalized as 
follows. 

Theorem 15.17 If V ={0}isafinite dimensional unitary vector spaceandf1 f2 . 
L(VV commute,then f1 and f2 can be simultaneously unitarily triangulated, i.e., 
there exists an orthonormal basis B of V, such that [f1 B B and [f2 B B are both 
upper triangular. 

Proof Exercise. . 



224 
15 Polynomials andthe Fundamental TheoremofAlgebra 

Exercises 

(In thefollowingexercises K is an arbitrary field.) 

15.1. Prove Lemma 15.1. 
15.2. Showthe following assertions for p1 p2 p3 .K[t : 
(a) p1|( 
p1p2. 
(b) p1|p2 and p2|p3 implythat p1|p3. 
(c) p1|p2 and p1|p3 implythat p1|( 
p2 +p3. 
(d) If p1|p2 and p2|p1,then thereexistsa c .K \{0}with p1 =cp2. 
15.3. Examinewhether thefollowing polynomials areirreducible: 
p1 =t3 .t2 +t .1.Q[tp4 =t3 .t2 +t .1.R[t 

p2 =t3 .t2 +t .1.C[tp5 =4t3 .4t2 .t +1.Q[t 

p3 =4t3 .4t2 .t +1.R[tp6 =t3 .4t2 .t +1.C[t 

Determinethe decompositions into irreduciblefactors. 

15.4. Decompose the polynomials 
p1 =t2 .2, p2 =t2 +2, p3 . 
t4 .1 and 
p4 =t2 +t +1intoirreduciblefactorsoverthe fields K =Q, K =R and 
K =C. 
15.5. Showthe following assertions for p .K[t : 
(a) Ifdeg( 
p =1, then pis irreducible. 
(b) Ifdeg( 
p .2and phas a root,then pis not irreducible. 
(c) Ifdeg( 
p .{23},then pis irreducibleifand onlyif pdoes not have a 
root. 
15.6. Let 
A .GLn(C , n .2, and let adj( 
A .Cnn be the adjunct of A. Show 
that there exist n .1 matrices Aj . 
Cnn with det(.Aj . 
det(A , j . 
1 n .1, and 
n.1 

adj(A . 
Aj 
j=1 

(Hint: Use PA to construct a polynomial p .C[t .n.1 with adj(A . 
p(A 
and express pas productof linearfactors.) 

15.7. Showthattwopolynomials 
pq .C[t ]\{0}havea commonrootifandonly 
if thereexist polynomials r1 r2 .C[t with0 .deg(r1 < 
deg( 
p such that 
0.deg(r2 < 
deg(q and p·r2 +q·r1 =0. 
15.8. Let V be a finite dimensional unitary vector space, f .L(VV , H ={g . 
L(VV |g =gad }and let 
1 
( 
f .g+g. 
fad

h1 . 
H >L(VV g 
> 


2


15.2 The Fundamental TheoremofAlgebra 
225 
1 
( 
f .g.g. 
fad

h2 
. 
H >L(VV g
> 


2i

Showthat h1 h2 .L(HH and h1 .h2 =h2 .h1. 

15.9. Let A.Cnn , S ={B.Cnn |B=BT}and let 
h1 
. 
S >Cnn 

B>AB +BAT 
h2 . 
S >Cnn 

B>ABAT 

Showthat h1 h2 .L(SS and h1 .h2 =h2 .h1. 

15.10. 
Let V be a C-vector space, f . 
L(VV and let U ={0}be a finite dimensional 
f-invariant subspace of V. Show that U contains at least one 
eigenvector of f. 
15.11. 
Let V ={0}be a K-vector space and let f .L(VV . Showthe following 
statements: 
(a) If 
K . 
C, then there exists an f-invariant subspace U of V with 
dim(U =1. 
(b) If K =R,then thereexistsan f-invariant subspace UofVwith dim(U . 
{12}. 
15.12. 
Prove Theorem 15.17. 
15.13. 
Construct an example showing that the condition f .g =g. 
f in Theorem 
15.17 is sufficientbut not necessary forthe simultaneous unitary triangulation 
of f and g. 
15.14. 
Let A . 
Kn n beadiagonal matrix with pairwisedistinct diagonal entries 
and B.Kn n with AB =BA. Showthatin this case Bisadiagonal matrix. 
What can you say about B,whenthediagonal entriesof Aare not all pairwise 
distinct? 
15.15. 
Showthat thematrices 
.11 01

A. 
B=

1.1 10 

commute and determine a unitary matrix Qsuch that QHAQ and QHBQ 
are upper triangular. 

15.16. 
Showthe following statementsfor p.K[t : 
(a) For all A.Kn n and S.GLn(K we have p(SAS.1 =Sp(AS.1. 
(b) For all ABC.Kn n with AB =CA we have Ap(B =p(CA. 
(c) If K =C and A.Cnn,then there exists a unitary matrix Q, such that 
QHAQ and QHp(AQare upper triangular. 
15.17. 
Let V be a finite dimensional unitary vector space. Let f . 
L(VV be 
f . 
fad . 
fad . 
f.
normal, i.e., f satisfies 


226 
15 Polynomials and the Fundamental Theorem of Algebra 

(a) Showthatif . 
. 
C is an eigenvalue of f,then Vf(. 
. 
is an f-invariant 
subspace. 
(b) Show(using(a))that 
f is diagonalizable.(Hint: Show by induction on 
dim(V ,that V is the direct sum of the eigenspaces of f.) 
(c) Show(using(a) or (b)),that f is even unitarily diagonalizable, i.e., there 
exists an orthonormal basis Bof V such that [ 
f BB is a diagonal matrix. 
(d) Let g . 
L(VV be unitarily diagonalizable. Show that g is normal. 
(This shows that an endomorphism on a finite dimensional unitary vector 
spaceis normal if and only ifitis unitarily diagonalizable.We will give 
a different proof of this result in Theorem 18.2.) 
15.18. 
Let Vbeafinite dimensional K-vector space, f . 
L(VV and V . 
U1.U2, 
where U1 U2 are f-invariant subspaces of V.Let,furthermore, fj :. 
f|Uj . 
L(Uj Uj , j. 
1 2. 
(a) Forevery v 
. 
Vthereexist unique u1 . 
U1and u2 . 
U2 with v 
. 
u1+u2. 
Showthat then also f(v 
. 
f(u1 . 
f(u2 . 
f1(u1 . 
f2(u2. 
(We write this as f . 
f1 . 
f2 and call f the direct sum of f1 and f2 
with respect to the decomposition V . 
U1 . 
U2.) 
(b) Showthat rank( 
f . 
rank( 
f1 . 
rank( 
f2 and Pf . 
Pf1 . 
Pf2. 
(c) Showthat a(. 
f . 
a(. 
f1 . 
a(. 
f2 for all . 
. 
K. 
(Here we set a(. 
h . 
0,if . 
is not an eigenvalue of h . 
L(VV .) 
(d) Showthat g(. 
f . 
g(. 
f1 . 
g(. 
f2 for all . 
. 
K. 
(Herewesetg(. 
h . 
dim(ker(.IdV.h evenif . 
is not an eigenvalue 
of h . 
L(VV .) 
(e) Showthat p( 
f . 
p( 
f1 . 
p( 
f2 for all p . 
K[t . 

Chapter16 
Cyclic Subspaces, Duality and the Jordan 
Canonical Form 

In this chapter we use the duality theory to analyze the proper ties of an 
endomorphism 

f on a finite dimensional vector space V in detail.We are particularly interested in the 
algebraic and geometric multiplicities of the eigenvalues of f and the characterization 
of the corresponding eigenspaces.Our strategy in this analysis is to decomposethe 
vector space V into a direct sum of f-invariant subspaces so that,with appropriately 
chosen bases,the essentialpropertiesof f will be obvious from its matrix representation. 
The matrix representation that we derive is called the Jordan canonical form 
of f.Becauseofits great importance therehave been manydifferent derivationsof 
this form using different mathematical tools. Our approach using duality theoryis 
based on an article by Vlastimil Ptak (1925–1999) from 1956 [Pta56]. 

16.1 Cyclic f-invariant Subspaces andDuality 
Let V bea finite dimensional K-vector space. If f .L(VV and 0 .V\{0},then 
there exists a uniquely defined smallest number m .N, such that the vectors 

0 f( 
0 fm.1( 
0 

are linearly independent and the vectors 

0 f( 
0 fm.1( 
0 fm( 
0 

are linearly dependent.Obviously m . 
dim(V , since at most dim(V vectors of V canbe linearly independent.The number m is calledthe grade of 0 with respect to 

f.We denotethisgradeby m( 
f 0 .Thevector 0 0is linearly dependent, and 
thusitsgradeis0(with respecttoany f). 
©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_16 

228 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

For 0 . 
0wehave m( 
f 0 1ifand onlyifthevectors 0 f( 
0 are linearly 
dependent.This holds if and onlyif 0 is an eigenvector of f.If 0 . 
0is not an 
eigenvector of f,then m( 
f 0 .2. 

For every j.N we define the subspace 

j.1

Kj( 
f 0 : 
span{ 
0 f( 
0 f ( 
0 }.V 

The space Kj( 
f 0 is calledthe jth Krylov subspace1 of f and 0. 

Lemma 16.1 If V isafinite dimensionalK-vector space,f .L(VV ,and 0 .V, 
then thefollowing assertions hold: 

(1)Ifm m( 
f 0 ,then Km( 
f 0 is an f-invariant subspace of V, and 
span{ 
0} 
K1( 
f 0 .K2( 
f 0 .··· 
. 
Km( 
f 0 Km+j( 
f 0 

for allj.N. 

(2)Ifm m( 
f 0 and U .V is an f-invariant subspace that contains the vector 
0,thenKm( 
f 0 .U.Thus,amongallf-invariant subspacesofVthat contain 
the vector 0,the Krylov subspace Km( 
f 0 is the one of smallest dimension. 

(3) If fm.1( 
0 . 
0and fm( 
0 0for an m . 
N, then dim(Kj( 
f 0 jfor 
j 1 m. 
Proof 
(1) Exercise. 
(2) The assertion is trivial if 0 0. Thus, let 0 . 
0with m d( 
f 0 . 
1and 

let U . 
V be an f-invariant subspace that contains 0.Then U also contains 
the vectors f( 
0 fm.1( 
0 , so that Km( 
f 0 . 
U and, in particular, 
dim(U .m dim(Km( 
f 0. 

(3) Let .0 .m.1 . 
K with 
0 .00 ++.m.1 fm.1( 
0 

m.1(v

If we apply fm.1 to bothsides,then0 .0 f0 and thus .0 0, since 
fm.1( 
0 . 
0.Ifm 1, then we applyinductively fm.k for k 2 m and 
obtain .1 ··· 
.m.1 0. Thus, the vectors 0 fm.1( 
0 are linearly 
independent,which impliesthat dim(Kj( 
f 0 jfor j 1 m. . 
	

m.1(v

Thevectors 0 f( 
0 f0 form,byconstruction,abasisoftheKrylov 
subspace Km( 
f 0 . The application of f to a vector fk( 
0 of this 
basis yields thenextbasisvector fk+1( 
0, k 01 m .2, and the application 

m.1(v

of f to thelastvector f0 yieldsalinear combinationofall basisvectors, since 

fm( 
0 .Km( 
f 0.Duetothis special structure,thesubspace Km( 
f 0 is called 
a cyclic f-invariant subspace. 

1AlekseyNikolaevich Krylov (1863–1945). 


16.1 Cyclic f-invariant Subspaces andDuality 229 
Definition 16.2 Let V ={0}bea K-vector space. An endomorphism f .L(VV 

mm.1

is called nilpotent,if f0holds for anm . 
N.Ifatthe same time f. 
0, 
then f is called nilpotent of index m. 

The zero map f 0 is the only nilpotent endomorphism of index m 1. If 
V ={0}, then the zero map is the only endomorphism on V.This map is nilpotent 

m.1 f0

of index m 1, whereinthis caseweomitthe requirement f. 
0. 

m.1(v

If f is nilpotent of index m and . 
0is any vector with f. 
0, then 

m.1 ( 
m.1( 
m.1(v

f( 
ffm( 
00· 
f.Hence fis an eigenvector of f 
correspondingtotheeigenvalue0.Our constructioninSect. 16.2willshowthat0is 
the only eigenvalue ofa nilpotent endomorphism (also cp. Exercise 8.3). 

Lemma 16.3 If V ={0} 
is a K-vector space and if f . 
L(VV is nilpotent of 
index m,then m .dim(V . 

m.1(v

Proof If f is nilpotent of index m, then there exists a 0 . 
V with f0 . 
0 

and fm( 
0 0. By (3) in Lemma16.1 the m vectors 0 fm.1( 
0 are linearly 
independent, which implies that m . 
dim(V . . 
	
Example 16.4 In the vector space K3 1 the endomorphism 

.. 
.. 


.10 

f : 
K31 > 
K31 
.. 
..

.2 
> 
.1 
.3 .2 

is nilpotentof index3,since f . 
0, f2 . 
0and f3 0. 
If U is an f-invariant subspace of V,then f|U .L(UU , where 

f|U : 
U > 
U u 
> 
f(u 
is the restriction of f to the subspace U (cp. Definition 2.12). 
Theorem 16.5 Let V be a finite dimensional K-vector space and f . 
L(VV . 

Then thereexistf-invariant subspaces U1 .V and U2 .V with VU1.U2,such 

that f|U1 .L(U1 U1 is bijective and f|U2 .L(U2 U2 is nilpotent. 
Proof If . 
ker( 
f , then f2( 
f( 
f( 
f(0 0. Thus, . 
ker( 
f2 and 
thereforeker( 
f .ker( 
f2 .Proceeding inductivelywesee that 

{0}.ker( 
f .ker( 
f2 .ker( 
f3 .··· 


m

Since Vis finite dimensional,thereexistsasmallest number m .N0 withker( 
fker( 
fm+j for all j.N.For this number m let 

mm

U1 : 
im( 
fU2 : 
ker( 
f


230 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

(If f is bijective, then m 0, U1 V and U2 ={0}.)We nowshowthat thespaces 
U1 and U2 satisfy the assertion. 
First observe that U1 and U2 are both f-invariant:If . 
U1,then fm(w 
for 
some w 
. 
V, and therefore f( 
f( 
fm(w 
fm( 
f(w 
. 
U1.If . 
U2,then 
fm( 
f( 
f( 
fm( 
f(0 0, and therefore f( 
. 
U2. 
Wehave U1 + 
U2 . 
V.An applicationofthe dimensionformula for linear maps 
(cp. Theorem 10.9)to fm givesdim(V dim(U1 + 
dim(U2 .If . 
U1 . 
U2,then 
fm(w 
for some w 
. 
V (since . 
U1)and hence 

mmmf2m

0 f( 
f( 
f(w 
(w 


m

The first equation holds since . 
U2.By the definitionofm we haveker( 
fker( 
f2m ,which implies fm(w 
0,and therefore fm(w 
0. FromU1.U2 

{0} 
we obtain VU1 . 
U2. 
Let now . 
ker( 
f|U1 . 
U1 be given. Since . 
U1,there exists a vector w 
. 
V with fm(w 
, which implies 0 f( 
f( 
fm(w 
fm+1(w 
.Bythe 

m

definition of m we haveker( 
fker( 
fm+1 , thus w 
. 
ker( 
fm , and therefore 
fm(w 
0. This impliesthatker( 
f|U1 ={0}, i.e., f|U1 is injective and thus 
also bijective(cp. Corollary 10.11). 
If, on the other hand, . 
U2,thenby definition0 fm(v( 
f|U2 
m( 
, and 
thus ( 
f|U2 
m is the zero map in L(U2 U2 ,sothat f|U2 is nilpotent. . 
	

Forthe furtherdevelopment we recallsometerms and results from Chap.11.Let 
V beafinite dimensional K-vector space and let V. 
be the dual space of V.IfU . 
V and W . 
V. 
aretwo subspaces andif thebilinear form 

. 
: 
U . 
W > 
K ( 
h 
> 
h( 
(16.1) 

is non-degenerate, then UWis calledadualpairwith respectto ..Thisrequiresthat 

dim(U dim(W .For f . 
L(U U the dual map f. 
. 
L(U. 
U. 
is defined by 
f. 
: 
U. 
> 
U. 
h 
> 
h. 
f 
For all . 
U and h . 
U. 
we have ( 
f.(h ( 
h( 
f( 
. Furthermore, ( 
fk . 


( 
f. 
k for all k . 
N0.The set 

U0 :={h . 
V.| 
h(u 0 for allu . 
U} 
is called the annihilator of U. This set is a subspace of V. 
(cp. Exercise 11.5). 
Analogously,the set 
W0 : 
{. 
V| 
h( 
0 for allh . 
W} 
is calledthe annihilator of W.This set is a subspace of V. 


16.1 Cyclic f-invariant Subspaces andDuality 231 
Lemma 16.6 Let V be a finite dimensional K-vector space, f . 
L(VV , V. 
the 
dual space of V,f .. 
L(V. 
V. 
the dualmapoff,andlet U . 
V and W . 
V. 
be 
twosubspaces. Then thefollowing assertions hold: 

(1) dim(V dim(W + 
dim(W0 dim(U + 
dim(U0 . 
(2)Iff is nilpotentof indexm,thenf. 
is nilpotent of index m. 
(3) IfW . 
V. 
is an f.-invariant subspace, then W0 . 
V is an f-invariant subspace. 
(4) IfUW area dualpairwithrespecttothebilinear form definedin(16.1), then 
VU . 
W0 . 
Proof 

(1) Exercise. 
(2) For all . 
V we have fm( 
0and hence, 
mm .m

0 h( 
f( 
(( 
f(h ( 
(( 
f. 
(h ( 


for every h . 
V. 
and . 
V, so that f. 
is nilpotent of index at most m. 
m.1

If ( 
f. 
0, then ( 
f. 
m.1(h 0 for all h . 
V., and therefore 0 
m.1( 
m.1

(( 
f. 
m.1(h ( 
h( 
ffor all . 
V. This implies that f0, 
in contradiction to the assumption that f is nilpotent of index m.Thus, f. 
is 
nilpotent of index m. 

(3) Let w 
. 
W0.Forevery h . 
W,wehave f.(h . 
W, and thus0 f.(h )(w 
h( 
f(w 
.Hence f(w 
. 
W0. 
(4) Ifu . 
U . 
W0, then h(u 0for all h . 
W, since u . 
W0.Since UW is 
adualpairwith respecttothebilinearform definedin(16.1),wehave u 0. 
Moreover,dim(U dim(W and using(1) we obtain 
dim(V dim(W + 
dim(W0 dim(U + 
dim(W0 

From U . 
W0 ={0} 
we obtain VU . 
W0. . 
	

Example 16.7 We consider the vector space V R21 with the canonical basis 
B ={e1 e2}.For the subspaces 

0 

U span . 
V

1 

W h . 
V. 
. 
[h]B {1}=[..] 
for an . 
. 
R . 
V. 


we have 

U0 

h . 
V. 
. 
[h]B {1}=[. 
0] 
for an . 
. 
R . 
V. 


W0 span 
1 
. 
V

.1 


232 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

In this example, we easily see that dim(V dim(W +dim(W0 dim(U + 
dim(U0 , and that UW formadualpairwith respecttothebilinearform definedin 
(16.1)with K R.Moreover, VU .W0. 

Thefollowing theorem presents, foragivennilpotent f, a decomposition of V into f-invariant subspaces.The ideaof the decompositionisto constructa dual pair 
of subspaces U . 
V and W . 
V., where U is f-invariant and W is f.-invariant. 
By(3)inLemma 16.6 then W0 is f-invariantandwith(4)inLemma 16.6it follows 
that VU .W0. 

Theorem 16.8 Let V bea finite dimensionalK-vector space and letf . 
L(VV be nilpotent of index m. Let 0 . 
V satisfy fm.1( 
0 . 
0 and leth0 . 
V. 
satisfy 
h0( 
fm.1( 
0 . 
0. 
Thenm( 
f 0 m( 
f. 
h0 m,andthef -andf .-invariant subspaces Km( 
f 0 
.V and Km( 
f. 
h0 .V.,respectively,area dualpairwithrespecttothebilinear 
form definedin(16.1). Furthermore, 

0

VKm( 
f 0 .(Km( 
f. 
h0 

where (Km( 
f. 
h00 is an f-invariant subspace of V. 

Proof Let 0 . 
V be a vector with fm.1( 
0 . 
0. Since fm( 
0 0, the space 
Km( 
f 0 is an m-dimensional f-invariant subspace of V (cp. (3)inLemma 16.1). 
Let h0 .V. 
be a vector with 

m.1

0. 
h0( 
f( 
0 (( 
f. 
m.1(h0 ( 
0 

Then,in particular,0 . 
( 
f. 
m.1(h0 . 
L(V. 
V. 
.Since f is nilpotent of index m, 
also f. 
is nilpotent of index m (cp. (2)inLemma 16.6), so that 

( 
f. 
m(h00.L(V. 
V. 


Therefore, Km( 
f. 
h0 is an m-dimensional f.-invariant subspace of V. 
(cp. (3)in 
Lemma 16.1). 

It remainstoshowthat Km( 
f 0 Km( 
f. 
h0 are a dual pair. Let 

m.1 

1 .jfj( 
0 .Km( 
f 0 
j 0 

beavector with h( 
1 .( 
1 h 0for allh .Km( 
f. 
h0 .Weshowinductively 
that then .0 ··· 
.m.1 0, and thus 1 0. 


16.1 Cyclic f-invariant Subspaces andDuality 233 
Using ( 
f. 
m.1(h0 .Km( 
f. 
h0 our assumptiononthevector 1 yields 

m.1 

m.1m.1+j

0 (( 
f. 
m.1(h0 ( 
1 h0( 
f( 
1 .jh0( 
f( 
0 
j 0 

.0h0( 
fm.1( 
0 

The last equation holds, since fm.1+j( 
0 0for j 1 m . 
1(because 

m

f0).From h0( 
fm.1( 
0 . 
0we obtain .0 0. 
Suppose now that .0 ··· 
.k.1 0fora k,1 . 
k . 
m . 
2. Using 
( 
f. 
m.1.k(h0 .Km( 
f. 
h0 our assumptiononthevector 1 yields 

m.1 

m.1.km.1+j.k

0 (( 
f. 
m.1.k(h0 ( 
1 h0( 
f( 
1 . 
jh0( 
f( 
0 
j 0 

.kh0( 
fm.1( 
0 

m.1+j.k(v

Thelast equation holds,since .j 0for j 0 k.1and f00 
for jk+1 m .1. 

Wehave 1 0asasserted, and thereforethe bilinear form definedin(16.1) 
for the spaces Km( 
f 0 Km( 
f. 
h0 is non-degenerateinthe firstvariable. Analogously, 
the bilinear form is non-degenerate in the second variable, and hence 
Km( 
f 0 Km( 
f. 
h0 are a dual pair. 

Using(4)in Lemma 16.6 we nowhave VKm( 
f 0 .(Km( 
f. 
h0 
0, where 
the space (Km( 
f. 
h0 
0,isby(3)in Lemma 16.6 an f-invariant subspace of V. . 
	

16.2 TheJordan CanonicalForm 
Let V be a finite dimensional K-vector space and f . 
L(VV . If there exists 
a basis B of V consisting of eigenvectors of f, then [f]BB isadiagonal matrix, 
i.e., f is diagonalizable. A necessary and sufficient condition for this is that the 
characteristic polynomial Pf decomposes into linear factors over K and that in 
addition g( 
f .ja( 
f . 
j for every eigenvalue . 
j (cp. Theorem 14.14). 

If Pf decomposes into linear factors but g( 
f . 
ja( 
f . 
j holds for at 
least one eigenvalue .j, then f is not diagonalizablebut can still be triangulated, 
i.e., there exists a basis B of V, such that [f]BB is an upper triangular matrix 
(cp. Theorem 14.17). From this triangular matrix we can readoffthe algebraic,but 
usuallynotthe geometricmultiplicitiesoftheeigenvalues.Thegoalofthefollowing 
constructionisto determinea basis Bof V, sothat [f]BB is upper triangular and in 
additiontothe algebraicalsorevealsthe geometricmultiplicitiesofthe eigenvalues. 

Under the assumption that Pf decomposes into linear factors over K, we will 
construct a basis B of V for which [f]BB is a blockdiagonal matrix of theform 


234 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

..

Jd1(.1 

..

] 
f]BB 

Jdm (.m 

where each diagonal block has theform 

.. 


. 
j 1 

.. 


.. 


Kdj dj

Jdj(.j : 
.. 
. 
(16.2) 

..

1 

. 
j 

for some .j . 
K and dj . 
N, j 1 m.Amatrixof theform(16.2)iscalleda 
Jordan blockof size dj corresponding to the eigenvalue . 
j. 

In the following construction we first do not assume that Pf decomposes into 
linearfactors.We onlyassume theexistenceofasingleeigenvalue .1 . 
K of f. 
Usingthiseigenvalue, we define the endomorphism 

g : 
f . 
.1IdV . 
L(VV 
By Theorem 16.5 there exist g-invariant subspaces U . 
V and W . 
V with 

VU . 
W 

such that 

g1 : 
g|U 

is nilpotent and g|W is bijective. Then U ={0}, since otherwise WV and 
g|W g|V g wouldbebijective, which contradictsthe assumptionthat .1 is an 
eigenvalue of f. 

Let g1 be nilpotent of index d1. Then by construction 1 . 
d1 . 
dim(U .Let 

d1.1 d1 d1.1 

w1 . 
U beavector with g (w1 . 
0. Sinceg1 (w1 0, the vectorg (w1 is

11 

aeigenvector of g1 correspondingtotheeigenvalue0.By(3)inLemma 16.1,the d1 
vectors 

d1.1 

w1 g1(w1 g (w1

1 

are linearly independent and U1 : 
Kd1(g1 ,w1 isa d1-dimensional g1-invariant 
subspace of U. 

Consider the basis 

B1 : 
g1 
d1.1(w1 g1(w1 w1 

of U1.Then thematrixrepresentation g1|U1 with respect to the basis B1 isgivenby 

[g1|U1]B1 B1 Jd1(0 . 
Kd1 d1 


16.2 The Jordan CanonicalForm 235 
Thisshows,in particular,thatthe characteristic polynomialof g1|U1 isgivenbythe 
monomial td1,and hence0isthe onlyeigenvalueof g1|U1.Moreover,byconstruction 

[g1|U1]B1 B1 =[g|U1]B1 B1. 

If d1 dim(U ,thenour constructioniscompleteforthemoment.If,ontheother 
hand, d1 dim(U ,then applying Theorem 16.8 to g1 . 
L(UU showsthat there 

exists a g1-invariant subspace U ={0} 
with UU1 . 
U, and we consider 

g2 : 
g1|U

Thismapisnilpotentof index d2, where1 . 
d2 . 
d1.Wenow carry out thesame 
construction as before: 

d2.1 d2.1

We determine a vector w2 . 
U with g2 (w2 . 
0. Then g2 (w2 is an 
eigenvector of g2, U2 : 
Kd2(g2 ,w2 isa d2-dimensional g2-invariant subspace of 

U . 
U and forthe basis 

g (w2 g2(w2

B2 : 
2 
d2.1 
w2 

of U2 we have 

[g2|U2]B2 B2 Jd2(0 . 
Kd2 d2 

where again [g2|U2]B2 B2 =[g|U2]B2 B2 by construction. 

After k . 
dim(U steps this procedureterminates.Wethenhave founda decomposition 
of U of theform 

UKd1(g1 ,w1 .. 
Kdk(gk ,wk Kd1(g ,w1 .. 
Kdk(g ,wk 

Inthesecond equationwehaveusedthatKdj(gj w 
j Kdj(g ,wk for j 1 k. 
If we combine the constructed bases B1 Bk to a basis B of U,then 

. 
...

[g|U1]B1 B1 Jd1(0 

. 
...

[g|U]BB 

[g|Uk]Bk Bk Jdk(0 

Thus, the nilpotent endomorphism g1 g|U has the characteristic polynomial 
td1++dk, and its onlyeigenvalueis0. 

We nowtransfertheseresultsto 

fg+ 
.1IdV 

Every g-invariant subspace is f-invariant and one observes easily that 

Kdj( 
f w 
j Kdj(g w 
jj 1 k 


236 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

(cp.Exercise 16.3).Hence,it follows that 

UKd1( 
f ,w1 .. 
Kdk( 
f ,wk 

For every j 1 k and0 . 
. 
. 
dj . 
1wehave 

+1

fg(w 
j gg(w 
j + 
.1g(w 
j .1g(w 
j + 
g(w 
j (16.3) 

where gdj(w 
j 0. The matrix representation of f|U with respect to the basis Bof 
U is thereforegivenby 

. 
...

] 
f|U1]B1 B1 Jd1(.1 

. 
...

] 
f|U]BB (16.4) 

] 
f|Uk]Bk Bk Jdk(.1 

The map g|W f|W . 
.1IdW is bijectiveby construction, i.e., .1 is not an 
eigenvalue of f|W. Therefore, a( 
f .1 dim(U d1 ++ 
dk. In order to 
determine g( 
f .1 ,let . 
Ubean arbitraryvector.Then thereexist scalars .j. 
. 
K 
with 

k dj.1 

.j,g(w 
j 

j 1 . 
0 

Using(16.3)we obtain 

k dj.1 
. 
k dj.1 k dj.1 

+1

f( 
. 
j. 
fg(w 
j . 
j,.1g(w 
j + 
. 
j,g(w 
j 
j 1 . 
0 j 1 . 
0 j 1 . 
0 

k dj.2 

.1 + 
. 
j,g+1(w 
j 
j 1 . 
0 


The vectors in the last sum are linearly independent. Hence, f( 
.1 if and 
onlyif .j. 
0for j 1 k and . 
01 dj . 
2. This showsthatevery 
eigenvector of f corresponding to the eigenvalue .1 has theform 

k 

dj.1

.jg(w 
j 

j 1 

whereatleast one . 
j is nonzero, so that we have 

d1.1dk.1

Vf(.1 span{g(w1 g(wk } 



16.2 The Jordan CanonicalForm 237 
Since gd1.1(w1 gdk.1(wk are linearly independent,itfollows that g( 
f .1 

k.The geometricmultiplicityoftheeigenvalue .1 thereforeisequaltothe numberof 
Jordan blocks correspondingtotheeigenvalue .1in thematrixrepresentation(16.4). 
Furthermore, we observe that in every subspace Kdj( 
f w 
j , the endomorphism f 
hasexactlyone (linear independent)eigenvector correspondingtotheeigenvalue .1. 
We summarize theseresultsin thefollowing theorem. 

Theorem 16.9 Let V bea finite dimensionalK-vector spaceandletf . 
L(VV . 
If .1 . 
Kisaneigenvalueoff, then thefollowing assertions hold: 

(1) There exist f -invariant subspaces {0}. 
U . 
V and W . 
V with VU . 
W. 
The map f |U . 
.1IdU is nilpotent and the map f |W . 
.1IdW is bijective. In 
particular, .1 is not an eigenvalue of f |W. 
(2) The subspace U from (1) can be written as 
UKd1( 
f ,w1 .. 
Kdk( 
f ,wk 

for some vectors w1 ,...,wk . 
U, where Kdj( 
f w 
j is a dj-dimensional f-
invariant subspace of V,j 1 k. Thisis calleda cyclic decomposition 
of U. 

(3) Thereexistsa basisB of U with 
..

Jd1(.1 

..

] 
f|U]BB 

Jdk(.1 

(4) We have a( 
f .1 d1 ++ 
dk andg( 
f .1 k. 
If f hasafurther eigenvalue .2 . 
.1, then it is an eigenvalue ofthe restriction 
f|W . 
L(WW and we can applyTheorem 16.9 to f|W.The vector space W thenisthedirect sumoftheform WX . 
Y,where f|X . 
.2IdX is nilpotent and 
f|Y . 
.2IdY is bijective. The space X has a cyclic decomposition analogous to (2) 

in Theorem 16.9, and there exists a matrix representation of f|X analogous to (3). 

This construction can be carried out for all eigenvalues of f.Ifthe characteristic 
polynomial Pf decomposes into linearfactorsover K,then we finally obtainacyclic 
decompositionofthe entirespace V,whichgivesthe following theorem. 

Theorem 16.10 Let V bea finite dimensionalK-vector spaceandletf . 
L(VV . 
If thecharacteristic polynomialPf decomposes into linear factorsoverK,then there 

exists a basis B of V, suchthat 
. 
Jd1(.1 . 
] 
f]B B . 
Jdm (.m . 
(16.5) 

where .1 .m . 
Karethe(not necessarily pairwisedistinct)eigenvaluesoff . 
For every eigenvalue .j offthena( 
f . 
j is equal to the sum of the sizes of all 


238 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

Jordan blocks corresponding to .j in(16.5), andg(f . 
j is equal to the number 
ofJordan blocks corresponding to . 
j in(16.5).A matrixrepresentationofthe form 
(16.5)is calledaJordan canonical form2 of f . 

From Theorem 14.14 we know that f . 
L(VV is diagonalizableif and only 
if Pf decomposes into linear factors over K and g(f . 
ja(f .j holds for 
every eigenvalue .j of f.If Pf decomposes into linear factors, then the Jordan 
canonical form(16.5)shows that g(f . 
ja(f .j if and onlyif every Jordan 
block corresponding to . 
j is of size 1. 

The Fundamental Theorem of Algebra yields the following corollary of Theorem 
16.10. 

Corollary 16.11 If Visafinite dimensionalC-vector space,theneveryf . 
L(VV hasaJordan canonical form. 

Thefollowing uniquenessresultjustifies the name canonical form. 

Theorem 16.12 Let V bea finite dimensionalK-vector space.Iff . 
L(VV has 
aJordan canonical form,thenitis uniqueuptotheorderoftheJordan blockson 
thediagonal. 

Proof Letdim(V n and let B1 B2 be two bases of V with 

. 
Jd1(.1 . 
A1 ] 
f]B1 B1 . 
Jdm (.m . 
. 
Kn n 
as well as 
. 
Jc1(.1 . 
A2 ] 
f]B2 B2 . 
Jck(.k . 
. 
Kn n 

For a given eigenvalue. 
j,1 . 
j. 
m, we define 

r(1 (. 
j : 
rank (A1 . 
. 
jIns s 012 

s 

Then 

d(1 (1 (1

(. 
j : 
r. 
j . 
r(. 
js 12 

ss.1(s 

is equal to the number of Jordan blocks J(.j . 
K. 
on thediagonal of A1 with 

. 
s.The number of Jordan blocks corresponding to the eigenvalue .j with exact 
size s thereforeisgiven by 
d(1 .j . 
d(1 (1 (1 (1

(s+1(. 
j rs.1(.j . 
2r(.j + 
rs+1(. 
j (16.6)

ss 

2MarieEnnemond Camille Jordan (1838–1922) derived thisform 1870.Twoyears earlier, Karl 
Weierstra. (1815–1897) proveda result that impliesthe Jordan canonical form. 


16.2 The Jordan CanonicalForm 239 
(cp. Example 16.13). 

The matrices A1 and A2 are similar and, therefore, have the same eigenvalues, 
i.e., 

{.1 .m}={.1 .k} 


Furthermore, 

. 
m. 
m. 
rank A1 ..In rank A2 ..In 

for all . 
.K and m .N0. 

In particular,forevery . 
j there exists .i .{.1 .k}with .i .j and for 
this .i and the matrix A2 we get 

(2 s(1

r(.i : 
rank A2 ..iIn r(. 
js 012

ss 

Now(16.6)shows that thematrix A2 has,uptoreordering, thesameJordan blocks 
on thediagonal as thematrix A1. . 
	

Example 16.13 This example illustrates the construction in the proof of Theorem 
16.12.If 

. 


. 


.. 


. 


.

J2(1 

.. 


. 
55 

. 


..

AJ1(1 

. 
R(16.7) 

. 


. 


. 


.

J2(0 

11 
1 
1 
01 
0 

then (A.1·I50 I5, 

. 


.. 


. 


. 


.. 


. 


.. 
.. 


. 


. 
2 
. 


.

A.1·I5 

,(A.1·I5 

. 


.. 


. 


. 


.. 


. 


01 
0 
0 
.11 
.1 

00 
0 
0 
1.2 
1 

and we get 
r0(1 5 r1(1 3 rs(1 2 s .2 
d1(1 2 d2(1 1 ds(1 0 s .3 
d1(1 .d2(1 1 d2(1 .d3(1 1 ds(1 .ds+1(1 0 s .3 


240 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

We now consider thepowersofa Jordan block Jd(. 
. 
Kdd.Since Id and Jd(0 
commute, 

k 
. 
k 

. 
k 
. 
p( 
j (.

k .k.j

(Jd(. 
(.Id + 
Jd(0 k (Jd(0 j (Jd(0 j 

jj! 


j 0 j 0 

for every k . 
N0, where p( 
j is the jth derivative of the polynomial p tk with 
respect to t, 

(0 (tk (0 ( 
j (tk ( 
j

ptkpk(k.1 ··(k.j+1 tk.jj 1 k 

We can now easily show thefollowing result. 

Lemma 16.14 If p . 
K[t]isa polynomialofdegreek .0,then 

k 

. 
p( 
j (. 
p(Jd(. 
(Jd(0 j (16.8)

j! 


j 0 

Proof Exercise. . 
	

Considered as a linear map from Kd 1 to Kd 1, the matrix Jd(0 represents an 
“upshift”, since 

.... 
.. 


.1 .2 .1 
.
.2... 
.
.2. 


.... 
.. 
. 
Kd 1

Jd(0 for all 

.... 
..

.d 
.d 0 .d 

Clearly, 
(Jd(0 . 
. 
0 . 
01 d.1 (Jd(0 d 0 

and hence the linear map Jd(0 is nilpotent of indexd.The sum on the right hand 
sideof(16.8)thereforehasat mostd terms, even when deg( 
pd. 

Moreover,theright hand sideof(16.8)shows thatp(Jd(. 
is an upper triangular 
matrix with constant entries on its diagonals.A matrixwith constant diagonalsis 
calleda Toeplitz matrix.3In particular,onthemain diagonalwehavetheentry p(. 
. 
From(16.8)wesee that p(Jd(. 
0holdsif and onlyif 

(d.1

p(. 
p(. 
··· 
p(. 
0 

Thus wehave shownthe following result. 

3OttoToeplitz (1881–1940). 


16.2 The Jordan CanonicalForm 241 
Lemma 16.15 Let p .K[t]bea polynomial andJd(. 
.Kdd beaJordan block. 

(1) The matrix p(Jd(. 
isinvertibleif and onlyif. 
is notarootofp. 
(2) We have p(Jd(. 
0 .Kdd if and onlyif. 
isad-foldrootofp, i.e.,if the 
linear factor (t .. 
d isadivisorofp. 
Let V be a finite dimensional K-vector space and let f . 
L(VV , where we 
do not assume that Pf decomposes into linearfactors. From theCayley-Hamilton 
theorem (Theorem 8.6)weknowthatPf( 
f 0.L(VV ,i.e., thereexistsamonic 
polynomial ofdegree at mostdim(V ,which annihilatesthe endomorphism f.Let 
p1 p2 .K[t]betwomonic polynomialsof smallest possibledegreewith p1( 
f 
p2( 
f 0. Then( 
p1.p2 ( 
f 0,and sincep1 and p2 aremonic, p1.p2 .K[t]is 
a polynomialwithdeg( 
p1 .p2 deg( 
p1 deg( 
p2 .The minimality assumption 
on deg( 
p1 and deg( 
p2 implies that p1 .p2 0, i.e., p1 p2.Thus,for every 

f .L(VV thereexistsauniquely determined monic polynomialofminimaldegree 
which annihilates f.Thisjustifies thefollowing definition. 

Definition 16.16 If V is finite dimensional K-vector space and f .L(VV ,then 
the uniquely determined monic polynomial ofminimal degree that annihilates f is 
calledthe minimal polynomial of f.We denotethis polynomialby Mf. 

By construction we always have deg(Mf .deg(Pf dim(V . 

Lemma 16.17 If V isa finite dimensionalK-vector space andf .L(VV ,then 
theminimal polynomialMf dividesevery polynomialthat annihilates f and is,in 
particular,a divisorofthecharacteristic polynomialPf. 

Proof For p 0we have p( 
f 0 and Mf divides p.If p . 
K[t]\{0}is a 
polynomialwith p( 
f 0, then deg(Mf .deg( 
p .Using division with remainder 
(cp. Theorem 15.4), there exist uniquely determined polynomials qr .K[t]with 
pq ·Mf +r and deg(r deg(Mf .Thus, 

0 p( 
fq( 
f Mf( 
f +r( 
fr( 
f 

The minimality of deg(Mf impliesthat r 0, and hence Mf divides p. . 
	

If Pf decomposes into linearfactors, then we canexplicitly construct Mf using 
theJordan canonical formof f. 

Lemma 16.18 Let V bea finite dimensionalK-vector space.Iff .L(VV has a 

Jordan canonical form with pairwisedistincteigenvalues.1 .k andifd1 dk 
aretherespectivemaximalsizesof the correspondingJordan blocks,then 

k 


dj

Mf (t .. 
j 
j 1 


242 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

Proof We know fromLemma16.17 that Mf is adivisor of Pf.Therefore, 

k 
. 
Mf (t . 
. 
j 
. 
j 
j 1 
for some exponents 1 ,...,k.If 
. 
Jd1(.1 . 
A 
. 
. 


Jdm (.m 

is a Jordan canonical form of f, then Mf( 
f 0 . 
L(VV is equivalent to 
Mf(A 0 . 
Kn n, where n dim(V .Wehave Mf(A 0 if and only if 
Mf(Jdj(.j 0for j 1 m. For this it is necessary and sufficient that 

Mf(J

(.j 0for j 1 k.ByLemma 16.15 this holdsif and onlyifevery

dj


dj

of the linearfactors (t . 
. 
j , j 1 k,is a divisor ofMf.Therefore, Mf has 
the desired form. . 
	

Example 16.19 If f isan endomorphismwiththeJordan canonicalform Ain(16.7), 
then 

3 t22 t2

Pf (t . 
1 Mf (t . 
1 

and 

. 


.. 


. 


. 


.. 


. 


. 
...

2 A2 
. 


.. 


.

Mf(A ( 
A. 
1· 
I5 

. 


.. 


. 


. 


.. 


. 


00 
0 
0 
1.2 
1 

12 
1 
1 
00 
0 

which shows that Mf(A 0. 
R55 and Mf( 
f 0. 
L(VV . 

TheJordan canonical form is of great importance in theoretical Linear Algebra. 
In practical applications,however,whereusually matricesover K R or K C 
are considered,itisnotsorelevant,since thereisno numerically stablemethodfor 
computingtheJordan canonical formofageneral matrixinfinite precision arithmetic. 
Thereasonforthelackofsuchamethodisthatthe entriesoftheJordan canonical 
formdo not depend continuously on the entriesof thegiven matrix. 

Example 16.20 Consider the matrix 
. 
. 
A(. 
. 
1 
00 . 
. 
R 


16.2 The Jordan CanonicalForm 243 
For everygiven . 
. 
0,thematrix A(. 
has thetwo distinct eigenvalues . 
and0, and 
hence thediagonal matrix 

. 
0

J(. 


00 

isaJordan canonical formof A(. 
.However,for . 
> 
0, we obtain 

01 00

A(. 
> 
J(. 
>

00 00 

Thus, J(. 
does not convergetothe Jordan canonical formof A(0 for. 
> 
0. 

A similar example is given by the matrices in Exercise 8.5: While A(0 isa 
Jordan block of size n correspondingto theeigenvalue1,forevery . 
. 
0we obtain 
adiagonalizable matrix A(. 
. 
Cnn with n pairwisedistinct eigenvalues. 

MATLAB-Minute. 

Let 
. 


10

T.1 22 

AT . 
C

11 

where T . 
C22 is a random matrix constructed with the command T= 
rand(2).Constructseveralsuch matricesandalways computetheeigenvalues 
usingthe command eig(A).Display the eigenvalues in format long. 
One observesthat thetwoeigenvalues arereal or complexconjugates, and that 
theyalwayshave an errorstartingfromthe 8thdigit afterthe decimal point, 
i.e., an error on the order of 10.8. This does not happen by chance, but is 
duetothebehavioroftheeigenvalues under perturbations,which arisefrom 
rounding errors in the computer. 

16.3 ComputationoftheJordan CanonicalForm 
We now derive a method for the computation of the Jordan canonical form of an 
endomorphism f on a finite dimensional K-vector space V. We assume that Pf 
decomposes into linearfactorsover K, and that therootsof Pf, i.e., theeigenvalues 
of f,areknown.The constructionfollowstheimportantstepsintheexistenceproof 
of theJordan canonical formin Sect.16.2. 

Supposethat . 
isan eigenvalueof f and that f hasacorresponding Jordan blockof 
size s.Then thereexist s linearly independentvectors t1 ts with ] 
f]
Js(.

BB 


244 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

for B ={t1 ts}.With t0 : 
0 and writingIdinstead of IdV for simplicity of 

notation, we then have 

( 
f . 
.Id )(t1 t0 
( 
f . 
.Id )(t2 t1 

( 
f . 
.Id )(ts ts.1 

hence ts. 
j ( 
f . 
.Id j(ts for j 01 s. 

Thevectors ts ts.1 t1 formasequenceasthe onewehave constructedinthe 
context ofthe Krylov subspaces, and 

span{ts ts.1 t1} 
Ks( 
f . 
.Id ts 

The reverse sequence 

t1 t2 ts 

is called a Jordan chainof f corresponding to the eigenvalue ..The vector t1 is an 
eigenvectorof f corresponding to ..For thevector t2 we then have ( 
f..Id )(t2 . 
0 
and 

( 
f . 
.Id 2(t2 ( 
f . 
.Id )(t10 

Hence t2 . 
ker(( 
f . 
.Id 2 ^ 
ker( 
f . 
.Id , and in general 

tj . 
ker(( 
f . 
.Id j ^ 
ker(( 
f . 
.Id j.1 j 1 s 

This motivatesthe following definition. 

Definition 16.21 Let V be a finite dimensional K-vector space, let f . 
L(VV have the eigenvalue . 
. 
K, and let k . 
N.A vector . 
V with 

. 
ker(( 
f . 
.Id k ^ 
ker(( 
f . 
.Id k.1 

is called a principal vectoroflevelk of f corresponding to the eigenvalue .. 

Principalvectorsoflevel one areeigenvectors. Principalvectorsofhigherlevels 
canbe considered generalizationsofeigenvectors,andtheyarethereforesometimes 
called generalized eigenvectors. 

Forthe computationofthe Jordan canonical formof f,wethus need to knowthe 
numberand lengthsoftheJordan chains correspondingtothedifferenteigenvalues 
of f.These correspondtothe numberand sizesoftheJordan blocksof f.If F is a 
matrix representation of f with respect to an arbitrary basis,then (cp. theproof of 
Theorem 16.12) 


16.3 Computationofthe Jordan CanonicalForm 
245 
s

ds(. 
: 
rank((F ..Is.1 .rank((F ..I 
dim(im(( 
f ..Id s.1 .dim(im(( 
f ..Id s 
dim(V .dim(ker(( 
f ..Id s.1 .(dim(V .dim(ker(( 
f ..Id s 
dim(ker(( 
f ..Id s .dim(ker(( 
f ..Id s.1 

is the number of Jordan blocks corresponding to . 
of size at least s.Thisimplies, in 
particular,that 

ds(. 
. 
ds+1(. 
. 
0 s 12 

and ds(. 
.ds+1(. 
is the number of Jordan blocks of exact size s corresponding 
to ..There exists a smallest number m . 
N with 

{0} 
ker(( 
f ..Id 0 . 
ker(( 
f ..Id 1 .··· 
.ker(( 
f ..Id m ker(( 
f ..Id m+1 
Hence ds(. 
0for alls . 
m +1, so that thereisnoJordan block corresponding 
to . 
of size m +1orlarger. 

In orderto computethe Jordan canonical form,wethereforeproceedas follows: 

(1) Determine the eigenvalues of f. 
(2) For every eigenvalue . 
of f carry out thefollowing steps: 
(a) Determine the smallest number m . 
N with 
ker(( 
f..Id 0 .ker(( 
f..Id 1 . 
··· 
. 
ker(( 
f..Id m ker(( 
f..Id m+1 
Then dim(ker(( 
f ..Id m a(. 
f . 
(b) For s 1 m determine 
ds(. 
dim(ker(( 
f ..Id s .dim(ker(( 
f ..Id s.1 0 

If s . 
m +1, then ds(. 
0, and 

d1(. 
dim(ker( 
f ..Id g(. 
f 

is the number of Jordan blocks corresponding to .. 

(c)Tosimplify notation,we write ds : 
ds(. 
and determine theJordan chains 
as follows: 
(i) Since dm .dm+1 dm,thereexist dm Jordan blocks of size m.For each 
of theseblocks we determineaJordan chainof dm principal vectors of 
level m, i.e., vectors 
t1 mt2 m tdm m . 
ker(( 
f ..Id m \ker(( 
f ..Id m.1 


246 16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

with thefollowing property: 

dm 

If .1 .dm . 
K with .iti m . 
ker(( 
f ..Id m.1 , then .1 
i 1 
··· 
.dm 0. Here the firstindexinti j indicates the number of the 

chain, and thesecond indicates thelevelof theprincipalvector (from 
ker(( 
f ..Id j and notker(( 
f ..Id j.1 ). 

(ii) For j mm .1 2weproceed as follows: 
When we have determined dj principal vectors oflevel j,say t1 jt2 j 
tdj j, we apply f ..Id to each of these vectors, hence 

ti j.1 : 
( 
f ..Id )(ti j 1.i .dj 

in orderto determinetheprincipalvectorsoflevel j.1. 

dj 

If .1 .dj . 
K with .iti j.1 .ker(( 
f ..Id j.2 ,then 
i 1 

.. 
.. 


dj dj 

.. 
..

0 ( 
f ..Id j.2 .iti j.1 ( 
f ..Id j.1 .iti j 

i 1 i 1 

dj 

and thus .iti j .ker(( 
f ..Id j.1 giving .1 ··· 
.dj 0. 
i 1 
If dj.1 dj,then thereexist dj.dj.1 Jordan blocks of size j.1.For 

thesewe need theJordan chainsof length j.1. Thus we extend the 

already computed 

t1 j.1 t2 j.1 tdj j.1 .ker(( 
f ..Id j.1 \ker(( 
f ..Id j.2 

to dj.1 principalvectorsoflevel ( 
j.1 (but onlyifdj.1 dj)via 

t1 j.1 t2 j.1 tdj.1 j.1 .ker(( 
f ..Id j.1 \ker(( 
f ..Id j.2 

dj.1 

wherethe following must hold:If .1 .dj.1 . 
K with .iti j.1 . 
i 1 
ker(( 
f ..Id j.2 ,then .1 ··· 
.dj.1 0. 
After completingthestepfor j 2,wehaveobtained (linearly independent) 
vectors t11 t21 td11 . 
ker( 
f ..Id .Since dim(ker( 
f ..Id d1, 
wehave founda basisofker( 
f ..Id .Inthisway wehave determined d1 
different Jordan chains that we combine as follows: 

T. 
: 
t11 t12 t1 mt21 t22 t2 . 
td11 td1 . 



16.3 Computationofthe Jordan CanonicalForm 247 
Each chain begins with an eigenvector, followed by principal vectors of 
increasinglevels. Here we usethe conventionthat the chains areordered 
decreasingly according to theirlength. 

(3)Jordan chains are linearly independent,if theirfirstvectors(theeigenvectors) 
are linearly independent.(Showthisasanexercise.)Thus,if .1 .. 
arethe 
pairwisedistinct eigenvaluesof f,then 
TT.1 T.. 


isa basis,for which ] 
f]TT is in Jordan canonical form. 

Example 16.22 We interpretthe matrix 

.. 


50 100 

..

01000 

.. 


.. 
55 

F .10 300 . 
R

.. 


..

00010 

00004 

as endomorphism on R51 . 

(1) The eigenvalues of F arethe rootsof PF (t . 
12(t . 
4 3.In particular PF 
decomposes into linearfactors and F hasaJordan canonical form. 
(2)Wenow considerthedifferenteigenvaluesof F: 
(a) For the eigenvalue .1 1we obtain 
.. 
.. 


40 100 

.. 
..

00000 

.. 
.. 


.. 
..

ker(F . 
I ker .10200 span{e2 e4}

.. 
.. 


.. 
..

00000 

00003 

Here dim(ker(F . 
I 2 a(1 F . 

Forthe eigenvalue.2 4we obtain 

.. 
.. 


10 100 

.. 
..

0 .3000 

.. 
.. 


.. 
..

ker(F . 
4I ker .10 .100 span{e1 . 
e3 e5}

.. 
.. 


.. 
..

000 .30 

00000 


248 
16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

.. 
.. 


00000 

.. 
..

09000 

.. 
..

2 
.. 
..

ker((F. 
4I ker 00000 span{e1 e3 e5}

.. 
.. 


.. 
..

00090 

00000 

Here dim(ker((F. 
4I 23 a(4 F . 

(b) For .1 1wehave d1(1 dim(ker(F. 
I 2. 
For .2 4wehave d1(4 dim(ker(F . 
4I 2 and d2(4 
dim(ker((F. 
4I 2 . 
dim(ker(F. 
4I 3. 
2 1. 
(c)Computationofthe Jordan chains: 
• 
For .1 1wehave m 1.As principalvectorsoflevel onewe choose 
t11 e2 and t21 e4.Theseforma basisofker(F. 
I :If .1 .2 . 
R 
with .1e2 + 
.2e4 0, then.1 .2 0.For .1 1we are finished. 
• 
For .2 4wehave m 2, and we choose a principal vector of level 
two, say t12 e1. For this vector we have: If .1 . 
R with .1e1 . 
span{e1 . 
e3 e5},then .1 0.We compute 
t11 : 
(F. 
4It12 e1 . 
e3 

Since d1(4 21 d2(4,wehaveto add to t11 another principal 
vector of levelone, and we chooset21 e5.Since thevectorsare linearly 

0

independent, .1t11 + 
.2t21 . 
ker((F. 
4I ={0} 
impliesthat .1 
.2 0. 

In this way we get 

T.1 
. 
. 
. 
. 
. 
. 
00 
10 
00 
01 
00 
. 
. 
. 
. 
. 
. 
and T.2 
. 
. 
. 
. 
. 
. 
11 0 
000 
.10 0 
000 
00 1 
. 
. 
. 
. 
. 
. 
(3) The coordinate transformation matrix is T =[T.1 T.2],and the Jordan canonical 
form of Fis 

. 
1
1 
41
4
4 


. 
.. 


01 000 

. 
. 
..


00 010 

. 
. 
.. 


. 
. 
T.1 
..

T.1FT where 00 .100 

. 
. 
.. 


. 
. 
..


10 100 
00 001 


16.3 Computationofthe Jordan CanonicalForm 
249 
Exercises 

(In thefollowingexercises K is an arbitrary field.) 

16.1. Prove Lemma 16.1 (1). 
16.2. Prove Lemma 16.6 (1). 
16.3. Let V be a K-vector space, f .L(VV and . 
. 
K.Prove ordisprove:A 
subspace U .V is f-invariant,ifitis ( 
f ..IdV -invariant. 
16.4. Let 
V be a finite dimensional K-vector space, f . 
L(VV , . 
V and 
. 
. 
K. Show that Kj( 
f Kj( 
f ..IdV for all j . 
N.Conclude 
thatthegradeof with respectto f isequaltothegradeof with respectto 
f ..IdV. 

16.5. Prove Lemma 16.14. 
16.6. Let V be a finite dimensional Euclidean or unitary vector space and let f . 
L(VV be selfadjoint and nilpotent. Showthat then f 0. 
16.7. Let 
V ={0}be a finite dimensional K-vector space, let f . 
L(VV be 
nilpotent of index m and suppose that Pf decomposes into linear factors. 
Showthe following assertions: 
(a) Pf tn with n dim(V . 
(b) Mf tm . 
(c) There exists a vector .V of grade m with respect f. 
(d) For every . 
.K we have Mf..IdV 
(t +. 
m . 
16.8. Let 
V be a finite dimensional K-vector space and f . 
L(VV . Show the 
following assertions: 
(a) ker( 
fj . 
ker( 
fj+1 for all j . 
0 and there exists an m . 
0 with 
m 
m

ker( 
fker( 
fm+1 .For this m we haveker( 
fker( 
fm+j forall 
j.1. 

(b) im( 
fj . 
im( 
fj+1 for all j . 
0 and there exists an . 
. 
0 with 
im( 
fim( 
f+1 . For this . 
we have im( 
fim( 
f+j for all 
j.1. 

mm+1 

(c) If m . 
. 
0 are minimal with ker( 
fker( 
fand im( 
fim( 
f+1 ,then m . 
mm

(Theorem 16.5 nowimpliesthat V ker( 
f.im( 
fis a decomposition 
of V into f-invariant subspaces.) 

16.9. Let 
V be a finite dimensional K-vector space and let f . 
L(VV be a 
projection(cp.Exercise 13.10). Showthe following assertions: 
(a) 
.im( 
f impliesthat f( 
. 
(b) V im( 
f .ker( 
f . 
(c) There exists a basis B of V with 
[f]BB 
Ik 

0n.k 

ktn.k

where k dim(im( 
f and n dim(V .Inparticular, Pf (t.1 
and . 
.{01}for every eigenvalue . 
of f. 


250 
16 Cyclic Subspaces,Duality andthe Jordan CanonicalForm 

(d) The map g IdV .f isaprojectionwithker(g im( 
f and im(g 
ker( 
f . 
16.10. 
Let V be a finite dimensional K-vector space and let UW . 
V be two 
subspaces with VU .W. Show that there exists a uniquely determined 
projection f .L(VV with im( 
f U andker( 
f W. 
16.11. 
Determinethe Jordan canonical formof thematrices 
.. 


.. 


210 00 

1.10 0 

..

.111 00 

... 
.

1.10 0 

.. 
44 
.. 
55 

A 
.RB .10300 .R

.. 


..

303 .3 

..

.1.10 11 

4.13 .3 

.2.11 .13 

usingthe method presentedin Sect. 16.3.Determine alsotheminimal polynomial. 


16.12. 
Determine the Jordan canonical form and the minimal polynomial of the 
linear map 
f :C.3[t]>C.3[t] 
.0 +.1t+.2t2 +.3t3 
> 
.1 +.2t+.3t3 

16.13. 
Determine(uptotheorderof blocks)all matrices Jin Jordan canonical form 
2

with PJ (t+13(t.1 3 and MJ (t+12(t.1. 

16.14. 
Let V ={0}be a finite dimensional K-vector space, f .L(VV , and supposethat 
Pf decomposes into linearfactors. Showthe following assertions: 
(a) 
Pf Mf holdsif and onlyif g(. 
f 1for all eigenvalues. 
of f. 
(b) 
f is diagonalizableif and onlyif Mf has only simple roots, i.e., roots 
with multiplicity one. 
(c) A root of . 
. 
K of Mf is simple if and only if ker( 
f ..IdV 
ker(( 
f ..IdV 
2. 
16.15. 
Let V bea K-vector spaceof dimension2 or3and let f .L(VV with Pf 
decomposinginto linearfactors. Showthat theJordan canonical formof f 
is uniquely determined by Pf and Mf.Whydoes this not hold anylonger if 
dim(V .4? 
16.16. 
Let A.Kn n beamatrixfor whichthe characteristic polynomial decomposes 
into linearfactors. Showthat thereexistsa diagonalizable matrix Dand a 
nilpotent matrix Nwith AD+Nand DN ND. 
16.17. 
Let A.Kn n beamatrixthat hasaJordan canonical form.We define 
.. 
.. 
. 


1 

.. 


. 
1. 
.Kn n 

InR : 
.in+1.j .. 
JnR(. 
: 
.. 


..

1 

. 
1 


16.3 
Computationofthe Jordan CanonicalForm 251 
Showthe following assertions: 
(a) IRJn(. 
IR Jn(. 
T .
n 
n 

(b) Aand AT are similar. 
(c) Jn(. 
IRJR(. 
.
nn 

(d) Acan be written as a product of two symmetric matrices. 
16.18. 
Determinefor thematrix 
.. 


511 

33 

..

A 
051 . 
R

004 
two symmetric matrices S1 S2 . 
R3 3 with AS1S2. 



Chapter17 
Matrix Functions and Systems 
of Differential Equations 

Inthis chapterwegivean introductiontothe areaof matrix functions.Wefirst define 
general matrix functions and derivetheirmostimportant properties. Usingtheexamplesof 
network analysis and chemical reactions,we illustratehowmatrixfunctions 
arise naturallyin applications.The network analysisexampleinvolvestheexponentialfunctionof 
matrices, and we study thepropertiesof thisimportant functionin 
detail.The analysisof chemical reactionkineticsleadstoasystemofordinarydifferential 
equations,whosesolutionagainis based on thematrixexponentialfunction. 

17.1 
Matrix Functionsand theMatrixExponential 
Function 
In thefollowing we will study functions that yieldforagiven n . 
n matrix again an 
n . 
n matrix. A possible definition of such a function is given by the entrywise 
application of scalar functions to the matrix. For instance, one could define for 

A . 
aij . 
Cnn thefunction sin(A by sin( 
A :. 
sin(aij . However, such a 
definitionisnot compatiblewiththematrixmultiplication, sincein general already 

A22


. 
a 
.

ij 

The following definition of the primary matrix function from [Hig08, Definition 
1.1–1.2] will turn out to be consistent with the matrix multiplication. Since 
this definitionis based on theJordan canonical form,weassume forsimplicity that 
A . 
Cnn.Our considerations also applytosquarematricesover R,aslong as they 
haveaJordan canonical form. 

Definition 17.1 Let A. 
Cnn have theJordan canonical form 

J . 
diag(Jd1(.1 Jdm (.m . 
S.1AS 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_17 

254 17 Matrix Functions andSystemsof DifferentialEquations 

and let . 
. 
C be such that {.1 .m}. 
.Afunction f . 
. 
. 
C is said to be 
defined on the spectrum of A,ifthe values 

f( 
j 

(.i for i . 
1 m and j. 
01 di . 
1 (17.1) 

exist. Here f( 
j (.i , j . 
1 di . 
1, is the jth derivative of the function f(. 
with respect to . 
evaluated at .i.If .i . 
R, then this is the real derivative, and for 
.i . 
C^ 
Rit is the complex derivative. Moreover, we assume that equal eigenvalues 
that occurindifferent Jordan blocks aremappedtothesamevaluesin(17.1). 

If f is defined on the spectrum of A then the primarymatrixfunction f(A is 
defined by 

f(A :. 
Sf (JS.1 where f(J :. 
diag( 
f(Jd1(.1 f(Jdm (.m (17.2) 

and 
. 
f(.i f(.i 
f(.i 
2. 
f(di.1 (.i 
(di.1 . 
. 
. 
. 
. 
. 
f(.i f(.i . 
. 
. 
. 
. 
. 
f Jdi (.i :. 
. 
f. 
(.i . 
for i . 
1 m. (17.3) 
. 
2. 
. 
. 
. 
. 
. 
f(.i . 
. 
f(.i 

Note that for the definition of f(A in(17.2)–(17.3)onlytheexistence of the 
values in(17.1)is required. 

Example 17.2 Let A . 
I2 . 
C2 2 and let f(z =v 
z (the square root function). 

vv 
If we set f(1 . 
1 =+1, then f(A . 
A . 
I2 by Definition 17.1.Ifwe 

v 
choose the other branch of the square root function, i.e., f(1 . 
1 =.1, then 

v 
f(A . 
A =.I2.The matrices I2 and .I2 are primary square roots of A . 
I2. 
Takingdifferent branchesofafunctionfordifferent Jordan blocks correspondingto 
thesameeigenvalueisincompatible with Definition 17.1.For instance, thematrices 

10 .10

X1 . 
and X2 =

0 .1 01 

areincompatible with Definition 17.1, despite thefact that X12 . 
I2 and X22 . 
I2. 

All solutions X . 
Cnn of the matrix equation X2 . 
Aare called square roots of 
thematrix A. 
Cnn.ButasExample 17.2shows, someofthesemaynotbeprimary 
squareroots accordingto Definition 17.1.Inthe following,by f(A we will always 
meanaprimary matrix function accordingto Definition 17.1,andwill usuallyomit 
theterm “primary”. 

In(16.8)wehaveshown that for each polynomial p . 
C[t. 
of degree k . 
0we 
have 


17.1 MatrixFunctions andthe Matrix ExponentialFunction 255 
k 

p( 
j (.i p(Jdi (.i . 
(Jdi (0 j (17.4)

j. 


j=0 

Asimple comparison showsthat this formula agrees with(17.3)for f . 
p.This 
means that the computation of p(Jdi(.i with(17.4)leadsto thesameresultasthe 
definition of p(Jdi(.i by(17.3). More generally,the following result holds. 

Lemma 17.3 Let A . 
Cnn andp . 
.ktk +. 
.1t . 
.0 . 
C[t]. Then(17.2)– 
(17.3)with f . 
pyieldsa matrix functionf( 
A that satisfies f( 
A . 
.kAk +. 
.1A. 
.0In. 

Proof Exercise. 
. 


If we consider,in particular,the polynomial f . 
t2 in(17.2)–(17.3), then the 
resulting f(A is equal to theproduct A. 
A.This shows that the definition of the 
primary matrixfunction f(A is consistent with the matrix multiplication. 

Thefollowing theorem,whichisof great practical and theoretical importance, 
showsthat thematrix f( 
A canalwaysbe writtenasa polynomialin A. 

Theorem 17.4 Let A . 
Cnn have theminimal polynomialMA, and letf (A be as 
in Definition 17.1. Then there exists a uniquely determined polynomial p . 
C[t. 
of 
degree at most deg(MA . 
1with f (A . 
p(A .In particular,Af (A . 
f( 
A A, 

f(AT . 
f(AT as well as f(VAV.1 . 
Vf (AV.1 for allV . 
GLn(C . 

Proof We will not present the proof here since it requires advanced results from 
interpolationtheory. Details canbe foundin [Hig08, Chap.1]. 
. 


Using Theorem 17.4 we can show that the primary matrix function f(A in 
Definition17.1is independentofthe choiceoftheJordan canonicalformof A.We 
already know fromTheorem 16.12,that theJordan canonical formof A is unique 
uptotheorderoftheJordan blocks.If 

J . 
diag(Jd1(.1 Jdm (.m . 
S.1AS 


S.1A


J
. 
diag(J

(.1 J

(.m . 

S

d1dm 

are two Jordan canonical forms of A, then J
. 
PT JP for a permutation matrix 
P . 
Rnn, where the matrices J and J are the same up to the order of diagonal 

blocks.Hence 

f(J . 
diag( 
f(Jd1(.1 f(Jdm (.m 

. 
P PTdiag( 
f(Jd1(.1 f(Jdm (.m PPT 

. 
P diag( 
f(Jd
1(.1 f(Jd
m (.m PT 

. 
Pf (J PT 


256 17 Matrix Functions andSystemsof DifferentialEquations 

Theorem 17.4 applied to the matrix Jyields the existence of a polynomial pwith 
f(J . 
p(J .Thus, we get 

f(A . 
Sf(JS.1 . 
Sp(JS.1 . 
p(A . 
p(SJ
S.1 . 

SPTp(JP
S.1 . 

SPT f(JP
S.1 
. 

Sf(J
S.1 

Letusnow consider theexponentialfunction f(z . 
ez that is infinitely often 
complex differentiable throughout C. In particular, ez is defined (in the sense of 
Definition 17.1)onthe spectrumof everygivenmatrix 

S.1 . 
Cnn 

A. 
Sdiag(Jd1(.1 Jdm(.m 

If t. 
C is arbitrary(but fixed),then the derivatives of thefunction etz with respect 
to the variable zaregiven by 

dj 

tz . 
tjtz

eej. 
012 

dzj 
Wewill usethe notationexp(M insteadof eMfortheexponentialfunctionofamatrix 

M.ForeveryJordan block Jd(. 
of Awe thenhave,by(17.3)with f(z . 
ez , 
. 
1 t t2 
2. 
td.1 
(d.1 . 
. 
. 
. 
exp(tJd(. 
. 
et. 
. 
. 
. 
. 
. 
. 
1 t 
t2 
2. 
. 
. 
. 
. 
. 
. 
. 
et. 
d.1 
. 
k=0 
1 
k. 
(tJd(0 k (17.5) 
. 
t . 
1 

and thematrixexponentialfunctionexp(tA is givenby 

S.1

exp(tA . 
Sdiag(exp(tJd1(.1 exp(tJdm(.m (17.6) 

The parameter twillbeusedinthenext sectioninthe context oflineardifferential 
equations. 

In Analysisitis shownthat forevery z. 
Cthefunction ez can be represented by 
the absolutely convergent series 

. 


. 
j

z

z =

e

j. 


j=0 

Usingthisseries and the equation (Jd(0 . 
. 
0for all. 
. 
d, we obtain 


17.1 MatrixFunctions andthe Matrix ExponentialFunction 257 
.. 




d.1 .. 


. 
(t. 
j 
. 
1

t. 
1 

..

exp(tJd(. 
. 
e(tJd(0 . 
=. 
(tJd(0 . 


. 
j. 
. 


=0 j=0 =0 



. 
j 
. 
(t. 
j.. 
1 

=. 
(tJd(0 . 


( 
j. 
. 
. 
. 


j=0 =0 



. 
j 
. 
. 
tj 
. 


jj.

. 
. 
(Jd(0 . 


j. 
. 
i 

j=0 =0 

. 
tj 
. 
(.Id . 
Jd(0 j 

j. 


j=0 

. 


1 

j

. 
(tJd(. 
(17.7)

j. 


j=0 

In this derivationwehaveusedthe absolute convergenceof theexponentialseries 
and the finitenessofthe series with thematrix Jd(0 .Thisallows the applicationof 
the Cauchy product formula1 for absolutely convergent series, which is also proven 
in Analysis. 

Lemma 17.5 If A . 
Cn n,t . 
C and exp(tA is thematrixexponentialfunctionin 
(17.5)–(17.6), then 

. 


exp(tA . 
1 
(tAj 

j. 


j=0 

Proof In(17.7)wehaveshown this already forJordan blocks.The assertionthen 
follows from 
.. 


..

1 
(tSJS.1 j . 
S. 
1 
(tJj 
. 
S.1 

j. 
j. 


j=0 j=0 

and therepresentation(17.6)ofthe matrixexponentialfunction. 
. 


We immediatelysee fromLemma17.5 that fora matrix A. 
Rnn and every real 
t the matrix exponentialfunction exp(tA is a real matrix. 

Thefollowing result presentsfurther important propertiesofthe matrixexponentialfunction. 


Cnn 

Lemma 17.6 If the two matrices A B . 
commute, then exp(A . 
B . 


Cnn 

exp(A exp(B . For every matrix A . 
we have exp(A . 
GLn(C with 
(exp(A .1 . 
exp(.A. 

1AugustinLouisCauchy(1789–1857). 


258 17 Matrix Functions andSystemsof DifferentialEquations 

Proof If Aand B commute,then theCauchyproduct formulayields 

.. 


. 


.. 
. 
j

11 11 

exp(A exp(B =. 
Aj 
. 
B. 
. 
A. 
Bj.

j. 
. 
. 
( 
j.. 
. 


j=0 =0 j=0 =0 



. 
j 
. 
.

1 j 1 

. 
A. 
Bj.. 
(A+Bj 

j. 
. 
j!

j=0 =0 j=0 

=exp(A+B 

Herewehaveusedthebinomialformulafor commutingmatrices(cp.Exercise 4.10). 
Since Aand .Acommute, wehave 

. 


1 

exp(A exp(.A =exp( 
A.A =exp(0 . 
0j =In

j. 


j=0 

and hence exp(A .GLn(C with (exp(A .1 =exp(.A . 


Fornon-commutingmatricesthestatementsinLemma17.6ingeneraldonothold 
(cp. Exercise 17.9). 

MATLAB-Minute. 

Computethe matrixexponentialfunctionexp(A forthe matrix 

.. 


1 .1345 

..

.1.2435 

.. 


.. 
55 

A. 
20 .315 . 
R

.. 


..

300 .2 .3 

400 .3.5 

usingthe command E1=expm(A).(Look at help expm.) 

Also compute the diagonalization of A using the command [S,D]=eig(A), 

and form thematrixexponentialfunctionexp(A as E2=S.expm(D)/S. 

Compare the matrices E1 and E2 and compute the relative error norm(E1


E2)/norm(E2).(Look at help norm.) 

Example 17.7 Let A =[aij].Cnn be a symmetric matrix with aii =0andaij . 
{01}forall ij =1 n.Weidentifythe matrix Awitha graphGA =(VA EA 
consisting of a set of n verticesVA ={1 n}and a set of edgesEA .VA .VA. 
For i =1 n the row i of Ais identified with the vertex i .EA, andevery entry 
aij =1isidentified with an edge(ij .EA.Due to the symmetry of A,wehave 
aij =1ifand onlyifaji =1.We therefore considerinthefollowingtheelements 


17.1 MatrixFunctions andthe Matrix ExponentialFunction 259 
of EA as unordered pairs, i.e., (ij =( 
ji .The followingexample illustratesthis 
identification: 
.. 


01110 

..

10011 

.. 


..

A. 
10001 

.. 


..

11000 
01100 
is identified with GA =(VA EA , where 
EA ={12345} 
VA ={(12 (13 (14 (24 (25 (35 } 
and thegraph GA can be displayed as follows: 


A path of length m from the vertex k1 to the vertex km+1 is an ordered list of 
vertices k1 k2 km+1, where (ki ki+1 . 
VA for i . 
1 m.If k1 . 
km+1, 
then thisisaclosed pathoflength m.Inthe aboveexample, paths from1to4 are 
givenby124 and125312 4; thesehavethe lengths2 and6, respectively. 
Inthemathematical fieldofGraph Theory one usually assumesthattheverticesin 
apathare pairwisedistinct.Ourdeviationfromthis conventionismotivatedbythe 
following interpretation of a matrix Aand its powers: 

An entry aij =1inthe matrix Ameans that thereexistsa pathoflength1from 
vertex i to vertex j, i.e., the vertices i and jare adjacent.If aij =0, then no such 
path exists. The matrix Ais therefore calledthe adjacency matrix of thegraph GA. 
If we squarethe adjacency matrix, then the entryin the (ij positionisgiven by 

n 

(A2 

ij . 
aiaj 
=1 

In thesum on theright hand side, we obtainforagiven . 
a1if and onlyif(i . 
.EA 
and (. 
j .EA.The sumonthe righthad side thereforeis equaltothe numberof 
vertices that are adjacent to both i and j.Hence the (ij entry of A2 is equal to the 
numberof pairwisedistinct paths from i to j(i 
. 
j), or the pairwisedistinct closed 
paths from i to i of length2in GA.More generally, one can show thefollowing (cp. 
Exercise 17.10): 

LetA =[aij].Cn n beasymmetric adjacency matrix, i.e.,A =AT withaii =0 
and aij .{01}foralli j =1 n, and letG A be thegraph identified withA. 
Then for eachm .Nthe (ij entryofAmisequaltothe numberof pairwisedistinct 
paths fromi toj (i 
. 
j)orthe pairwisedistinct closed pathsfromi toiof length 
minGA. 


260 17 Matrix Functions andSystemsof DifferentialEquations 

Forthe above matrix Awe obtain 

.. 
.. 


31012 26541 

.. 
..

13210 62145 

.. 
.. 


A2 . 
.. 
A3 . 
..

02210 and 51024 

.. 
.. 


.. 
..

11121 44222 

20012 15420 

The3pairwisedistinct closed pathsof length2from1to1 are 

121 131 141 

and the4pairwisedistinct pathsof length3from1to4 are 

1214 1314 1414 1424 

Numerous realworld applicationsinvolve networksthat canbe modeled mathematically 
usinggraphs.Examples include social,biological,telecommunication or 
airline networks.Thepropertiesofsuch networksare studiedintheinterdisciplinary 
area of Network Science.Animportant taskisto identify participantsinthe network 
that are centralinthe sensethat theirfunctionality hasasignificant impact on the 
entire network.Ifthe network has been modeledbyagraph, then we can study the 
centrality ofthevertices.Forexample,avertex canbe considered centralifitis connectedtoalargepartofthegraphviamany 
shortclosedpaths.Longer connections 
areusuallyless important,andthuspathsshouldbescaleddown accordingtotheir 
length.If we usethe scalingfactor1 m. 
fora pathoflength m,then forthevertex i 
in thegraph GA with the adjacency matrix Awe obtaina centrality measureofthe 
form 
. 


11 1 

A. 
A2 . 
A3 . 


1. 
2. 
3. 


ii 

Therelativeorderingofthevertices accordingto this formulais not changed when 
we add the constant1.We then obtainthe centrality of the vertex i as 

. 


11 

I . 
A. 
A2 . 
A3 +. 
(exp( 
A ii

23. 


ii 

Another important quantity is the so-called communicability between the vertices i 
and jfor i 
. 
j,whichisgivenbythe weighted sumofthe pairwisedistinct paths 
from i to j, i.e., by 

. 


I . 
A. 
1 
A2 . 
1 
A3 +. 
(exp( 
A ij

23. 


ij 

Forthe above matrix AtheMATLABfunction expm yields 


17.1 MatrixFunctions andthe Matrix ExponentialFunction 261 
.. 


3763031953225002792718176 

..

3195337630181762792722500 

.. 


..

exp(A . 
2250018176248811274919204 

.. 


..

2792727927127492890712749 

1817622500192041274924881 

Thevertices1and2havethe largest centrality,followedby4,3and5.If wewould 
definethe centralityofavertexasthenumberofadjacentvertices,theninthisexample 
wecouldnot distinguish betweenthevertices34and5.Thelargest communicability 
in thisexampleexists between thevertices1and2. 

Further information concerningthe analysisof networksusing adjacencymatrices 
and matrix functions can be found in the article [EstH10]. 

17.2 SystemsofLinearOrdinaryDifferentialEquations 
Adifferential equation describes a relationship between a desired function and its 
derivatives. Such equations are used in all areas of science and engineering for 
modelingphysical phenomena. Ordinarydifferential equationsinvolveafunctionof 
onevariableanditsderivatives,while partialdifferential equationsinvolvefunctions 
of severalvariables and their partial derivatives.Inthissectionwefocus on ordinary 
differential equationsof first order, i.e., thoseinwhich onlythe function and its first 
derivative occur. 

Asimpleexamplefor themodelingwith ordinarydifferential equationsof first 
orderistheincreaseordecreaseofabiological population,suchas bacteriainapetri 
dish.Let y . 
y(t be the size of the population at time t.If thereis enough food 
andif theexternal conditions (e.g.temperature orpressure)are constant,then the 
populationgrows witha (real)rate k > 
0, that is proportional to the current number 
of individuals. This can be described by the equation 

d 

y.:. 
y . 
ky (17.8)

dt 

Clearly, one can also take k < 
0, and then the population shrinks. 

We are then looking for a function y . 
D . 
R . 
R that satisfies(17.8). The 
general solutionof(17.8)isgivenby theexponentialfunction 

y . 
cetk 

where c . 
R is an arbitrary constant.Fora unique solution of(17.8)we need to 
knowthe sizeof the populationat agiveninitial time t0.Inthisway we obtainthe 
initial value problem 

y.. 
ky y(t0 . 
y0 


262 17 Matrix Functions andSystemsof DifferentialEquations 

which, as we will show below,issolveduniquelybythe function 

(t.t0 k

y =ey0 

Example 17.8 In a chemical reaction certain initial substances (called educts or 
reactants) aretransformed into other substances (calledproducts).Reactions canbe 
distinguished concerning theirorder.Herewe onlydiscussreactionsof first order, 
wherethe reactionrateis determinedbyonly one educt.Inreactionsof secondand 
higher order one typically obtains nonlinear differential equations,which arebeyond 
our focus in this chapter. 

If, for example, the educt A1 is transformed into the product A2 with the rate 
.k1 < 
0, then we write this reaction symbolically as 

k1

A1 A2 

and we modelit mathematicallyby theordinarydifferential equation 

y.1 =.k1y1 

Here the value y1(t is the concentration of the substance A1 at time t.For the 
concentration of the product A2, which grows with the rate k1 > 
0, wehave the 
corresponding equation y.2 =k1y1. 

It may happen that a reaction of first order develops in both directions. If A1 
transformsinto A2 with the rate .k1, and A2 transformsinto A1 with the rate .k2, 
i.e., 

k1 

A1 . 
A2 

k2 

then we can model this reaction mathematically by the system of linear ordinary 
differential equations 

y.1 =.k1y1 +k2y2 

y.2 =k1y1 .k2y2 

T

Combining the functions y1 and y2 in a vector valued function y =[y1 y2],we 
can write this system as 

.k1 k2 

y.=Ay where A. 


k1 .k2 


17.2 Systemsof Linear Ordinary DifferentialEquations 263 
The derivativeofthe function y(t is always considered entrywise, 

y.1 

y.. 


y.2 

Reactions can alsohave severalsteps.Forexample,areactionofthe form 

k2

k1 . 
k4

A1 A2 . 
A3 A4 

k3 

leads to thedifferential equations 

y.1 =.k1y1 

y.2 =k1y1 .k2y2 +k3y3 

y.3 =k2y2 .(k3 +k4 y3 

y.4 =k4y3 

and thus to the system 

.. 


.k10 00 

..

k1 .k2 k30 

..

y.=Ay where A. 


..

0 k2 .(k3 +k40 

00 k40 

Thesumofthe entriesineach columnof Ais equal to zero, since forevery decrease 
in a substance with a certain rate other substances increase with the same rate. 

In summary,a chemical reactionoffirst order leadstoasystemoflinear ordinary 
differential equationsof first order that canbe writtenas y.=Ay witha (real)square 
matrix A. 

Wenowderivethe general theoryfor systemsoflinear (real or complex)ordinary 
differential equationsof first orderof theform 

y.=Ay +gt .[0 a. 
(17.9) 

Here A.Kn n isagiven matrix, a isagiven positivereal number, g :[0 a]>Kn 1 
isagiven function, y :[0 a]. 
Kn 1 is the desired solution, and we assume that 
K =R or K =C.If g(t =0 . 
Kn 1 for all t .[0 a],then thesystem(17.9)is 
called homogeneous,otherwise it is called non-homogeneous.Foragiven systemof 
theform(17.9), thesystem 

y.=Ay t .[0 a. 
(17.10) 

is calledthe associated homogeneous system. 


264 17 Matrix Functions andSystemsof DifferentialEquations 

Lemma 17.9 The solutionsof the homogeneous system(17.10)forma subspaceof 
the(infinite dimensional)K-vector spaceofthe continuouslydifferentiablefunctions 
from theinterval [0 a]toK n 1 . 

Proof Wewillshowtherequiredproperties accordingto Lemma9.5.The function 
w 
=0is continuously differentiable on[0 a]and solves the homogeneous system 
(17.10). Thus,the solutionsetof this systemis not empty.If 

w1 ,w2 :[0 a]>Kn 1 

are continuouslydifferentiablesolutionsandif .1 .2 .K,then w 
=.1w1+.2w2 
is continuously differentiable on [0 a], and 

w.=.1w.1 +.2w.2 =.1Aw1 +.2Aw2 =Aw 


i.e., thefunction w 
isasolutionofthe homogeneous system. 
. 


Thefollowing characterizationofthe solutionsofthe non-homogeneous system 
(17.9)is analogoustothe characterizationofthe solutionsetofa non-homogeneous 
linear systemof equationsin Lemma 6.2(alsocp.(8)inLemma 10.7). 

Lemma 17.10 If w1 :[0 a]>Kn 1 isasolutionofthe non-homogeneous system 
(17.9), then every other solution y can be written as y =w1 +w2, where w2 is a 
solutionofthe associated homogeneous system(17.10). 

Proof If w1 and yaresolutionsof(17.9), then y...w1 =( 
Ay +g .(Aw1 . 
g . 
A(y.w1 .The difference w2 :=y.w1 thusisasolutionofthe associated homogeneous 
system and y =w1 +w2. 
. 


In order to describe the solutions of systems of ordinary differential equations ,we 
consider for a given matrix A.Kn n the matrix exponential function exp(tA from 
Lemma17.5or(17.5)–(17.6),where we now consider t .[0 a]as real variable.The 
power series of the matrix exponential function in Lemma 17.5 converges,and it can 
be differentiated termwise with respect to the variable t, where again the derivative 
of a matrix with respect to the variable t is considered entrywise. This yields 


The same result is obtained by the entrywise differentiation of the matrix exp(tA in 
(17.5)–(17.6)with respect to t.With 


17.2 Systems of Linear Ordinary Differential Equations 265 
. 
which also gives dt exp(tA . 
Aexp(tA . 

Theorem 17.11 

(1) The unique solution of the homogeneous differential equation system(17.10) 
for a given initial condition y(0 . 
y0 . 
Kn 1 is given by the function y . 
exp(tAy0. 
(2) The setofall solutionsof the homogeneous differential equationsystem(17.10) 
forms an n-dimensional K-vector space with the basis {exp(tAe1 
exp(tA en}. 
Proof 

y0.Hence yisasolutionof(17.10)that satisfies 
theinitial condition.If w 
is another such solution and u :. 
exp(.tA )w,then 


whichshows that thefunction u has constant entries.In particular,wethenhave 

266 17 Matrix Functions and Systems of Differential Equations 

(2)Each of the functions exp(tA ej exp(tA en :[0 a]>Kn 1, j=1 n, 
. 
Kn n is
solves the homogeneous system y.. 
Ay.Since the matrix exp(tA 
invertible for every t .[0 a](cp. Lemma 17.6), these functions are linearly 
independent. 

If 

y is an arbitrary solution of y.. 
Ay,then 
y(0 =y0 for some y0 .Kn 1.By 

(1)then 

y is the unique solution of the initial value problem with y(0 =y0,so 
that 

y =exp(tAy0.As a consequence,
y is a linear combination of the functions 
exp(tAe1 exp(tA en. 

To describe the solution of the non-homogeneous system(17.9), we need the 
integral of functions of the form 

.. 


w1 

.. 


w 
=
.. 
:[0 a]>Kn 1 

wn 

For every fixed t .[0 a]we define 

.. 
t . 


. 
0 w1(s ds 

t 

.. 


w(s ds :=
.. 
. 
Kn 1 
0 
. 
t 

0 wn(s ds 

i.e., we apply the integral entrywise to the function w.By this definition we have 

. 




dt 

w(s ds =w(t 

dt 

0 

for all t .[0 a].We can now determine an explicit solution formula for systems of 
linear differential equations based on the so-called Duhamel integral.2 

Theorem 17.12 The unique solutionofthe non-homogeneous differential equation 
system(17.9)with theinitial conditiony(0 =y0 .Kn 1 is given by 

. 


t 

y =exp(tAy0 +exp(tA exp(.sAg(s ds (17.11) 

0 

Proof The derivativeofthe function y defined in(17.11)is 

. 




dd t 

y.. 
(exp(tAy0 . 
exp(tA exp(.sAg(s ds 

dt dt 

0 

. 


t 

=Aexp(tAy0 +Aexp(tA exp(.sAg(s ds +exp(tA exp(.tAg 

0 

2Jean-Marie Constant Duhamel(1797–1872). 


17.2 Systemsof Linear Ordinary DifferentialEquations 267 
. 


t 

=Aexp(tAy0 +Aexp(tA exp(.sAg(s ds +g 

0 

=Ay +g 

Furthermore, wehave 

. 


0 

y(0 =exp(0 y0 +exp(0 exp(.sAg(s ds =y0 

0 

so that y also satisfies theinitial condition. 

Let now 

y be another solution of(17.9)that satisfies theinitial condition.By 
Lemma 17.10 we then have 

y . 
y +w, where w 
solves the homogeneous system 
(17.10). Therefore, w 
=exp(tAc for some c . 
Kn 1 (cp. (2)inTheorem 17.11). 
For t =0we obtain y0 =y0 +c, where c =0and hence

y =y. 


In the above theoremswehaveshown that fortheexplicit solutionof systemsof 
linear ordinarydifferential equationsof first order,wehaveto computethe matrix 
exponentialfunction.Whilewehaveintroducedthis functionusingtheJordan canonicalformofthegivenmatrix, 
numerical computationsbasedontheJordan canonical 
form are not advisable(cp. Example 16.20). Becauseofits significant practical relevance, 
numerous different algorithms for computingthe matrixexponentialfunction 
have been proposed. But, as showninthe article [MolV03],noexistingalgorithmis 
completely satisfactory. 

Example 17.13 Theexamplefromcircuitsimulationpresentedin Sect. 1.5 leadto 
thesystemof ordinarydifferential equations 

dR 11 

I =. 
I . 
VC . 
VS

dt LLL 
d 1 

VC =. 
I 

dt C 

Using(17.11)and theinitial values I(0 . 
I0 and VC(0 . 
VC0, we obtain the 
solution 

. 
. 
. 
. 


I .RL .1 LI0 

=exp t

VC .1 C 0 VC 
0 

. 
. 
. 
. 


. 


t .RL .1 L VS(s

. 
exp (t .s ds

.1 C 00

0 

Example 17.14 Letusalso consider anexample fromMechanics.A weight with 
mass m > 
0isattached toaspringwith thespring constant. 
> 
0.Let x0 > 
0bethe 
distanceoftheweightfromits equilibrium position,as illustratedinthe following 
figure: 


268 17 Matrix Functions andSystemsof DifferentialEquations 


Wewantto determinethe positionx(t oftheweightat time t .0, wherex(0 . 
x0. The extension of the spring is described by Hooke’slaw.3 The corresponding 
ordinarydifferential equationof second orderis 

d2 . 


x.. 
x =. 
x 

dt2m 

with initial conditions x(0 =x0 and x.(0 =v0, where v0 > 
0isthe initialvelocity 
of the weight. We can write this differential equation of second order for x as a 
system of first order by introducing the velocity v 
as new variable. The velocity is 
givenbythe derivativeofthe positionwith respectto time, i.e., v 
=.x, and thus for 
the accelerationwehave v.=.x, which yields the system 

.y =Ay where A. 
. 
01 
.. 
m 0 . 
and y . 
. 
. 
x 
v 


T

Theinitial conditionthenis y(0 =y0 =[x0 ,v0]. 

ByTheorem17.11,the unique solutionofthis homogeneous initialvalue problem 
isgivenbythe function y =exp(tAy0.We consider Aas an element of C22.The 
eigenvalues of Aarethe two complex (non-real) numbers .1 =i. 
and .2 =.i. 
. 


. 


.

.1, where . 
:. 
.Corresponding eigenvectors are 

m 

11

21 21 

s1 =.Cs2 =.C

i. 
.i. 


and thus 

it.

e0 11

S.1

exp(tAy0 =Sy0 S =.C22 

.it.

0 ei. 
.i. 


3SirRobert Hooke(1635–1703). 


17.2 Systemsof Linear Ordinary DifferentialEquations 
269 
Exercises 

3

17.1 Construct a matrix A=[aij].C2 2 with A3 
. 
a.
ij 

17.2Determine allsolutions X.C22 ofthematrix equation X2 =I2,and classify 
whichofthesesolutions areprimary squarerootsof I2. 
17.3 Determine a matrix X.C22 with real entries and X2 =.I2. 
17.4 Prove Lemma 17.3. 
17.5Prove thefollowing assertions for A.Cnn: 
(a) det(exp( 
A =exp(trace( 
A . 
(b) If AH =.A,then exp( 
A is unitary. 
(c) If A2 =I,then exp(A . 
1(e+1 I+1(e.1 A.
2 
e 2e 

17.6 Let A=Sdiag(Jd1(.1 Jdm(.mS.1 .Cnn with rank(A =n.Determine 
theprimary matrix function f(A for f(z =z.1.Does this function 
also existif rank( 
A < 
n? 
i.

17.7 Let log :{z=rei. 
|r > 
0 .. 
< 
. 
< 
.}>C, re>ln(r +i.,bethe 
principlebranchofthe complexlogarithm(whereln denotesthereal natural 
logarithm). Showthat this functionis defined on thespectrumof 
01 

22 

A=.C

.10 

and computelog(A as well as exp(log(A . 

17.8 Compute 
.. 
.. 
exp . 
01 
.10 . 
exp . 
.11 
.1.3 . 
sin 
.. 
. 
11 
0 . 
1 
00 . 
.. 


17.9 Construct two matrices AB.C22 with exp( 
A+B =exp(A exp(B. 
17.10 Prove theassertiononthe entriesof Ad in Example 17.7. 
17.11 Let 
.. 


511 

33 

..

A. 
051 .R

004 

Computeexp(tA for t.Randsolvethe homogeneoussystemofdifferential 

T

equations y.=Aywith theinitial condition y(0 =[111]. 

17.12 
Compute the matrix exp(tA from Example 17.14 explicitly and thus show 
that exp(tA . 
R2 2 (for t . 
R), despite the fact that the eigenvalues and 
eigenvectors of Aare not real. 

Chapter18 
SpecialClasses ofEndomorphisms 

In this chapter we discuss some classes of endomorphisms(or square matrices) 
whoseeigenvalues and eigenvectorshavespecial properties. Such properties only 
exist under further assumptions, and in this chapter our assumptions concern the 
relationship between thegiven endomorphism and its adjoint endomorphism.Thus, 
we focus on Euclidean or unitary vector spaces. This leads to the classes of normal, 
orthogonal, unitary and selfadjoint endomorphisms. Each of these classes has 
a natural counterpartinthe setof square(real or complex) matrices. 

18.1 NormalEndomorphisms 
We start with the definition ofa normal1 endomorphism or matrix. 

Definition 18.1 Let V beafinite dimensional Euclideanor unitaryvector space.An 
endomorphism f .L(VV iscalled normal if f.fad . 
fad .f.Amatrix A.Rnn 
or A.Cnn is called normal if ATA=AAT or AHA=AAH, respectively. 

For all z .C we have zz =|z|2 =zz.The property of normality can therefore 
be interpretedasa generalizationofthispropertyof complex numbers. 

Wewillfirststudythepropertiesofnormal endomorphismsonafinite dimensional 
unitary vector space V.Recallthe following results: 

(1) If Bisan orthonormal basisof Vandif f .L(VV ,then([f]BB 
H =[fad ]BB 
(cp. Theorem 13.12). 
(2) Every 
f .L(VV can be unitarily triangulated (cp. Corollary 14.20, Schur’s 
theorem). This does not holdin generalin theEuclidean case, since notevery 
real polynomial decomposes into linearfactorsover R. 
1This termwasintroducedby OttoToeplitz (1881–1940)in 1918in thecontext ofbilinear forms. 

©SpringerInternationalPublishing Switzerland 2015 
271 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_18 

272 18 Special ClassesofEndomorphisms 

Usingtheseresults we obtainthe following characterizationofnormal endomorphisms 
on a unitary vector space. 

Theorem 18.2 If V isa finite dimensional unitary vector space,thenf .L(VV is normalifand onlyifthereexistsanorthonormal basisB ofV suchthat [f]BB is 
adiagonal matrix, i.e., f isunitarily diagonalizable. 

Proof Let f . 
L(VV be normal and let B be an orthonormal basis of V such 
that R :. 
[f]BB is an upper triangular matrix. Then RH =[fad]BB, and from 

f . 
fad . 
fad . 
f we obtain 

RRH =[f . 
fad ]BB =[fad . 
f]BB =RHR 

We nowshowbyinductiononn =dim(V that R is diagonal.Thisisobvious for 
n =1. 
Letthe assertion holdfor an n .1, and let R .Cn+1 n+1 be upper triangular with 
RRH =RHR.We write Ras 
. 
R1 r1

R . 


0 .1 

where R1 .Cnn is upper triangular, r1 .Cn 1, and .1 .C.Then 

R1RH H RHRH 

1 +r1r.1r11 R11 r1

1 

H 2 . 
RRH =RHR . 
HH 2

.1r|.1|r1 R1 r1 r1 +|.1|

1 

HH

From |.1|2 =r1 r1 +|.1|2 we obtain r1 r1 =0, hencer1 =0and R1R1 
H =R1 
HR1. 
By theinductionhypothesis, R1 .Cnn is diagonal, and therefore 

R10

R . 


0 .1 

is diagonal as well. 

Conversely,supposethat thereexistsorthonormal basis B of V such that [f]BB 
is diagonal.Then [fad ]BB . 
([f]BB 
H is diagonal and, since diagonal matrices 
commute, wehave 

[f . 
fad ]BB =[f]BB[fad ]BB =[fad ]BB[f]BB =[fad . 
f]BB 

whichimplies f . 
fad . 
fad . 
f, and hence f is normal. . 


The application of this theorem to the unitary vector space V . 
Cn 1 with the 
standard scalar product and a matrix A.Cnn viewed as element of L(VV yields 
thefollowing “matrixversion”. 

Corollary 18.3 AmatrixA.Cnn is normalifandonlyifthereexistsanorthonormal 
basis of Cn 1 consistingofeigenvectorsofA, i.e.,Ais unitarily diagonalizable. 


18.1 NormalEndomorphisms 273 
The following theorem presents another characterization of normal endomorphisms 
on a unitary vector space. 

Theorem 18.4 If V isa finite dimensional unitary vector space,thenf .L(VV is normalifand onlyifthereexistsa polynomialp .C[t]with p( 
f . 
fad. 

Proof If p( 
f . 
fad for a polynomial p .C[t],then 

f . 
fad . 
f .p( 
f . 
p( 
f . 
f . 
fad . 
f 

and hence f is normal. 

Conversely,if f is normal, then there exists an orthonormal basis B of V, such 
that [f]BB =diag(.1 .n . Furthermore, 

[fad ]BB =([f]BB 
H =diag .1 .n 

Let p .C[t]be a polynomial with p(.j =. 
j for j =1 n. Such a polynomial 
can be explicitly constructed using the Lagrange basis of C[t].n.1 (cp. Exercise 
10.12). Then 

[fad ]BB =diag .1 .n =diag p(.1 p(.n =p(diag(.1 .n 

=p [f]BB =[p( 
f ]BB 
and hence also fad =p( 
f . . 


Severalother characterizationsof normal endomorphismsona finite dimensional 
unitary vector space and of normal matrices A . 
Cnn can be found in the article[
HorJ12] (see alsoExercise 18.8). 

We now consider theEuclidean case, wherewefocus on real squarematrices. 
Allthe results canbe formulated analogously for normal endomorphismsona finite 
dimensional Euclidean vector space. 

Let A .Rnn be normal, i.e., ATA . 
AAT.Then Aalso satisfies AHA . 
AAH 
and when A is considered as an element of Cnn,itis unitarily diagonalizable, i.e., 
A . 
SDSH holds for a unitary matrix S .Cnn andadiagonal matrix D .Cnn . 
Despite thefact that Ahas real entries, neither Snor Dwill be real in general, since 
Aas an element of Rnn may notbe diagonalizable.Forinstance, 

12 

22

A=.R

.21 

isanormalmatrixthatis not diagonalizable(overR).Considered as element ofC2 2 , 
it has theeigenvalues1+2iand1.2iand it is unitarily diagonalizable. 

To discussthe caseofreal normalmatricesin more detail,we firstprovea “real 
version” of Schur’s theorem. 


274 18 Special ClassesofEndomorphisms 

Theorem 18.5 For every matrix A . 
Rn n there exists an orthogonal matrix 
U .Rn n with 

.. 


R11 R1m 
.. 
. 
Rnn 

UT AU =R =
.. 


Rmm 

wherefor everyj =1 m eitherRjj .R11 or 

( 
j ( 
j

rr

12 22 ( 
j

Rjj . 
( 
j ( 
j .Rwith r3 =0 

rr

34 

Inthesecond caseRjj has, consideredas complexmatrix,apairofcomplexconjugate 
eigenvalues of theform . 
j ±i. 
j with .j .R and .j . 
R \{0}. The matrix R is 
called a real Schur form of A. 

Proof We proceed via induction on n.For n . 
1wehave A =[a11]. 
R and 
U =[1]. 

Supposethat theassertion holds forsome n .1and let A .Rn+1 n+1 be given. 
WeconsiderAas an element of Cn+1 n+1.Then Ahas an eigenvalue . 
=.+i. 
.C, 
.. 
. 
R, corresponding to the eigenvector v 
. 
x +iy . 
Cn+11 , xy . 
Rn+11 , 
and we have Av 
=.v.Dividingthis equationinto its real and imaginary parts,we 
obtainthetworeal equations 

Ax =.x .. 
y and Ay =.x +.y (18.1) 

We have two cases: 

Case 1: . 
. 
0. Then thetwo equationsin(18.1)are Ax . 
.x and Ay . 
.y. Thus at least one of the real vectors x or y is an eigenvector corresponding 
to the real eigenvalue . 
of A. Without loss of generality we assume that this is 
the vector x and that 	x	2 . 
1. We extend x by the vectors w2 ,...,wn+1 to an 
orthonormal basis of Rn+11 with respecttothestandardscalar product.The matrix 
U1 :=[x ,w2 ,...,wn+1].Rn+1 n+1 then is orthogonal and satisfies 

.

U1 
T AU1 . 


0 

A1 

foramatrix A1 .Rnn.Bythe inductionhypothesis thereexistsanorthogonal matrix 
U2 .Rnn such that R1 :=U2 
TA1U2 has the desired form.The matrix 

10

U :=U1 

0U2 

is orthogonal and satisfies 


18.1 NormalEndomorphisms 275 
10 10 .

UT AU . 
U1 
T AU1 . 


=:R

0UT 

R1

20U20 

where Rhas the desired form. 

Case2: .=0.We first show that xy are linearly independent.If x =0, then 
using .=0inthe first equationin(18.1)impliesthat alsoy=0. Thisis not possible, 
since theeigenvector v 
=x +iymust be nonzero. Thus, x =0, and using.=0in 
thesecond equationin(18.1)impliesthat alsoy=0.If xy .Rn 1\{0}are linearly 
dependent,then thereexistsa ..R\{0}with x =.y.Thetwo equationsin(18.1) 
then can be written as 

1 

Ax =(....x and Ax . 
(.+..x 

.

whichimplies that .(1+.2 . 
0. Since1+.2 
. 
0for all .. 
R, thisimplies 
.=0, which contradictsthe assumptionthat.=0. Consequently,xy are linearly 
independent. 

We can combine thetwo equationsin(18.1)tothe system 

..

A[xy]=[xy. 


... 


where rank([xy]. 
2. Applying theGram-Schmidt method with respect to the 
standard scalar product of Rn+11 to the matrix [xy].Rn+1 2 yields 

r11 r12

[xy]=[q1 q2. 
=:QR1

0 r22 

with QTQ=I2 and R1 .GL2(R .Itthen follows that 

.. 
..

AQ =A[xy]R.1 =[xy. 
R.1 =QR1 R.1 

1 11

... 
... 


The real matrix 
. 


.. 
R.1

R2 :=R11

... 


has, considered as element of C22,the pairofcomplexconjugateeigenvalues .±i. 
with .=0.In particular,the(2 1 -entry ofR2 is nonzero, since otherwise R2 would 
have two real eigenvalues. 

Weagainextendq1 q2 byvectorsw3 ,...,wn+1 to an orthonormal basis of Rn+11 
with respecttothestandardscalar product.(For n =1thelistw3 ,...,wn+1is empty.) 
Then U1 :=[Q ,w3 ,...,wn+1].Rn+1 n+1 is orthogonal and we have 

R2

U1 
TAU1 =U1 
T AQA[w3 ,...,wn+1]=U1 
T QR2 A[w3 ,...,wn+1]. 


0 

A1 


276 18 Special ClassesofEndomorphisms 

for a matrix A1 . 
Rn.1 n.1. Analogously to the first case, an application of the 
inductionhypothesis to this matrix yields the desired matrices Rand U. . 


Theorem 18.5 impliesthe following result forreal normalmatrices. 

Corollary 18.6 AmatrixA. 
Rnn is normalifandonlyifthereexistsanorthogonal 
matrixU . 
Rn n with 

UT AU . 
diag(R1 Rm 

where, foreveryj. 
1 m eitherRj . 
R11 or 

.j .j 22

Rj =. 
Rwith . 
j 
. 
0

..j .j 

Inthesecond casethe matrixRjhas, consideredas complexmatrix,apairofcomplex 
conjugateeigenvaluesof theform . 
j . 
i. 
j. 

Proof Exercise. . 


Example 18.7 The matrix 

. 
vv 
. 


02. 
2

1 v 


..

A=. 
211 . 
R33 

v

2 

211 

has, considered asa complex matrix,the eigenvalues1 i .i.Itistherefore neither 
diagonalizable nor can it be triangulated over R.For the orthogonal matrix 

.. 


02 0

1 vv 


33 

..

U =. 
20 2 . 
R

vv

2 

20 2 

thetransformed matrix 
.. 
010 
UT AU =.100

.. 


001 

is in real Schur form. 

18.2 Orthogonal andUnitary Endomorphisms 
In this section we extend the concept of orthogonal and unitary matrices to endomorphisms. 



18.2 Orthogonal and Unitary Endomorphisms 277 
Definition 18.8 Let V be a finite dimensional Euclidean or unitary vector space. 
An endomorphism f . 
L(VV is called orthogonal or unitary, respectively, if 

fad . 
f =IdV. 

If fad . 
f =IdV, then fad . 
f is bijective and hence f is injective(cp. Exercise 
2.7). Corollary 10.11 impliesthat f is bijective. Hence fad is the unique inverse 
of f, and we also have f . 
fad =IdV (cp. our remarksfollowing Definition 2.21). 

Note that an orthogonal or unitary endomorphism f is normal, and therefore all 
results fromthe previous sectionalso applyto f. 

Lemma 18.9 Let V be a finite dimensional Euclidean or unitary vector space and 
let f .L(VV be orthogonalor unitary,respectively.IfBisanorthonormal basis 
of V,then [f]B B is an orthogonal or unitary matrix, respectively. 

Proof Letdim(V =n.For every orthonormal basis B of V we have 

In =[IdV]BB =[fad . 
f]BB =[fad ]BB[f]BB =([f]BB 
H[f]BB 

and thus [f]BB is orthogonal or unitary, respectively. (In the Euclidean case 
([f]BB 
H =([f]BB 
T.) . 


In thefollowing theorem we show that an orthogonal or unitary endomorphism 
is characterizedby thefact thatit does not change thescalar productof arbitrary 
vectors. 

Lemma 18.10 Let V bea finite dimensional Euclidean or unitary vector space with 
the scalar product . 
·. Then f .L(VV is orthogonal or unitary, respectively, if 
and onlyiff(v 
f(w 
=v,wfor all v,w 
.V. 

Proof If f is orthogonal or unitary and if v,w 
.V,then 

v,w=IdV(v 
w. 
( 
fad . 
f )(v 
w 
=f(v 
f(w 
. 


On theother hand, supposethat v,w=f(v 
f(w 
for all v,w 
.V.Then 

0=v,w.f(v 
f(w 
=v,w. 
v,( 
fad . 
f )(w 


. 
v,(IdV . 
fad . 
f )(w 
Since the scalar product is non-degenerate and v 
canbe chosen arbitrarily,wehave 
(IdV . 
fad . 
f )(w 
=0for allw 
.V, and hence IdV . 
fad . 
f. . 


Wehave thefollowing corollary (cp. Lemma12.13). 

Corollary 18.11 If V isa finite dimensional Euclidean or unitary vector space with 
the scalar product . 
·,f . 
L(VV is orthogonal or unitary, respectively, and 
	·	=. 
·12 is the norminducedby thescalarproduct,then 	f(v 
	=	v	for 
all v 
.V. 


278 18 Special ClassesofEndomorphisms 

Forthevector spaceV =Cn 1 with thestandardscalar product and induced norm 
	v	2 =(vH v 
1 2 as well as a unitary matrix A .Cnn,wehave 	Av	2 =	v	2 for 
all v 
.Cn 1.Thus, 

	Av	2

	A	2 . 
sup =1 

v.Cn 1\{0}	v	2 

(cp.(6)inExample 12.4).This holds analogouslyfororthogonal matrices A.Rnn . 

We now study the eigenvalues and eigenvectors oforthogonal and unitary endomorphisms. 


Lemma 18.12 Let V be a finite dimensional Euclidean or unitary vector space and 
let f .L(VV be orthogonal or unitary, respectively. If . 
is aneigenvalueoff, 
then |.|=1. 

Proof Let . 
·be the scalar product on V.If f(v 
=.v 
with v 
=0, then 

v,v=IdV(v 
v=( 
fad . 
f )(v 
v=f(v 
f(v 
=.v 
.v=|.|2v,v. 


and v,v=0impliesthat|.|=1. . 


Thestatementof Lemma 18.12 holds,in particular,for unitary and orthogonal 
matrices.However,oneshouldkeepinmindthatan orthogonalmatrix(oranorthogonal 
endomorphism)maynothaveaneigenvalue.Forexample,theorthogonal matrix 

0.1 

22

A=.R

10 

has the characteristic polynomial PA =t2+1, which has no real roots. If considered 
as an element of C22,the matrix Ahas the eigenvalues iand .i. 

Theorem 18.13 

(1) IfA.Cnn is unitary,then thereexistsa unitary matrixU .Cn n with 
UH AU =diag(.1 .n 

and |. 
j|=1forj=1 n. 

(2) IfA.Rnn is orthogonal,then thereexistsanorthogonal matrixU .Rn n with 
UT AU =diag(R1 Rm 
where for every j=1 m either Rj =[.j].R1 1 with .j =±1or 
. 
. 


cj sj 22 22

Rj =.Rwith sj =0 and cj +sj =1

.sj cj 


18.2 Orthogonal and Unitary Endomorphisms 279 
Proof 

(1) A unitary matrix A .Cnn is normal and hence unitarily diagonalizable(cp. 
Corollary 18.3). By Lemma 18.12, all eigenvalues of Ahave absolute value 1. 
(2) An orthogonal matrix Ais normal and hence by Corollary 18.6 there exists an 
orthogonal matrix U .Rnn with UT AU =diag(R1 Rm , where either 
Rj .R11 

or 

. 
j .j 22

Rj =.R

.. 
j .j 

with .j =0. In the first casethen Rj =[. 
j]with |. 
j|=1byLemma18.12. 
Since A and U are orthogonal, also UT AU is orthogonal, and hence every 
diagonal block Rjisorthogonal as well. From RTj Rj =I2we obtain .2 
j+.2 
j =1, 
so that Rj has the desired form. . 


Wenowstudytwoimportant classesoforthogonal matrices. 

Example 18.14 Let ij n .N with1.i < 
j.n and let . 
.R.We define 

Rij(. 
:. 


. 
. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


1 
1 
cos(. 
.sin(. 
1 
1 
sin(. 
cos(. 
1 
1 

. 
. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


. 


<i 
<j 
^^ 


ij 
The matrix Rij(. 
=[rij].Rnn is equal to theidentity matrix In except for its 
entries 
rii =cos(. 
rij =.sin(. 
rji =sin(. 
rjj =cos(. 
For n =2wehavethe matrix 

cos(. 
.sin(.

R12(. 
. 


sin(. 
cos(. 



280 18 Special ClassesofEndomorphisms 

which satisfies 

cos(. 
sin(. 
cos(. 
.sin(.

R12(. 
TR12(. 
. 


.sin(. 
cos(. 
sin(. 
cos(. 


cos2(. 
+sin2(. 
0

. 
0 cos2(. 
+sin2(. 
. 
I2 . 
R12(. 
R12(. 
T 
One easily sees that each of the matrices Rij(. 
.Rnn is orthogonal.The multiplication 
of a vector v 
. 
Rn 1 with the matrix Rij(. 
results ina(counterclockwise) 
rotation of v 
by the angle . 
in the (ij -coordinate plane. In Numerical Mathematics, 
the matrices Rij(. 
are called Givens rotations.2 Thisis illustratedinthe 
figure below forthevector v 
=[10 075]T .R21 and the matrices R12(. 
2 and 
R12( 
2. 
, which represent rotations by 90 and 120 degrees, respectively. 

3 


Example 18.15 For u .Rn 1 \{0}we define the Householder matrix 

2 

T .Rnn 

H(u :. 
In . 
uu(18.2) 

uTu 

and for u . 
0weset H(0 :. 
In.For every u . 
Rn 1 then H(u is an orthogonal 
matrix (cp. Exercise 12.17). The multiplication of a vector v 
.Rn 1 with the matrix 
H(u describes a reflection of v 
at thehyperplane 

T

(span{u}.. 
y .Rn 1 |uy =0 

i.e.,thehyperplaneofvectorsthat areorthogonalto u with respect to the standard 
scalarproduct.Thisisillustratedinthefigurebelowforthevector v 
=[175 05]T . 
R21 and theHouseholder matrix 

01

H(u . 


10 

which corresponds to u =[.11]T .R2 1 . 

2WallaceGivens (1910–1993),pioneerof Numerical Linear Algebra. 


18.2 Orthogonal and Unitary Endomorphisms 281 
MATLAB-Minute. 

Let u =[531]T . 
R31 . Apply the command norm(u) to compute 
the Euclidean norm of u and form the Householder matrix H=eye(3)(
2/(u’.u)).(u.u’).Check the orthogonality of H via the computation of 
norm(H’.H-eye(3)). Form the vector v=H.u and compare the Euclidean 
norms of u and v. 

18.3 SelfadjointEndomorphisms 
Wehavealready studied selfadjoint endomorphisms f onafinite dimensional Euclideanor 
unitaryvector space.The defining propertyforthisclassof endomorphisms 
is f . 
fad (cp. Definition 13.13). 

Obviously, selfadjoint endomorphisms are normal and hence the results of 
Sect.18.1 hold.We nowstrengthen someof theseresults. 

Lemma 18.16 For a finite dimensional Euclidean or unitary vector space V and 
f .L(VV ,the following statements are equivalent: 

(1) f is selfadjoint. 
H

(2)Foreveryorthonormal basisB ofV we have [f]BB =([f]BB . 
H

(3) Thereexistsanorthonormal basisB ofV with [f]BB =([f]BB . 
(In theEuclidean case ([f]BB 
H =([f]BB 
T.) 
Proof In Corollary 13.14 we have already shownthat (1)implies(2), and obviously 
(2)implies(3).If(3) holds,then [f]BB =([f]BB 
H =[fad]BB (cp. Theorem 
13.12), and hence f . 
fad ,sothat (1) holds. . 


Wehavethefollowing strong resultonthediagonalizabilityofselfadjoint endomorphismsin 
boththe Euclidean and the unitary case. 

Theorem 18.17 If V is a finite dimensional Euclidean or unitary vector space and 
f .L(VV is selfadjoint,then thereexistsanorthonormal basisB ofV suchthat 
[f]B B is a real diagonal matrix. 


282 
18 Special ClassesofEndomorphisms 

Proof Consider first the unitary case. If f is selfadjoint,then f is normal and hence 
unitarily diagonalizable(cp. Theorem 18.2). Let B be an orthonormal basis of V so 
that [f]BB isadiagonal matrix.Then [f]BB =[fad ]BB =([f]BB 
H impliesthat 
thediagonal entriesof [f]BB,which arethe eigenvaluesof f, are real. 

Let V be an n-dimensional Euclidean vector space. If B ={v1 ,...,vn}is an 

orthonormal basis of V,then[f]
B is symmetricandin particular normal.ByCorol-

B 

lary 18.6,thereexistsanorthogonal matrix U =[uij].Rnn with 

UT[f]BU =diag(R1 Rm

B 

wherefor j=1 m either Rj .R11 or 

.j .j 22

Rj =.Rwith .j =0

..j .j 

Since UT[f]
BU is symmetric,a2.2block Rj with .j =0cannot occur.Thus,

B 
UT[f]
BU is a real diagonal matrix.

B 

We define the basis B ={w1 ,...,wn}of V by 

(w1 ,...,wn =(v1 ,...,vnU 

Then, by construction, U =[IdV]B 
. 


B and hence UT =U.1 =[IdV]BB.Therefore, 
UT[f]BU =[f]BB.If . 
·. 
is the scalar product on V, then vi v 
j. 
.ij,

B 
ij=1 n.With UTU =In we get 

. 
nn 
. 
nn 
n 

wi w 
j. 
uki vkujv. 
. 
ukiujvk ,v. 
ukiukj =.ij 
k=1 =1 k=1 =1 k=1 

Hence B is an orthonormal basis of V. 
. 


This theorem has thefollowing “matrixversion”. 

Corollary 18.18 

(1) IfA.Rnn is symmetric,then thereexistan orthogonal matrixU .Rnn anda 
diagonal matrixD .Rn n with A=UDUT . 
(2) IfA 
. 
Cn n is Hermitian, then there exist a unitary matrixU . 
Cnn anda 
diagonal matrixD .Rn n with A=UDUH. 
Thestatement(1)inthis corollaryisknownasthe principal axes transformation. 
Wewill briefly discussthe backgroundof this namefromthe theoryofbilinear forms 
and their applications in geometry.Asymmetric matrix A =[aij].Rnn defines a 
symmetric bilinear form on Rn 1 via 


18.3 Selfadjoint Endomorphisms 283 
nn 

.A . 
Rn 1 . 
Rn 1 ^ 
R ,(xy 
^ 
yT Ax . 
aijxiyj 
i=1 j=1 

The map 

qA . 
Rn 1 ^ 
R x 
^ 
.A(xx . 
xT Ax 

is calledthe quadratic form associated with this symmetric bilinear form. 

Since Ais symmetric, there exists an orthogonal matrix U =[u1 un. 
such 
that UT AU . 
Disareal diagonal matrix.If B1 ={e1 en},then[.A]B1.B1 . 
A. 
The set B2 ={u1 un} 
forms an orthonormal basis of Rn 1 with respect to the 
standardscalar product, and [u1 un]=[e1 en]U, hence U =[IdRn 1]B2 B1. 
Forthe changeof bases fromof B1 to B2 we obtain 

T[.A]B2.B2 =[IdRn 1]B2 B1 [.A]B1.B1[IdRn 1]B2 B1 . 
UT AU . 
D 

(cp. Theorem 11.14). Thus,the real diagonal matrix Drepresentsthe bilinear form 
.A defined by Awith respect to the basis B2. 

The quadratic form qA associated with .A is also transformed to a simpler form 
by this change of bases, since analogously 

.. 


ny1 

2 
.. 


qA(x . 
xT Ax . 
xTUDUTx . 
yTDy . 
.iyi . 
qD(yy . 
.. 
:. 
UTx 
i=1 

yn 

Thus,the quadratic form qA is turnedintoa “sumofsquares”, definedbythe quadratic 
form qD. 

The principal axes transformation isgivenbythe changeof bases fromthe canonical 
basis of Rn 1 to the basisgivenbythe pairwiseorthonormaleigenvectorsof Ain 
Rn 1.The n pairwiseorthogonal subspaces span{uj}, j. 
1 n,formthe nprincipal 
axes.The geometricinterpretation of thistermis illustratedinthe following 
example. 

Example 18.19 Forthe symmetric matrix 

41 

22 

A=. 
R

12 

we have 

. 
v 
. 


3. 
20

UT AU . 
v. 
D

03 . 
2 


284 18 Special ClassesofEndomorphisms 

with the orthogonal matrix U=[u1 u2].R22 and 

c .s 

u1 . 
u2 . 
where 

sc 

v 


1. 
21 

c. 
. 
=09239 s . 
. 
=03827 

vv 


(1. 
22 +1 (1. 
22 +1 

(The numbers hereare rounded to thefourth significant digit.)With theassociated 

22

quadratic form qA(x =4x1 +2x1x2 +2x2, we define the set 

EA ={x .R21 |qA(x .1=0} 


As describedabove,the principalaxes transformation consistsinthetransformation 
fromthe canonical coordinatesystemtoacoordinatesystemgivenbyan orthonormal 
basisof eigenvectorsof A.If we carry out this transformation and replace qAby the 
quadratic form qD, we get the set 

22 

. 
. 
yy

T .R


ED . 
y.R21 |qD(y .1=0 =[y1 y2]21 1 . 
2 .1=0 

.2 .2 

12 

11 

where .1 . 
v=04760 .2 . 
v=07941 

3. 
23. 
2 

This set forms the ellipse centered at the origin of the two dimensional cartesian 
coordinatesystem(spannedbythe canonical basisvectors e1 e2)withaxesof lengths 
.1 and .2,whichis illustratedonthe left partofthe following figure: 


The elements x .EA aregiven by x =Uyfor y.ED.The orthogonal matrix 

c.s

U. 


sc 


18.3 Selfadjoint Endomorphisms 285 
is a Givens rotation that rotates the ellipse ED counterclockwise by the angle 
cos.1(c =03926 (approximately 225degrees). Hence EA is just a “rotated version” 
of ED.The right partofthe figure above showsthe ellipse EA in the cartesian 
coordinatesystem.The dashed lines indicatethe respectivespansofthevectors u1 
and u2,which arethe eigenvectorsof Aand the principal axes of the ellipse EA. 

Let A.Rnn be symmetric.Foragivenvector v 
.Rn 1 and a scalar . 
.R, 

Tx .Rn 1

Q(x =xT Ax +v 
x +. 


isa quadratic functionin n variables (the entriesof thevector x). Thesetof zerosof 
this function, i.e., the set {x .Rn 1 |Q(x =0},is called a hypersurface of degree 
2or a quadric.InExample 18.19 we have already seen quadricsinthe case n =2 
and with v 
=0.We nextgive some furtherexamples. 

Example 18.20 

(1) Let n =3, A=I3, v 
=[000]T and . 
=.1. The corresponding quadric 
[x1 x2 x3]T .R31 | 
x12 +x22 +x32 .1=0 

is thesurfaceof the ballwith radius1 around theorigin: 


10

(2) Let n =2, A. 
, v 
=[02]T and . 
=0. The corresponding quadric 
00 

[x1 x2]T .R21 | 
x12 +2x2 =0 

is a parabola: 



286 18 Special ClassesofEndomorphisms 

.. 


100 

.

(3) Let n . 
3, A. 
000.
,v 
=[020]T and . 
. 
0. The corresponding quadric 
000 

[x1 x2 x3]T . 
R31 | 
x12 . 
2x2 . 
0 
is a parabolic cylinder: 



Corollary 18.18 motivatesthe following definition. 

Definition 18.21 If A . 
Rnn is symmetric or A . 
Cnn is Hermitian with n. 
positive, n. 
negative and n0 zero eigenvalues (counted with their corresponding 
multiplicities),then thetriple (n. 
n. 
n0 is calledthe inertia of A. 

Letusfirst consider,for simplicity, onlythe caseof real symmetric matrices. 

Lemma 18.22 If A . 
Rn n symmetric has the inertia (n. 
n. 
n0 , then A and 
SA . 
diag(In+.In. 
0n0 are congruent. 

Proof Let A . 
Rnn be symmetric and let A . 
UUT with an orthogonal matrix 
U . 
Rnn and . 
. 
diag(.1 .n . 
Rnn.If Ahas theinertia (n. 
n. 
n0 ,then 
we can assume without loss of generality that 

.. 


n. 
. 
. 
n.. 
diag(n. 
,n. 
0n0 

.. 


0n0 

where the diagonal matrices n. 
and n. 
contain the positive and negative eigenvalues 
of A,respectively, and0n0 . 
Rn0 n0.Wehave . 
. 
SA, where 

. 
Rnn 

SA :. 
diag(In+.In. 
0n0 
12 12 In0

. 
:. 
diag((n. 
,(.n.. 
GLn(R 


18.3 Selfadjoint Endomorphisms 287 
Here (diag(.1 .m 
12 . 
diag( 
v 
.1 v 
.m and thus 

T

A. 
UUT . 
USAUT . 
(U. 
SA(U. 
. 


This result willbe usedin theproofof Sylvester’slawof inertia.3 

Theorem 18.23 The inertia of a symmetric matrix A . 
Rn n is invariant under 
congruence, i.e., foreverymatrixG . 
GLn(R thematricesAandGTAGhave the 
same inertia. 

Proof Theassertionistrivial for A . 
0.Let A 
. 
0have the inertia(n. 
n. 
n0, 
then not both n. 
and n. 
canbe equalto zero.We assume without lossof generality 
that n. 
> 
0. (Ifn+. 
0, then thefollowing argument canbe appliedforn. 
> 
0.) 

By Lemma 18.22 there exist G1 . 
GLn(R and SA . 
diag(In+.In. 
0n0 with 
A . 
G1 
TSAG1.Let G2 . 
GLn(R be arbitrary and set B :. 
G2 
T AG2. Then B 
is symmetric and has an inertia (
n. 
n
. 
n0 .Therefore, B . 
G3 
TSBG3 for SB . 
diag(I
n+.I
n
. 
0
n0 and a matrix G3 . 
GLn(R . If we show that n+. 
n. 
and 
n0 . 
n0,then also n.. 
n.. 

Wehave 


T 


G.1 G.1 TGT . 
GT

A. 
BG.21 . 
23 SBG3G.21 
4 SBG4 G4 :. 
G3G.1 

22 

and G4 . 
GLn(R impliesthat rank( 
A . 
rank(SB . 
rank(B , hence n0 . 
n0. 
We set 

G.1 =[u1 un. 
,v1 ,...,vn. 
,w1 ,...,wn0. 
and

1 

G.1 =[u1 

u 
v1 
vw

1 
]

4 
n. 
n
. 
wn0

Let V1 :. 
span{u1 un+} 
and V2 :. 
span{v1 

v 
n
. 
w

1 w

n0}.Since n. 
> 
0, we have dim(V1 . 
1. If x . 
V1 \{0},then 

n. 


x . 
.juj . 
G.11 [.1 .n. 
00]T 

j=1 

for some .1 .n+. 
R that are not all zero. This implies 

n. 


xT Ax . 
.2 > 
0

j 
j=1 

3James Joseph Sylvester (1814–1897) proved this result for quadratic forms in 1852. He also 
coined the name law ofinertia which accordingtohimis“expressing thefactof theexistenceof 
aninvariable numberinseparably attachedto such [bilinear]forms”. 


288 18 Special ClassesofEndomorphisms 

If, on the other hand, x . 
V2, then an analogous argument shows that xT Ax . 
0. 
Hence V1 .V2 ={0}, and thedimensionformula forsubspaces (cp. Theorem 9.29) 
yields 

dim(V1 +dim(V2 .dim(V1 .V2 =dim(V1 +V2 .dim(Rn 1 =n 

. 
. 
. 
. 
. 


=n+=n.n+=0 
and thus n+.n+.Ifwerepeat thesame constructionbyinterchanging therolesof 
n. 
and 
n+,then
n+.n+.Thus, n+=n. 
and theproofis complete. . 


In thefollowing result we transferLemma 18.22 and Theorem 18.23 to complex 
Hermitian matrices. 

Theorem 18.24 Let A . 
Cn n be Hermitian with the inertia (n. 
n. 
n0 . Then 
thereexistsa matrixG .GLn(C with 

A=GH diag(In. 
In. 
0n0 G 

Moreover,for every matrixG .GLn(C thematricesAandGHAGhave thesame 
inertia. 

Proof Exercise. . 


Finally,wediscussa special classofsymmetric and Hermitianmatrices. 

Definition 18.25 Areal symmetric or complex Hermitiann .n matrix Ais called 

(1) positive semidefinite,ifv 
HAv 
.0for allv 
.Rn 1 resp. v 
.Cn 1, 
(2) positive definite,ifv 
HAv> 
0for allv 
.Rn 1 \{0}resp. v 
.Cn 1 \{0}. 
Ifin(1)or(2)thereverseinequalityholds,thenthe correspondingmatricesarecalled 
negative semidefinite or negative definite, respectively. 

Forselfadjoint endomorphismswedefine analogously:IfVisafinite dimensional 
Euclidean or unitaryvector space with thescalar product . 
·and if f .L(VV is 
selfadjoint,then f is called positivesemidefinite or positive definite,iff(v 
v.0 
for all v 
.V resp. f(v 
v> 
0for allv 
.V \{0}. 

The following theorem characterizes symmetric positive definite matrices; see 
Exercise18.19andExercise18.20forthe transferofthe resultstopositive semidefinite 
matrices resp. positive definite endomorphisms. 

Theorem 18.26 If A.Rnn is symmetric,thenthefollowing statementsare equivalent: 


(1) Ais positive definite. 
(2) All eigenvalues of Aare real and positive. 
(3) Thereexistsa lowertriangular matrixL .GLn(R with A. 
LLT . 

18.3 Selfadjoint Endomorphisms 
289 
Proof 

(1) 
. 
(2 . 
The symmetric matrix A is diagonalizable with real eigenvalues (cp. 
(1)inCorollary 18.18).If . 
is an eigenvalue with associated eigenvector v, i.e., 
T 
T

Av=.v,then .vv=v 
TAv>0andv 
v>0impliesthat. 
>0. 

(2) 
.(1 :Let A=UT diag(.1 .nU be adiagonalization Awith an orthogonal 
matrix U . 
Rnn (cp. (1)inCorollary 18.18)and. 
j >0, j . 
1 n. 
Let v.Rn 1 \{0}be arbitrary and let w:=Uv.Then w=0andv=UT w,so 
that 
v 
TAv=(UT w 
TUT diag(.1 .nU(UT w 
=w 
T diag(.1 .n w 


n 

2

. 
. 
jw>0
j 
j=1 


(3) 
.(1 :If A. 
LLT with L .GLn(R ,then for every v.Cn 1 \{0}we have 
v 
TAv=v 
T LLT v=	LT v	22 >0 

since LT isinvertible.(Note that herewedo not need that Lis lowertriangular.) 

(1) 
. 
(3 . 
Let A . 
UT diag(.1 .nU be a diagonalization of A with an 
orthogonal matrix U . 
Rnn (cp. (1)inCorollary 18.18). Since A is positive 
definite,we know from(2) that . 
j >0, j=1 n.Weset 
12 :=diag( 
.1 .n 
12 )(12UT

and then have A . 
(U=. 
BTB.Let B . 
QR be a QRdecompositionoftheinvertible 
matrix B(cp. Corollary 12.12), where Q.Rnn 
is orthogonal and R . 
Rnn is an invertible upper triangular matrix.Then A . 
BTB =(QR T(QR . 
LLT, where L :. 
RT . . 


One easily sees that an analogous result holds for complex Hermitian matrices 
A . 
Cnn.Inthis caseinassertion(3) thelower triangular matrixis L . 
GLn(C 
with A. 
LLH . 

The factorization A . 
LLT in (3)is calleda Cholesky factorization4 of A.It 
is special case of the LU-decompositioninTheorem 5.4.Infact,Theorem 18.26 
showsthat an LU-decomposition ofa (real) symmetric positive definite matrix can 
be computed without row permutations. 

In order to computethe Cholesky factorization of the symmetric positive definite 
matrix A=[aij].Rnn, we consider the equation 

4Andre-LouisCholesky(1875–1918). 


290 
18 Special ClassesofEndomorphisms 

. 
... 


l11 l11 ··. 
ln1 

. 
...

A. 
LLT . 
. 
... 


ln1 ··. 
lnn lnn 

Forthe first rowof Awe obtain 

a11 . 
l112 =. 
l11 =v 
a11 
(18.3) 

a1j

a1j . 
l11lj1 =. 
lj1 . 
j. 
2 n (18.4)
l11 

Analogously,for therows i . 
2 n of Awe obtain 

i 
. 
i.1 

. 
#12 

aii . 
lijlij =. 
lii . 
aii . 
l2 
(18.5)

ij 
j=1 j=1 

n ii.1 

aij . 
likljk . 
likljk . 
likljk . 
liilji 
k=1 k=1 k=1 

. 
i.1 
#

1 

=. 
lji . 
aij . 
likljk for j> 
i (18.6)
lii k=1 
Thesymmetric or Hermitian positive definite matrices areclosely relatedtothe 
positive definite bilinear formsonEuclidianor unitaryvector spaces. 

Theorem 18.27 If V is a finite dimensional Euclidian or unitary vector space and 
if .isasymmetric or Hermitianbilinear form on V,respectively, then thefollowing 
statements are equivalent: 

(1) .is positive definite, i.e., .(vv 
> 
0for all v 
. 
V \{0}. 
(2)Forevery basisB ofV the matrix representation [.]B.B is (symmetric or Hermitian) 
positive definite. 
(3) Thereexistsa basisB ofV suchthat the matrix representation [.]B.B is (symmetric 
or Hermitian) positive definite. 
Proof Exercise. 
. 


Exercises 

18.1 Let A. 
Rnn be normal. Showthat .Aforevery .. 
R, Ak forevery k . 
N0, 
and p(A for every p . 
R[t. 
are normal. 
18.2 Let AB . 
Rnn be normal. Are A. 
B and AB then normal as well? 
18.3 Let A. 
R22 be normalbut not symmetric. Showthat then 
..

A. 


... 



18.3 
Selfadjoint Endomorphisms 291 
for some . 
.R and . 
.R\{0}. 

18.4Prove Corollary 18.6 usingTheorem 18.5. 
18.5 Show that real skew-symmetric matrices (i. e., matrices with 
A =.AT . 
Rnn)and complexskew-Hermitianmatrices(i.e., matrices withA=.AH . 
Cnn)are normal. 
18.6 Let V be a finite dimensional unitary vector space and let f . 
L(VV be 
normal. Showthe following assertions: 
(a) If 
f . 
f2,then f is selfadjoint. 
(b) If 
f2 . 
f3,then f . 
f2. 
(c) If 
f is nilpotent,then f =0. 
18.7 Let Vbeafinite dimensionalrealorcomplexvectorspaceandlet f .L(VV be diagonalizable. Showthat thereexistsa scalar product on V such that f is 
normal with respect to this scalar products. 
18.8 Let A.Cnn. Showthe following assertions: 
(a) 
Ais normalifand onlyifthereexistsa normalmatrix B with n distinct 
eigenvalues that commutes with A. 
(b) 
Ais normalifand onlyif A+aI is normalforevery a .C. 
(c) Let H(A :. 
1(A+AH be theHermitian and S(A :. 
1(A.AH the
2 
2

skew-Hermitian partof A. Showthat A. 
H( 
A +S( 
A , H(AH . 
H(A 
and S(AH =.S( 
A . Show, furthermore,that Ais normalifand onlyif 
H(A and S( 
A commute. 

. 
az+b
18.9 Show that if 
A . 
Cnn is normal and if f(z cz+d with ad .bc 
. 
0is 
defined on the spectrum of A,then f(A =(aA +bI )(cA +dI .1. 
(The map f(z is calleda Mobius transformation.5 Such transformations play 
an importantrolein FunctionTheoryandinmanyother areasofMathematics.) 

18.10 
Let V be a finite dimensional Euclidian or unitary vector space and let f . 
L(VV be orthogonal or unitary, respectively. Show that f.1 exists and is 
again orthogonal or unitary, respectively. 
18.11 
Let u . 
Rn 1 and let the Householder matrix H(u be defined as in(18.2). 
Showthe following assertions: 
(a) For u 
. 
0 the matrices H(u and [.e1 e2 en. 
are orthogonally 
similar, i.e., there exists an orthogonal matrix Q.Rnn with 
QTH(uQ =[.e1 e2 en. 


(This implies that H(u only has the eigenvalues 1 and .1 with the 
algebraic multiplicities n .1and 1, respectively.) 

(b) Every orthogonal matrix A.Rnn canbe writtenasproductof n Householder 
matrices, i.e., there exist u1 un . 
Rn 1 with A . 
H(u1 
H(un . 
5AugustFerdinand Mobius (1790–1868). 


292 
18 Special ClassesofEndomorphisms 

T

18.12 
Let v 
.Rn 1 satisfy vv 
=1. Show that there exists an orthogonal matrix 
U .Rnn with Uv 
=e1. 
18.13 
Transferthe proofsofLemma18.22and Theorem 18.23to complexHermitian 
matrices and thus show Theorem 18.24. 
18.14 
Determinefor thesymmetric matrix 
10 6 

22 

A=.R

610 

an orthogonal matrix U .R2 2 such that UT AU is diagonal.Is A positive 
(semi-)definite? 

18.15 
Let K .{RC}and let {v1 ,...,vn}be a basis of Kn 1.Prove ordisprove:A 
H
matrix A . 
AH .Kn n is positive definite if and onlyif v 
jAv 
j > 
0for all 
j=1 n. 

18.16 
UseDefinition 18.25to test whetherthesymmetric matrices 
1112 21 

22 

. 
R

1121 12 

are positive (semi-)definite.Determinein all cases theinertia. 

18.17 
Let 
. 
A11 A12 .Rnn 

A. 


AT 

12 A22 

11 .GLm(R , A12 .Rmn. 
mn.

with A11 =AT 
m and A22 =AT m.The 

22 .Rn.
matrix S :. 
A22 .AT 
11 A12 .Rmm is called the Schur complement6 of

12 A.1 
A11 in A. Showthat Ais positive definite if A11 and Sare positive definite. 
(For the Schur complement, see also Exercise 4.17.) 

18.18 
Showthat A.Cnn is Hermitian positivedefiniteifandonlyifxy=yH Ax 
defines a scalar product on Cn 1. 
18.19 
ProvethefollowingversionofTheorem 18.26 for positivesemidefinite matrices. 
If A .Rnn is symmetric,thenthefollowing statementsare equivalent: 

(1) Ais positive semidefinite. 
(2) All eigenvalues of A are real and nonnegative. 
(3) Thereexistsan upper triangular matrixL .Rn n with A =LLT . 
18.20 
Let V be a finite dimensional Euclidian or unitary vector space and let f . 
L(VV be selfadjoint. Show that f is positive definite if and only if all 
eigenvalues of f are real and positive. 
18.21 
Let A .Rnn.Amatrix X .Rnn with X2 . 
Ais called a square root of A 
(cp. Sect. 17.1). 
6IssaiSchur (1875–1941). 


18.3 Selfadjoint Endomorphisms 
293 
(a) Showthatasymmetric positive definite matrix A. 
Rnn hasasymmetric 
positive definite square root. 
(b) Showthat thematrix 
.. 


33 6 6 
A. 
624 .12

.. 


6.12 24 

is symmetric positive definite and compute a symmetric positive definite 
square root of A. 

(c) Showthat thematrix A. 
Jn(0, n. 
2, does not have a square root. 
18.22 
Showthat thematrix 
.. 
210 
33 

..

A. 
121 . 
R

012 

is positive definite and compute a Cholesky factorization of A using(18.3)– 
(18.6). 

18.23 
Let AB . 
Cnn be Hermitian and let Bbe furthermore positive definite. 
Showthat the polynomial det(tB. 
A . 
C[t].n has exactly nreal roots. 
18.24 
Prove Theorem 18.27. 

Chapter19 
TheSingularValue Decomposition 

The matrix decomposition introduced in this chapter is very important in many 
practical applications, sinceit yields the best possible approximation(ina certain 
sense)of a given matrix by a matrix of low rank.A low rank approximation can be 
considereda“compression”ofthedatarepresentedbythegiven matrix.We illustrate 
this below with anexamplefromimage processing. 

We first prove the existence of the decomposition. 

Theorem 19.1 LetA .Cn m withn .mbegiven. Then thereexist unitary matrices 

V . 
Cn n and W .Cm m suchthat 
. 
. 
A= 
VWH with . 
= 
r 0r m.r 
0n.r r 0n.r m.r .Rn m ,r =diag(.1 .r 
(19.1) 

where .1 ..2 .···. 
.r > 
0and r =rank(A. 
0.Cnm 

Proof If A=0, then we setV = 
In, . 
= 
, r =[], W = 
Im, and we are 
finished. 

Let A 
= 
0 and r := 
rank(A .Since n . 
m,wehave1 . 
r . 
m, and since 
AHA.Cmm is Hermitian, thereexistsaunitary matrix W =[w1 ,...,wm].Cmm 
with 

.Rmm 

WH(AHA W =diag(.1 .m 

(cp. (2)inCorollary 18.18).Without loss of generality we assume that .1 . 
.2 . 
···..m.For every j=1 m then AHAw 
j =. 
jw 
j, and hence 

HH

. 
jww 
j = 
w 
j AHAw 
j =Aw 
j22 .0

j 

i.e., . 
j .0for j= 
1 m.Then rank(AHA =rank( 
A =r (tosee this,modify 
theproofof Lemma 10.25 forthe complex case).Therefore, thematrix AHA has 
exactlyr positive eigenvalues .1 .r and m .r times theeigenvalue0.Wethen 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_19 

296 19 TheSingularValueDecomposition 

12

define . 
j := 
.j , j = 
1 r, and have .1 . 
.2 . 
··· 
. 
.r.Let r be asin 
(19.1), 

r 0

D := 
.GLm(R X =[x1 xm]:=AWD.1 
0 Im.r 

Vr :=[x1 xr], and Z :=[xr+1 xm].Then 

VHVrVHZ VH Ir 0

rr r

=[Vr Z]=XHX =D.1WHAH AWD.1 = 


ZHVr ZHZ ZH 00 

which implies, in particular, that Z = 
0 and VHVr = 
Ir.We extend the vectors

r 
x1 xr to an orthonormal basis {x1 xr 
xr+1 
xn}of Cn 1 with respect to 
thestandardscalar product.Then thematrix 

xn].Cnn

V :=[Vr 
xr+1 


is unitary. From X = 
AWD.1 and X =[Vr Z]=[Vr 0] 
we finally obtain 
A=[Vr 0]DWH and A=VWH with . 
as in(19.1). . 


Astheproofshows, Theorem 19.1canbe formulated analogouslyforreal matrices 
A . 
Rnm with n . 
m.In this case the two matrices V and W are orthogonal.If 
n < 
m we can applythe theorem to AH (resp. AT in the real case). 

Definition 19.2 A decomposition ofthe form(19.1)is called a singular value 
decomposition or short SVD1 of the matrix A.The diagonal entries of the matrix 

r are called singular values and the columns of V resp. W are called left resp. right 
singular vectors of A. 
From(19.1)we obtainthe unitary diagonalizationsof thematrices AHA and 
AAH , 

20 20

AHArWH rVH

=W and AAH =V

00 00 

The singular values of A are therefore uniquely determined as the positive square 
rootsofthe positive eigenvaluesof AHAor AAH.The unitary matrices V and W in 
thesingularvalue decomposition,however, are(astheeigenvectorsin general)not 
uniquely determined. 

1Inthedevelopmentofthisdecompositionfromspecial casesinthemiddleofthe19th centurytoits 
current generalformmanyimportant playersofthe historyofLinear Algebraplayeda role.Inthe 
historical notes concerning thesingularvaluedecompositionin [HorJ91] one finds contributions 
of Jordan (1873),Sylvester (1889/1890) andSchmidt (1907).The currentformwas shownin 1939 
by Carl HenryEckart(1902–1973) andGaleYoung. 


19 TheSingularValueDecomposition 297 

If we write theSVDof Ain theform 

. 
. 
. 


Im r 0

A=VWH = 
V WHW WH =:UP

0n.mm 00m.r 

PH .Cmm

then U .Cnm has orthonormal columns, i.e., UHU = 
Im, and P = 
is positive semidefinite with theinertia (r 0 m .r .The factorization A =UP is 
called a polar decomposition of A.Itcanbe viewed asa generalizationofthe polar 
representation ofcomplex numbers, z =ei.|z|. 


Lemma 19.3 Suppose that the matrix A . 
Cn m with rank(A = 
r has an SVD 
of theform(19.1)withV =[v1 ,...,vn]and W =[w1 ,...,wm]. Considering 
A as an element of L(Cm 1 Cn 1 , we then have im(A = 
span{v1 ,...,vr}and 
ker( 
A =span{wr+1 ,...,wm}. 

Proof For j =1 r we have Aw 
j =VWH w 
j =Vej =. 
jv 
j =0, since 
.j =0. Hence theser linear independent vectors satisfy v1 ,...,vr .im(A .Now 
r =rank(A =dim(im( 
A impliesthat im(A =span{v1 ,...,vr}. 

For j=r+1 m wehave Aw 
j =0,and hence thesem.r linear independent 
vectors satisfy wr+1 ,...,wm .ker(A .Then dim(ker(A =m .dim(im(A = 
m .r impliesthatker(A =span{wr+1 ,...,wm}. . 


An SVDofthe form(19.1)canbe writtenas 

r 

H

A= 


. 
jv 
jw 
j 
j=1 

H

Thus, A can be written as a sum of r matrices of the form .jv 
jw 
j , where 

H

rank . 
jv 
jw 
=1. Let

j 

k 

H

Ak := 
. 
jv 
jw 
for some k 1.k .r (19.2)

j 
j=1 

Then rank(Ak = 
k and, using that the matrix 2-norm is unitarily invariant (cp. 
Exercise 19.1), we get 

A.Ak2 =diag(.k+1 .r 2 =.k+1 (19.3) 

Hence A is approximatedby the matrix Ak, where the rank of the approximating 
matrixandthe approximationerrorinthematrix2-normareexplicitlyknown.The 
singularvalue decomposition, furthermore,yieldsthe best possible approximation 
of Aby a matrix ofrank kwith respecttothematrix2-norm. 

Theorem 19.4 WithAk asin(19.2), we have A.Ak2 .A.B2 for every 
matrixB .Cn m with rank(B =k. 


298 19 TheSingularValueDecomposition 

Proof Theassertionisclear for k =rank( 
A ,since then Ak =Aand A.Ak2 =0. 

Let k < 
rank(A . 
m.Let B . 
Cnm with rank(B = 
k be given, then 
dim(ker(B = 
m .k, where we consider B as an element of L(Cm 1 Cn 1 .If 
w1 ,...,wm arethe right singularvectorsof Afrom(19.1), then U :=span{w1 
wk+1}has the dimension k +1. Since ker(B and U are subspaces of Cm 1 with 
dim(ker(B +dim(U =m +1, wehaveker(B .U ={0}. 

Let v 
.ker(B .U with v2 =1begiven. Then thereexist.1 .k+1 .C 


k+1 
k+1 

2

with v 
= 
1 . 
jw 
j and 1 |.j|2 =v=1. Hence

j=j=2 

k+1 k+1 

(A.B v 
=Av 
. 
Bv 
= 
. 
jAw 
j = 
. 
j. 
jv 
j 

=0 j=1 j=1 

and, therefore, 

k+1 

A.B2 = 
max ( 
A.B y2 .( 
A.B )v2 = 
. 
j. 
jv 
j 

2

y2=1 

j=1 

. 
k+1 

	12

2

=|.j.j|(since v1 ,...,vk+1 are pairwise orthonormal) 
j=1 

. 
k+1 

	12

2

..k+1 |.j|(since .1 .···..k+1) 
j=1 

=.k+1 =A.Ak2 

which completes theproof. . 


MATLAB-Minute. 

The command A=magic(n) generates for n .3ann.n matrix Awith entries 
from1 to n2, so that all row, column and diagonal sums of A are equal.The 
entries of Atherefore from a “magic square”. 
Compute the SVD ofA=magic(10) using the command [V,S,W]=svd(A). 
What can be said about the singular values of A and what is rank(A)?Form 
Ak for k =1 2 rank(A asin(19.2)andverify numerically the equation 
(19.3). 

TheSVDis oneofthemostimportantand practical mathematical toolsinalmost 
allareasof science, engineeringand social sciences,inmedicineandevenin psychology.
Itsgreat importanceisduetothefactthattheSVDallowsto distinguish between 
“important”and “non-important”informationinagiven data.In practice,thelatter 


19 TheSingularValueDecomposition 299 

corresponds, e.g., to measurement errors, noise in the transmission of data, or fine 
detailsinasignal or an image thatdo not play an important role.Often, the “important”
information correspondstothelarge singularvalues,andthe “non-important” 
informationtothe small ones. 

In many applications one sees,furthermore,that thesingularvaluesofagiven 
matrix decay rapidly, so that there exist only few large and many small singular 
values.Ifthisisthe case, then thematrix canbe approximatedwellbya matrix with 
low rank, since already for a small k the approximation error A. 
Ak2 = 
.k+1 is 
small.A lowrank approximationAk requires little storage capacity in the computer; 
only kscalars and2kvectorshavetobestored.ThismakestheSVDapowerful tool 
in all applications where data compression is of interest. 

Example 19.5 We illustratetheuseoftheSVDinimage compressionwithapicture 
that we obtained from the research center Matheon:MathematicsforKeyTechnologies2.
Thegreyscalepictureisshownontheleftofthefigurebelow.Itconsists 
of 286 . 
152pixels,whereeachofthepixelsisgivenbyavalue between0and64. 
Thesevalues arestoredinareal 286 . 
152 matrix Awhich has (full) rank 152. 


We computeanSVD A= 
VWT usingthe command [V,S,W]=svd(A) in MATLAB. 
Thediagonal entriesof thematrix S, i.e., thesingularvaluesof A,are ordered 
decreasinglybyMATLAB(as in Theorem 19.1).For k = 
100 20 10 we now 
compute matrices Ak with rank k asin(19.2)using the command Ak=V(:,1:k). 
S(1:k,1:k).W(:,1:k)’.These matrices represent approximations of the original 
picture based on the klargestsingularvalues and the corresponding singularvectors. 
Thethree approximations areshownnexttotheoriginal pictureabove.The quality 
of the approximation decreases with decreasing k,buteven the approximationfor 
k = 
10 showsthe essentialfeaturesof the “Matheon bear”. 

Another important applicationofthe SVDarisesinthe solutionoflinear systems 
of equations.If A. 
Cnm has an SVDofthe form(19.1), we define thematrix 

.1 

†VH . 
Cmn r 0 
. 
Rmn 
A† := 
Wwhere † := 
(19.4)

00 

2WethankFalkEbertforhishelp.The originalbearcanbe seeninfrontoftheMathematicsbuilding 
of theTUBerlin. Moreinformation on MATHEON can be found at www.matheon.de. 


300 19 TheSingularValueDecomposition 

One easily sees that 

Ir 0 
WH .Rmm 

A†A=W 

00 

If r =m =n,then Aisinvertibleandtheright hand sideoftheabove equationis 
equal to theidentity matrix In.Inthis casewehave A† = 
A.1.The matrix A† can 
thereforebe viewed as a generalized inverse,thatin the caseof aninvertible matrix 
Ais equaltotheinverseof A. 

Definition 19.6 Thematrix A† in(19.4)iscalledMoore-Penroseinverse3or pseudo-
inverse of A. 

Let A.Cnm and b.Cn 1begiven.Ifthe linear systemof equations Ax =bhas 
no solution,thenwecantrytofindan 

x .Cm 1 such that A

x is “as closeaspossible” 
to b.Using theMoore-Penroseinverse we obtainthe best possible approximation 
with respect to theEuclidean norm. 

Theorem 19.7 LetA .Cn m with n .m andb .Cn 1 be given. If A =VWH is 
an SVD, and A† is asin(19.4), then

x =A†bsatisfies 

b.A
x
2 .b.Ay2 for all y 
.Cm 1 

and 

.. 


. 
12 

2

r 
. 
H 
. 


. 
v 


. 
jb 
. 


..

x
2 = 
. 
.y2 

. 
.j . 


j=1 

for all y 
.Cm 1 with b.A
x
2 =b.Ay2. 

Proof Let y 
.Cm 1 be given and let z =[.1 .m]T :=WHy.Then 

22 22

b.Ay2 =b.VWH y2 =V(VHb.z 2 =VHb.z

2 

rn 

22 

= 
v 
Hj b..j.j. 
+ 
v 
Hj b
. 


j=1 j=r+1 

n 

2

H 

v 


. 
jb (19.5) 
j=r+1 

H

Equality holds if and onlyif . 
j = 
v 
jb . 
j for all j =1 r.Thisis satisfied 

if z =WHy 
=†VHb.The last equation holdsif and onlyif 

y 
=W†VHb=A†b=
x 

3EliakimHastings Moore(1862–1932) andSir RogerPenrose (1931–). 


19 TheSingularValueDecomposition 
301 

The vector 

x thereforeattainsthelower bound(19.5). 
The equation 

.. 


. 
12

2

r 
. 
H 
. 


. 
v 


. 
jb 
. 


.. 
. 
. 
j . 


x
2 = 
. 


j=1 

is easily checked. Every vector y 
.Cm 1 that attainsthelower bound(19.5)must 
have theform 

. 

T 

v1 
Hb v 
Hb

r 

y 
=W yr+1 ym.1 .r 

for some yr+1 ym .C,which impliesthat y2 .x
2. 
. 


Theminimizationproblem forthevector 

x can be written as 

b.A
x
2 = 
min b.Ay2 

y.Cm 1 

If 
.. 


.11 

.. 


.Rm 2

A= 
.. 


.m 1 

for(pairwisedistinct) .1 .m .R,then this minimizationproblem corresponds 
to theproblemof linear regression and theleastsquares approximationinExample 
12.16,that wehave solved with the QR-decomposition of A.If A=QR is this 
decomposition, then A† =( 
AHA .1AH (cp. Exercise 19.5) and we have 

A† =(RHQHQR .1RHQH =R.1(RH .1RHQH =R.1QH 

Thus,the solutionofthe least-squares approximationinExample 12.16is identical 
to thesolutionofthe above minimizationproblem usingthe SVDof A. 

Exercises 

19.1 Showthat theFrobenius norm and thematrix2-normare unitarily invariant, 
i.e., that 
PAQF =AF and PAQ2 =A2 for all A . 
Cnm and 
, Q.Cmm

unitary matrices P .Cnn . 

2

(Hint: Forthe Frobenius norm one can usethatAF =trace(AHA .) 

12

19.2Use theresultofExercise 19.1toshowthat AF = 
.12 ++.2 and
r 
A2 =.1, where .1 .···..r > 
0are the singular values of A.Cnm . 

2

19.3 Showthat A2 =AH2 and A2 =AHA2 for all A.Cnm . 
2

19.4 Showthat A2 .A1 A. 
for all A.Cnm . 

302 
19 TheSingularValueDecomposition 

19.5 Let 
A .Cnm and let A† be theMoore-Penroseinverse of A. Show the following 
assertions: 
.1AH

(a) If rank(A =m,then A† =(AHA . 
(b) The matrix X = 
A† is the uniquely determined matrix that satisfies the 
following four matrix equations: 
(1) AXA =A, 
(2) XAX =X, 
(3) ( 
AX H =AX, 
(4) (XAH =XA. 
19.6 Let 
.. 
.. 


21 
5 

32 
31 

.. 
..

A= 
03 .Rb= 
2 .R1.2 .5 

Computethe Moore-Penroseinverseof Aand a vector 

x .R2 1 such that 

(a) b.A
x
2 .b.Ay2 for all y 
.R2 1, and 
(b) x
2 .y2 for all y 
.R21 with b.Ay2 =b.A
x
2. 
19.7Prove thefollowing theorem: 
Let A .Cnm andB .C. 
m with m .n .. Then AHA=BHBifand only 
if B =UA for a matrix U .C. 
n with UHU =In.If AandB arereal,then 
U can also be chosen to be real. 
(Hint: One direction is trivial. For the other direction consider the unitary 
diagonalization of AHA =BHB.Thisyieldsthe matrix W in theSVD of A 
and of B. Showthe assertionusing thesetwo decompositions.Thistheorem 
and its applications can be found in the article [HorO96].) 


Chapter20 
TheKroneckerProductand LinearMatrix 
Equations 

Manyapplications,in particularthestability analysisofdifferential equations,lead 
to linear matrix equations, such as AX + 
XB = 
C.Herethe matrices A, 
B, 
C are 
given and the goal is to determine a matrix X that solves the equation(we willgive 
a formal definition below). In the description of the solutions of such equations, 
theKronecker product,1 another productof matrices,isuseful.In this chapter we 
developthe most important propertiesofthisproducts and we study its applicationin 
the context oflinear matrix equations.Manymoreresults on this topic canbe found 
in the books [HorJ91, LanT85]. 

Definition 20.1 If K is a field, A=[aij]. 
Km,m and B . 
Kn,n,then 

.. 


a11 B ··· 
a1mB 

. 
.. 
.

A. 
B :=[aijB]=
. 
.. 
. 
,

.. 


am1B ··· 
ammB 

is calledthe Kroneckerproduct of Aand B. 

TheKronecker product is sometimes calledthe tensor product of matrices.This 
product definesamap from Km,m .Kn,n to Kmn,mn.The definition canbeextended 
to non-squarematrices,but forsimplicity we consider here onlythe caseof square 
matrices.The following lemma presentsthe basic computational rulesof the Kronecker 
product. 

Lemma 20.2 Forall squarematricesA, 
B, 
CoverK, thefollowing computational 
rules hold: 

(1) A.(B.C) 
=(A. 
B) 
.C. 
1LeopoldKronecker (1832–1891) is said to have used this productinhis lectures in Berlin in the 
1880s.It was defined formally forthe first timein 1858by Johann GeorgZehfuss (1832–1901). 

©SpringerInternationalPublishing Switzerland 2015 

J. Liesen andV.Mehrmann, Linear Algebra,SpringerUndergraduate 
MathematicsSeries, DOI 10.1007/978-3-319-24346-7_20 

304 
20 TheKronecker Productand Linear Matrix Equations 

(2) 
(.A) 
.B =A.(.B) 
=.(A.B) 
for all . 
.K. 
(3) 
(A+B) 
.C =( 
A.C) 
+(B.C), whenever A+Bis defined. 
(4) 
A.(B+C) 
=( 
A.B) 
+(A.C), wheneverB+Cis defined. 
(5) 
(A . 
B)T =AT . 
BT, and thereforetheKroneckerproductof twosymmetric 
matrices is symmetric. 
Proof Exercise. 
. 


In particular,in contrasttothestandard matrixmultiplication,theorderofthe 
factorsinthe Kronecker product does not change under transposition. Thefollowing 
result describes thematrixmultiplicationoftwo Kronecker products. 

Lemma 20.3 For A, 
C .Km,m andB, 
D .Kn,n we have 

(A.B)(C .D) 
=(AC) 
.(BD). 


Hence,in particular, 

(1) 
A.B =(A.In)(Im .B) 
=(Im .B)(A.In), 
(2) 
(A.B).1 =A.1 .B.1,ifAandB areinvertible. 
Proof Since A.B =[aijB]and C .D =[cijD], the block Fij . 
Kn,n in the 
block matrix [Fij]=(A.B)(C .D) 
is givenby 

m m m 
. 
. 
. 
. 
. 


Fij = 
(aikB)(ckjD) 
= 
aikckj BD = 
aikckj BD. 


k=1 k=1 k=1 

Forthe block matrix[Gij]=( 
AC) 
.(BD) 
with Gij .Kn,n we obtain 

m 

Gij =gij BD, 
where gij = 
aikckj, 


k=1 

whichshows (A.B)(C.D) 
=(AC) 
.(BD).Now(1) 
and (2) 
easily followfrom 
this equation. . 


IngeneraltheKroneckerproductis non-commutative(cp.Exercise20.2),butwe 
have thefollowing relationship between A.B and B.A. 

Lemma 20.4 For A . 
Km,m and B . 
Kn,n there exists a permutation matrix 
P .Kmn,mn with 
PT( 
A.B)P =B.A. 


Proof Exercise. 
. 


Forthe computationofthe determinant,trace and rankofaKronecker product 
there exist simple formulas. 


20 TheKronecker Productand Linear Matrix Equations 305 

Theorem 20.5 For A .Km,m andB .Kn,n thefollowing rules hold: 

m

(1) det(A.B) 
=(det A)n (det B)=det(B.A). 
(2) trace(A.B) 
=trace(A)trace(B) 
=trace(B.A). 
(3) rank(A.B) 
=rank(A)rank(B) 
=rank(B.A). 
Proof (1)From(1)inLemma 20.3andthemultiplicationtheoremfor determinants 
(cp. Theorem 7.15)we get 

det(A.B) 
=det ((A.In)(Im .B)) 
=det(A.In) 
det(Im .B). 


ByLemma20.4 thereexistsapermutationmatrix Pwith A.In =P(In.A)PT . 
This impliesthat 

n

det(A.In) 
=det P(In .A)PT=det(In .A) 
=(det A). 


n

Since det(Im .B) 
= 
(det B)m,it then follows that det(A.B) 
= 
(det A)

(det B)m, and therefore also det(A.B) 
=det(B.A). 

(2) From (A.B) 
=[aijB]we obtain 
m n m n 
. 

. 
. 
. 
. 
. 
. 


trace(A.B) 
= 
aiibjj = 
aii bjj =trace(A)trace(B) 
i=1 j=1 i=1 j=1 

=trace(B)trace(A) 
=trace(B.A). 


(3) Exercise. . 

For a matrix A =[a1,...,an]. 
Km,n with columns aj . 
Km,1, j =1,...,n, 
we define 
.. 


a1 

..

a2 

.. 
. 
Kmn,1

vec(A) 
:=. 
. 
. 
. 


. 
. 
.

. 


an 

The applicationofvec turnsthematrix Aintoa“columnvector”andthus“vectorizes” 

A. 
Lemma 20.6 The map vec : 
Km,n > 
Kmn,1 is an isomorphism. In particular, 
A1,...,Ak .Km,n are linearly independent if and onlyifvec(A1),...,vec(Ak) 
. 
Kmn,1 are linearly independent. 

Proof Exercise. . 


We now consider the relationship between the Kronecker product and the vec 
map. 


306 20 TheKronecker Productand Linear Matrix Equations 
Hence,in particular, 
1,...,n,the jth column of ACB is givenby 
which implies that vec(ACB) 
obtain (1), while (1)and the linearityofvecyield (2). . 
In order to study the relationship between the eigenvalues of the matrices A,B and 
those of the Kronecker product A.B, we use bivariate polynomials,i.e., polynomials 
in two variables(cp.Exercise 9.10).If 

is such a polynomial, then for A. 
Km,m and B . 
Kn,n we define the matrix 

Here we have to be careful with the order of the factors, since in general Ai . 


The following result is known as Stephanos’ theorem. 2 

2 Named after Cyparissos Stephanos (1857–1917) who in 1900 showed besides this result also the 
assertion of Lemma 20.3. 


20 The Kronecker Product and Linear Matrix Equations 307 

Theorem 20.9 LetA. 
Km,m andB . 
Kn,nbe two matrices that have Jordan normal forms 
and the eigenvalues .1,...,.m . 
K and.1,...,.n . 
K, respectively. 
If p(A,B)is defined as in(20.1), then the following assertions hold: 

(1) The eigenvalues of p(A,B)are p(.k,.)fork = 
1,...,m and . 
= 
1,...,n. 
(2) The eigenvalues of A. 
Bare .k · 
.. 
fork = 
1,...,m and . 
= 
1,...,n. 
(3) The eigenvalues of A.In+Im.Bare.k+.fork = 
1,...,m and. 
= 
1,...,n. 
Proof Let S . 
GLm(K)and T . 
GLn(K)be such that S.1AS = 
JA and T.1BT = 
JB are in Jordan canonical form.The matrices JA and JB are upper triangular.Thus, 
forall i, 
j. 
N0 thematrices JAi , JBj and JAi . 
JBj are upper triangular.The eigenvalues 

jj

of JAi and JBj are .i 
1,...,.i and .1,...,.n, respectively. Thus, p(.k,.), k = 


m 
1,...,m, . 
= 
1,...,n, are the diagonal entries of the matrix p(JA,JB).Using 
Lemma 20.3 we obtain 

which implies(1). 
The assertions (2) and (3)follow from(1) with p(t1,t2) 
= 
t1t2 and p(t1,t2) 
= 
t1 + 
t2, respectively. . 


The following result on the matrix exponential function of a Kronecker product 
is helpful in applications that involve systems of linear differential equations. 

Lemma 20.10 For A. 
Cm,m,B . 
Cn,n andC := 
(A. 
In)+ 
(Im . 
B)we have 

exp(C) 
= 
exp(A). 
exp(B). 


Proof From Lemma 20.3 we know that the matrices A. 
In and Im . 
B commute. 
UsingLemma 17.6 we obtain 


308 20 The Kronecker Product and Linear Matrix Equations 
where we have used the properties of the matrix exponential series 
(cp. Sect. 17.1). . 


For given matrices Aj . 
Km,m , Bj . 
Kn,n , j = 
1,...,q, and C . 
Km,n an 
equation of the form 

A1XB1 + 
A2XB2 + 
...+ 
AqXBq = 
C (20.2) 

is called a linear matrix equation for the unknown matrix X . 
Km,n . 



Theorem 20.11 Amatrix X . 
Km,n solves(20.2)if and onlyif

x := 
vec(X) 
. 


Kmn,1 solves the linear system of equations 

q 

Gx = 
vec(C), 
where G := 
BTj . 
Aj. 
j=1 

Proof Exercise. . 


We now consider two special cases of (20.2). 

Theorem 20.12 For A. 
Cm,m,B . 
Cn,n andC . 
Cm,n the Sylvester equation3 

AX + 
XB = 
C (20.3) 

hasa unique solution if and only ifA and .B have no common eigenvalue.If all 
eigenvalues of A and B have negative real parts,then the unique solution of(20.3) 
is given by 

. 


. 


X =. 
exp(tA)Cexp(tB)dt. 


0 

(Asin Sect.17.2 the integral is defined entrywise.) 

3JamesJosephSylvester (1814–1897). 


20 The Kronecker Product and Linear Matrix Equations 309 

Proof Analogous to the representation in Theorem 20.11,we can write the Sylvester 
equation (20.3)as 

(In . 
A+ 
BT . 
Im)x = 
vec(C). 


If Aand Bhave the eigenvalues .1,...,.m and .1,...,.n, respectively, then G= 
In. 
A+ 
BT. 
Im by(3)inTheorem 20.9 has the eigenvalues .k+.,k= 

1,...,n.Thus, G is invertible, and the Sylvester equation is uniquely solvable, 

Let Aand Bbe matrices with eigenvalues that have negative real parts.Then A and 
.Bhaveno common eigenvalues and(20.3)has a unique solution. LetJA = 
S.1AS 
and JB = 
T.1BT be Jordan canonical forms of Aand B.We consider the linear 
differential equation 

thatis solvedby thefunction 

(cp.Exercise 20.10).Thisfunctionsatisfies 


constant

Integrationofequation(20.4)fromt = 
0to t =. 
yields 

(Here we use without proof the existence of the infinite integrals.)This implies that 


is the unique solutionof(20.3). . 

Theorem 20.12 also gives the solution of another important matrix equation. 
Corollary 20.13 For A,C. 
Cn,n the Lyapunov equation4 

AX + 
XAH =.C (20.5) 

4Alexandr MikhailovichLyapunov(also LjapunovorLiapunov; 1857–1918). 


310 
20 The Kronecker Product and Linear Matrix Equations 

If,furthermore,C is Hermitian positive definite,then also 


has a unique solution X . 
Cn,n if the eigenvalues of A have negative real parts. 

XisHermitian positive 
definite. 

Proof Since by assumption A and .AH have no common eigenvalues, the unique 
solvability of(20.5)follows from Theorem20.12, and the solution is given by the 
matrix 
If C is Hermitian positive definite,then X is Hermitian and for x .Cn,1 \{0}we 