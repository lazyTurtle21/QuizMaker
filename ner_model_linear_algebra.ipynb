{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "83c2Q2oRJ41d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6209cdff-e743-4e6b-cb4a-142dcd81fb9f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/linear_.txt\",'r') as file_words:\n",
        "  vocab = sorted([i.replace(\"•\",\"\").strip().lower() for i in file_words.read().split(\"\\n\") if i], key=lambda x: -len(x))\n",
        "  vocab = [[i[:i.rindex(\" \")], i[i.rindex(\" \")+1:]] for i in vocab]\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Strang-Linear Algebra.txt\",'r') as file_text:\n",
        "  book = file_text.read().lower()\n",
        "\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/linear_.txt\",'w') as file_words:\n",
        "  file_words.write(\"\\n\".join(sorted([\" \".join(i) for i in vocab])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSbdOsIvQV2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7831ded4-f1ee-435a-cebb-171df5fb67b6"
      },
      "source": [
        "vocab[:5]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['closure under addition and scalar multiplication', 'act'],\n",
              " ['negative semidefinite quadratic form', 'typ'],\n",
              " ['positive semidefinite quadratic form', 'typ'],\n",
              " ['general solution to a linear system', 'obj'],\n",
              " ['determinant test for invertibility', 'act']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wzxflq5J41m",
        "colab_type": "code",
        "outputId": "a6b87b3c-b297-4cf3-f5a7-5826a6e67f68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count = 0\n",
        "\n",
        "EXAMPLE_DATA = [\n",
        "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
        "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "data = []\n",
        "sentences = []\n",
        "\n",
        "for sentence in book.split(\".\"):  \n",
        "  sentence = sentence.replace(\"\\n\",\" \").replace(\"¤\",\"\").replace(\"§\",\"\").strip()\n",
        "  if len(sentence) > 25 and list(sentence).count(\" \") < len(sentence)/3:\n",
        "    words = []\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    taken = []\n",
        "    for word in vocab:\n",
        "      index = sentence.find(word[0])\n",
        "      if index != -1 and index not in taken and (len(sentence) <= index+len(word[0])+2 or not sentence[index+len(word[0])].isalpha() or sentence[index+len(word[0])] == \"s\") :\n",
        "        words.append([index, index+len(word[0]),word[1]])\n",
        "        taken.extend([i for i in range(index, index+len(word[0]))])\n",
        "      # elif index != -1:\n",
        "        # print(word, \"NOT THERE\")\n",
        "    if words:\n",
        "      data.append((sentence, {\"entities\": words}))\n",
        "    sentences.append(sentence)\n",
        "print(data[:1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('0 0 contents preface iv 1 matrices and gaussian elimination 1 1', {'entities': [[39, 59, 'act'], [26, 34, 'obj']]})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04XdKlr87uUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# finding other terms not in vocabulary among words in the books\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# sent = \" \".join(sentences)\n",
        "# for i in vocab:\n",
        "#   sent = sent.replace(i[0],\"\")\n",
        "\n",
        "# stop_words = set(stopwords.words('english')) \n",
        "# pos_data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in nltk.pos_tag(nltk.word_tokenize(sent)))\n",
        "# for word in pos_data.conditions():\n",
        "#   if len(word) > 3 and word.isalpha() and word not in stop_words:\n",
        "#     tags = pos_data[word].keys()\n",
        "#     print(word, ' '.join(tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQKO3VOHJ41p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "LABEL = [\"act\",\"obj\",'trm','typ']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqncQG3xJ41u",
        "colab_type": "code",
        "outputId": "a67fa4a7-8b59-45f5-d484-8239577cf8cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "TRAIN_DATA = data\n",
        "\n",
        "model=None\n",
        "new_model_name='new_model'\n",
        "output_dir=\"/content/drive/My Drive/Colab Notebooks/\"\n",
        "n_iter=10\n",
        "\n",
        "if model is not None:\n",
        "    nlp = spacy.load(model)  # load existing spaCy model\n",
        "    print(\"Loaded model '%s'\" % model)\n",
        "else:\n",
        "    nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "    print(\"Created blank 'en' model\")\n",
        "\n",
        "# create the built-in pipeline components and add them to the pipeline\n",
        "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "# otherwise, get it so we can add labels\n",
        "else:\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# add labels\n",
        "for _, annotations in TRAIN_DATA:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "# get names of other pipes to disable them during training\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    # reset and initialize the weights randomly – but only if we're\n",
        "    # training a new model\n",
        "    if model is None:\n",
        "        nlp.begin_training()\n",
        "    for itn in range(n_iter):\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(\n",
        "                texts,  # batch of texts\n",
        "                annotations,  # batch of annotations\n",
        "                drop=0.5,  # dropout - make it harder to memorise data\n",
        "                losses=losses,\n",
        "            )\n",
        "        print(\"Losses\", losses)\n",
        "\n",
        "# test the trained model\n",
        "for text, _ in TRAIN_DATA[:5]:\n",
        "    doc = nlp(text)\n",
        "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    # print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "# save model to output directory\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)\n",
        "\n",
        "    # test the saved model\n",
        "    print(\"Loading from\", output_dir)\n",
        "    nlp2 = spacy.load(output_dir)\n",
        "    # for text, _ in TRAIN_DATA:\n",
        "    #     doc = nlp2(text)\n",
        "    #     print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    #     print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'ner': 8190.428913842075}\n",
            "Losses {'ner': 4551.906466298161}\n",
            "Losses {'ner': 3882.1840589916724}\n",
            "Losses {'ner': 3431.8633731578434}\n",
            "Losses {'ner': 3218.3677229521045}\n",
            "Losses {'ner': 3077.4697634866598}\n",
            "Losses {'ner': 2836.669673738813}\n",
            "Losses {'ner': 2798.720741677354}\n",
            "Losses {'ner': 2549.4905235042165}\n",
            "Losses {'ner': 2495.484536203597}\n",
            "Entities [('nullspace', 'obj')]\n",
            "Entities []\n",
            "Entities [('plane', 'obj'), ('line', 'obj')]\n",
            "Entities [('matrix', 'obj')]\n",
            "Entities []\n",
            "Saved model to /content/drive/My Drive/Colab Notebooks\n",
            "Loading from /content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR8I3lKJJ414",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "a8e269fd-7541-411b-9079-d33adabe926c"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/test.txt\") as filee:\n",
        "    test_text = filee.read()\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities in '(One may verify this also by carrying out the matrix multiplications.) Using (1)\n",
            "and (2) we obtain\n",
            "det(Pi j A) = det \u001f\n",
            "Gi j(1)(Gi j(−1))T Gi j(1)Mj(−1)A\n",
            "\n",
            "= det(Gi j(1)) det((Gi j(−1))T ) det(Gi j(1)) det(Mj(−1)) det(A)\n",
            "= (−1) det(A). \b\t\n",
            "Since det(A) = det(AT ) (cp. (5) in Lemma 7.10), the results in Lemma 7.13 for\n",
            "the rows of A can be formulated analogously for the columns of A.\n",
            "Example 7.14 Consider the matrices\n",
            "A simple calculation shows that det(A) = −4. Since B is obtained from A by\n",
            "exchanging the first two columns we have det(B) = − det(A) = 4.\n",
            "The determinant map can be interpreted as a map of (Rn,1)n to R, i.e., as a map of\n",
            "the n columns of the matrix A ∈ Rn,n to the ring R. If ai, aj ∈ Rn,1 are two columns\n",
            "of A,\n",
            "by (3) in Lemma 7.13. Due to this property the determinant map is called an alternating map of the columns of A. Analogously, the determinant map is an alternating\n",
            "map of the rows of A.\n",
            "This property is called the linearity of the determinant map with respect to the rows\n",
            "of A. Analogously we have the linearity with respect to the columns of A. Linear\n",
            "maps will be studied in detail in later chapters.\n",
            "The next result is called the multiplication theorem for determinants.\n",
            "\n",
            "Since our proof relies on Theorem5.2, which is valid for matrices over a field\n",
            "K, we have formulated Theorem7.15 for A, B ∈ Kn,n. However, the multiplication\n",
            "theorem for determinants also holds for matrices over a commutative ring R with unit.\n",
            "A direct proof based on the signature formula of Leibniz can be found, for example,\n",
            "in the book “Advanced Linear Algebra” by Loehr [Loe14, Sect. 5.13]. That book\n",
            "also contains a proof of the Cauchy-Binet formula for det(AB) with A ∈ Rn,m and\n",
            "B ∈ Rm,n for n ≤ m. Below we will sometimes use that det(AB) = det(A) det(B)\n",
            "7.2 Properties of the Determinant 91\n",
            "holds for all A, B ∈ Rn,n, although we have shown the result in Theorem7.15 only\n",
            "for A, B ∈ Kn,n.\n",
            "The proof of Theorem7.15 suggests that det(A) can be easily computed while\n",
            "transforming A ∈ Kn,n into its echelon form using elementary row operations'\n",
            "obj matrix\n",
            "obj matrices\n",
            "obj determinant\n",
            "obj matrix\n",
            "obj property\n",
            "obj determinant\n",
            "obj determinant\n",
            "obj property\n",
            "obj determinant\n",
            "trm theorem\n",
            "obj matrices\n",
            "trm theorem\n",
            "obj matrices\n",
            "obj Determinant\n",
            "typ echelon form\n",
            "obj row\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KvOecFcJ42L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "def distractors(chapter_sentences, key, sentence):\n",
        "  candidates = []\n",
        "  cand_sents = []\n",
        "  nlp2 = spacy.load(output_dir)\n",
        "  key_nlp = nlp2(key)\n",
        "\n",
        "  label = key_nlp.ents\n",
        "\n",
        "  tfidf_vectorizer=TfidfVectorizer()\n",
        "  tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(chapter_sentences)\n",
        "  tag = tfidf_vectorizer.get_feature_names()\n",
        "  n = tfidf_vectorizer_vectors.sum(axis=0).A1\n",
        "  candidates_scores = []\n",
        "  result = dict(zip(tag,n))\n",
        "\n",
        "  key_score = sum([result[key.split()[i]] for i in range(len(key.split()))])/len(key.split())\n",
        "  print(key_score)\n",
        "  max_score = [- float(\"inf\"),\"\"]\n",
        "  key_tags = []\n",
        "\n",
        "  sent = sentence\n",
        "  context = sent[:sent.index(key)] + \".\" +sent[sent.index(key)+len(key):]\n",
        "  print(context)\n",
        "  context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
        "  print(context)\n",
        "  for word in context:\n",
        "      tag = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
        "      key_tags.append(tag)\n",
        "\n",
        "  if label:\n",
        "    key_label = label[0].label_\n",
        "  for sentence in chapter_sentences:\n",
        "    doc = nlp2(sentence)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == key_label:\n",
        "          candidates.append(ent.text)\n",
        "          cand_sents.append(sentence)\n",
        "          print(ent.label_, ent.text)\n",
        "\n",
        "  for cand, sent in zip(candidates, cand_sents):\n",
        "    if cand not in key and key not in cand:\n",
        "      sent_simil = sum([2 for i in sent if i in sentence])/(len(sentence.split())+len(sent.split())) #sentence_similarity()\n",
        "      cand_tags = 0\n",
        "      context = sent[:sent.index(cand)] + \".\" +sent[sent.index(cand)+len(cand):]\n",
        "      print(context)\n",
        "      context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
        "      print(context)\n",
        "      for word in [0,1,2,3]:\n",
        "        tag = nltk.pos_tag(nltk.word_tokenize(context[word]))[0][1]\n",
        "        cand_tags -= int(tag != key_tags[word])                                                     #context_similarity()\n",
        "\n",
        "      diff_score = (sum([result[cand.split()[i]] for i in range(len(cand.split()))])/len(cand.split()) - key_score + 1)/2   #importance_difference()\n",
        "      score = [sent_simil + cand_tags/4 - diff_score, cand]\n",
        "      max_score = max([max_score, score],key=lambda x: x[0])\n",
        "  print(max_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLjwUQJpKKn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "25d5f85e-9816-4e06-ea4a-6b8d02464c71"
      },
      "source": [
        "distractors(['this is a symmetric matrix and also a plane is here','this zero matrix can be invertible','use matrix multiplication for this problem'], 'symmetric matrix', 'this is a symmetric matrix and also a plane is here')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5278877930684712\n",
            "this is a . and also a plane is here\n",
            "['is', 'a', 'and', 'also']\n",
            "obj symmetric matrix\n",
            "obj plane\n",
            "obj matrix\n",
            "this is a symmetric matrix and also a . is here\n",
            "['also', 'a', 'is', 'here']\n",
            "[4.868090043234419, 'plane']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kStEIlsHBORC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text, _ in TRAIN_DATA[:50]:\n",
        "    doc = nlp(text)\n",
        "    print( [(ent.text, ent.label_) for ent in doc.ents], \"||||\", doc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}