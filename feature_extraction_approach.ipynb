{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = read_file('data/Strang-Linear Algebra.txt')\n",
    "chapter = OrderedDict(get_one_chapter_strang(3, book, subsections=True, \n",
    "                                      split=True, sentence_spliter=lambda ss: nlp(ss).sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Orthogonal Vectors and Subspaces', 'Cosines and Projections onto Lines ', 'Projections and Least Squares ', 'Orthogonal Bases and Gram-Schmidt ', 'The Fast Fourier Transform '])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for gap-fill question-generatable and informative sentences selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### FEATURES\n",
    "\n",
    "def get_tags(sentence):\n",
    "    return [token.tag_ for token in nlp(sentence)]\n",
    "\n",
    "def get_noun_adj_tokens(words):\n",
    "    return [token.lemma_ for token in nlp(words) \n",
    "            if token.pos_ == 'ADJ' or token.pos_ == 'NOUN']\n",
    "\n",
    "    \n",
    "# Informative \n",
    "def is_first_sentence(f, c):\n",
    "    return f == c\n",
    "\n",
    "\n",
    "def has_superlatives(curr):\n",
    "    pos_tags = get_tags(curr)\n",
    "    return 'JJR' in pos_tags or 'JJS' in pos_tags\n",
    "\n",
    "\n",
    "def has_abbreviation(curr):\n",
    "    is_abbr = lambda word: word.upper() == word and len(word) > 1\n",
    "    return any(is_abbr(x) for x in curr.split())\n",
    "\n",
    "\n",
    "def has_correct_ending(curr):\n",
    "    return curr[-1] in ['?', '.', '!']\n",
    "\n",
    "# Generative\n",
    "\n",
    "\n",
    "def relative_number_of_words(curr):\n",
    "    abs_n = abs(len(curr.split()) - 10)\n",
    "    return -abs_n if abs_n > 5 else abs_n\n",
    "\n",
    "\n",
    "def relative_index(i, doc_length):\n",
    "    abs_i = abs(i - doc_length/2)\n",
    "    return abs_i if abs_i > doc_length/4 else -abs_i\n",
    "\n",
    "\n",
    "def common_tokens_count(curr, title):\n",
    "    curr_tokens = get_noun_adj_tokens(curr)\n",
    "    title_tokens = get_noun_adj_tokens(title.lower())\n",
    "    \n",
    "    return sum([tok in curr_tokens for tok in title_tokens])\n",
    "\n",
    "\n",
    "def begins_with_discourse_connective(curr):\n",
    "    discource_connective = ['because', 'since', 'when', 'thus', \n",
    "                            'however', 'although', 'for example', \n",
    "                            'and', 'for instance', 'how', 'in other words',\n",
    "                            'therefore', 'up to this point']\n",
    "    curr = curr.lower()\n",
    "    return any(curr.startswith(x) for x in discource_connective)\n",
    "\n",
    "\n",
    "def nouns_number(curr):\n",
    "    return sum(x.pos_ == 'NOUN' for x in nlp(curr))\n",
    "\n",
    "\n",
    "def pronouns_number(curr):\n",
    "    return sum(x.pos_ == 'PRON' for x in nlp(curr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights = {\n",
    "    +4:lambda s, indx, title, first_s, doc_length:  is_first_sentence(s, first_s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_superlatives(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_abbreviation(s),\n",
    "    +.5:lambda s, indx, title, first_s, doc_length:  relative_number_of_words(s),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  common_tokens_count(s, title),\n",
    "    -2:lambda s, indx, title, first_s, doc_length:  begins_with_discourse_connective(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  nouns_number(s),\n",
    "    -2.5:lambda s, indx, title, first_s, doc_length:  pronouns_number(s),\n",
    "    +0.01:lambda s, indx, title, first_s, doc_length:  relative_index(indx, doc_length),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  has_correct_ending(s)\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def get_sentence_score(sentence, index, title, first_sentence, doc_length, weights):\n",
    "    return sum(key * weights[key](sentence, index, title, first_sentence, doc_length) \n",
    "               for key in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(sum(len(chapter[x]) for x in chapter))\n",
    "global_indx = 0\n",
    "for key in chapter:\n",
    "    document = chapter[key]\n",
    "    title = key\n",
    "    doc_length = len(document)\n",
    "    first_sentence = document[0]\n",
    "    \n",
    "    for i, sentence in enumerate(document):\n",
    "        scores[global_indx] = get_sentence_score(\n",
    "                              sentence, i, title, first_sentence, \n",
    "                              doc_length, feature_weights)\n",
    "        global_indx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = OrderedDict((key, len(chapter[key])) for key in chapter)\n",
    "\n",
    "def get_sentence_index_in_document(doc_sent_indx, docs):\n",
    "    indexes =list(docs.values())\n",
    "    i = -1\n",
    "    prev = 0\n",
    "    while doc_sent_indx >= 0:\n",
    "        i += 1\n",
    "        prev = doc_sent_indx\n",
    "        doc_sent_indx -= indexes[i]\n",
    "    doc_name = list(docs.keys())[i]\n",
    "    return doc_name, prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sentences with best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal Bases and Gram-Schmidt \n",
      "In an orthogonal basis, every vector is perpendicular to every other vector.\n",
      "16.11\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "The Gram-Schmidt process and its interpretation as a new factorization A = QR.\n",
      "13.99\n",
      "\n",
      "Orthogonal Vectors and Subspaces\n",
      "A basis is a set of independent vectors that span a space.\n",
      "13.71\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "improvement is easy: Divide each vector by its length, to make it a unit vector.\n",
      "13.08\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "1 0  reflects every point (x;y) into (y;x), its mirror image across the 45 line.\n",
      "12.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordered_scores = np.flip(np.argsort(scores))\n",
    "top_scores = ordered_scores[:5]\n",
    "\n",
    "top_sentences = []\n",
    "for s in top_scores:\n",
    "    doc_name, index = get_sentence_index_in_document(s, docs)\n",
    "    top_sentences.append((doc_name, index))\n",
    "    \n",
    "    print(doc_name)\n",
    "    print(chapter[doc_name][index])\n",
    "    print(scores[s])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: ['an orthogonal basis', 'every vector', 'every other vector'],\n",
       "             1: ['The Gram-Schmidt process',\n",
       "              'its interpretation',\n",
       "              'a new factorization',\n",
       "              'A = QR'],\n",
       "             2: ['A basis', 'a set', 'independent vectors', 'a space'],\n",
       "             3: ['improvement', 'each vector', 'its length', 'it'],\n",
       "             4: ['every point',\n",
       "              'x;y',\n",
       "              '(y;x',\n",
       "              'its mirror image',\n",
       "              'the 45 line']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "key_list = defaultdict(lambda: list())\n",
    "\n",
    "for i, (doc_name, sent_i) in enumerate(top_sentences):\n",
    "    sent = chapter[doc_name][sent_i]\n",
    "    for chunk in nlp(sent).noun_chunks:\n",
    "        key_list[i].append(chunk.text)\n",
    "\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_word(chunk):\n",
    "    importance_order = ['ADJ','NOUN', 'NUM']\n",
    "    for pos in importance_order:\n",
    "        for i in nlp(chunk):\n",
    "            if str(i.pos_) == pos:\n",
    "                return i.text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADV', 'NOUN', 'ADJ', 'SCONJ', 'NUM', 'CCONJ', 'NUM']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.pos_ for x in nlp('Even numbers such as 2 and 4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'such'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_word('Even numbers such as 2 and 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_occurance(key, title):\n",
    "    return common_tokens_count(key, title)\n",
    "\n",
    "\n",
    "def document_occurance(key, doc):\n",
    "    total = 0\n",
    "    for s in doc:\n",
    "        total += common_tokens_count(key, s)\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_depth_in_syntactic_tree(token, depth=0):\n",
    "    d = [get_depth_in_syntactic_tree(child, depth+1) for child in token.children]\n",
    "    d.append(0)\n",
    "    return max(d)\n",
    "    \n",
    "\n",
    "def depth_in_sentence(key, s):\n",
    "    most_imp = get_most_important_word(key)\n",
    "    for tok in nlp(s):\n",
    "        if str(tok.text) == most_imp:\n",
    "            return get_depth_in_syntactic_tree(tok)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_key(key_list, sentence, doc, title):\n",
    "    scores = [title_occurance((key), title)  + \\\n",
    "              document_occurance((key), doc) +\n",
    "              depth_in_sentence((key), sentence)\n",
    "                  for key in key_list]\n",
    "    return key_list[scores.index(max(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an orthogonal basis', 'every vector', 'every other vector']\n",
      "['The Gram-Schmidt process', 'its interpretation', 'a new factorization', 'A = QR']\n",
      "['A basis', 'a set', 'independent vectors', 'a space']\n",
      "['improvement', 'each vector', 'its length', 'it']\n",
      "['every point', 'x;y', '(y;x', 'its mirror image', 'the 45 line']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: 'an orthogonal basis',\n",
       "             1: 'a new factorization',\n",
       "             2: 'independent vectors',\n",
       "             3: 'each vector',\n",
       "             4: 'the 45 line'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in key_list:\n",
    "    doc = chapter[top_sentences[i][0]]\n",
    "    doc = [i.lower() for i in doc]\n",
    "    s = doc[top_sentences[i][1]]\n",
    "    print(key_list[i])\n",
    "    key_list[i] = get_best_key(key_list[i], s, doc, top_sentences[i][0])\n",
    "# #     print(key_list[i])\n",
    "#     for j in key_list[i]:\n",
    "#         dist = distractors(doc, j.lower(), s)\n",
    "#     print(\"Proposed distractor -\", dist)\n",
    "\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a LinearUp - automatically generated quiz\n",
      "\n",
      "\n",
      "In __________, every vector is perpendicular to every other vector.\n",
      "(a) an orthogonal basis\t\n",
      "(Answer:an orthogonal basis)\n",
      "\n",
      "The Gram-Schmidt process and its interpretation as __________ A = QR.\n",
      "(a) a new factorization\t\n",
      "(Answer:a new factorization)\n",
      "\n",
      "A basis is a set of __________ that span a space.\n",
      "(a) independent vectors\t\n",
      "(Answer:independent vectors)\n",
      "\n",
      "improvement is easy: Divide __________ by its length, to make it a unit vector.\n",
      "(a) each vector\t\n",
      "(Answer:each vector)\n",
      "\n",
      "1 0  reflects every point (x;y) into (y;x), its mirror image across __________.\n",
      "(a) the 45 line\t\n",
      "(Answer:the 45 line)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from quiz_generation import create_quiz\n",
    "\n",
    "n = 10\n",
    "\n",
    "questions = []\n",
    "for i, (doc_name, sentence_index) in enumerate(top_sentences[:n]):\n",
    "    s = chapter[doc_name][sentence_index]\n",
    "    questions.append([s, key_list[i]])\n",
    "\n",
    "\n",
    "print(create_quiz(questions, correct_answer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##   top_sentences = [ (name of document, index in document),...]  see above s = chapter[top_sentences[i][0]][top_sentences[i][1]]\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "##get sentence = chapter[name of document][index]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def distractors(chapter_sentences, key, sentence):\n",
    "  candidates = []\n",
    "  cand_sents = []\n",
    "  nlp2 = spacy.load('.')\n",
    "  key_nlp = nlp2(key)\n",
    "\n",
    "  label = key_nlp.ents\n",
    "\n",
    "  tfidf_vectorizer=TfidfVectorizer()\n",
    "  tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(chapter_sentences)\n",
    "  tag = tfidf_vectorizer.get_feature_names()\n",
    "  n = tfidf_vectorizer_vectors.sum(axis=0).A1\n",
    "  candidates_scores = []\n",
    "  result = dict(zip(tag,n))\n",
    "\n",
    "  key_score = sum([result[key.split()[i]] for i in range(len(key.split()))])/len(key.split())\n",
    "#   print(key_score)\n",
    "  max_score = [- float(\"inf\"),\"\"]\n",
    "  key_tags = []\n",
    "\n",
    "  sent = sentence\n",
    "  context = sent[:sent.index(key)] + \".\" +sent[sent.index(key)+len(key):]\n",
    "#   print(context)\n",
    "  context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
    "#   print(context)\n",
    "  for word in context:\n",
    "      tag = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "      key_tags.append(tag)\n",
    "\n",
    "  if label:\n",
    "    key_label = label[0].label_\n",
    "#   else:\n",
    "#     key_label = 'obj'\n",
    "  for sentence in chapter_sentences:\n",
    "    doc = nlp2(sentence)\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_ == key_label:\n",
    "          candidates.append(ent.text)\n",
    "          cand_sents.append(sentence)\n",
    "#           print(ent.label_, ent.text)\n",
    "\n",
    "  for cand, sent in zip(candidates, cand_sents):\n",
    "    if cand not in key and key not in cand:\n",
    "      sent_simil = sum([2 for i in sent if i in sentence])/(len(sentence.split())+len(sent.split())) #sentence_similarity()\n",
    "      cand_tags = 0\n",
    "      context = sent[:sent.index(cand)] + \".\" +sent[sent.index(cand)+len(cand):]\n",
    "#       print(context)\n",
    "      context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
    "#       print(context)\n",
    "      for word in [0,1,2,3]:\n",
    "        if word < len(key_tags) and word < len(context):\n",
    "            tag = nltk.pos_tag(nltk.word_tokenize(context[word]))\n",
    "#             print(tag)\n",
    "            tag = tag[0][1]\n",
    "            cand_tags -= int(tag != key_tags[word])                                                     #context_similarity()\n",
    "\n",
    "      diff_score = (sum([result[cand.split()[i]] for i in range(len(cand.split()))])/len(cand.split()) - key_score + 1)/2   #importance_difference()\n",
    "      score = [sent_simil + cand_tags/len(key_tags) - diff_score, cand]\n",
    "      max_score = max([max_score, score],key=lambda x: x[0])\n",
    "  return max_score[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plane'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors(['this is a symmetric matrix and also a plane is here',\n",
    "             'this can be an invertible matrix',\n",
    "             'use matrix multiplication for this problem'], \n",
    "            'matrix', 'this is a symmetric matrix and also a plane is here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
