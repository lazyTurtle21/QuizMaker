{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = read_file('data/Strang-Linear Algebra.txt')\n",
    "chapter = OrderedDict(get_one_chapter_strang(2, book, subsections=True, \n",
    "                                      split=True, sentence_spliter=lambda ss: nlp(ss).sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Vector Spaces and Subspaces', 'Solving Ax = 0 and Ax = b ', 'Linear Independence, Basis, and Dimension ', 'The Four Fundamental Subspaces ', 'Graphs and Networks ', 'Linear Transformations '])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for gap-fill question-generatable and informative sentences selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### FEATURES\n",
    "\n",
    "def get_tags(sentence):\n",
    "    return [token.tag_ for token in nlp(sentence)]\n",
    "\n",
    "def get_noun_adj_tokens(words):\n",
    "    return [token.lemma_ for token in nlp(words) \n",
    "            if token.pos_ == 'ADJ' or token.pos_ == 'NOUN']\n",
    "\n",
    "    \n",
    "# Informative \n",
    "def is_first_sentence(f, c):\n",
    "    return f == c\n",
    "\n",
    "\n",
    "def has_superlatives(curr):\n",
    "    pos_tags = get_tags(curr)\n",
    "    return 'JJR' in pos_tags or 'JJS' in pos_tags\n",
    "\n",
    "\n",
    "def has_abbreviation(curr):\n",
    "    is_abbr = lambda word: word.upper() == word and len(word) > 1\n",
    "    return any(is_abbr(x) for x in curr.split())\n",
    "\n",
    "\n",
    "def has_correct_ending(curr):\n",
    "    return curr[-1] in ['?', '.', '!']\n",
    "\n",
    "# Generative\n",
    "\n",
    "\n",
    "def relative_number_of_words(curr):\n",
    "    abs_n = abs(len(curr.split()) - 10)\n",
    "    return -abs_n if abs_n > 5 else abs_n\n",
    "\n",
    "\n",
    "def relative_index(i, doc_length):\n",
    "    abs_i = abs(i - doc_length/2)\n",
    "    return abs_i if abs_i > doc_length/4 else -abs_i\n",
    "\n",
    "\n",
    "def common_tokens_count(curr, title):\n",
    "    curr_tokens = get_noun_adj_tokens(curr)\n",
    "    title_tokens = get_noun_adj_tokens(title.lower())\n",
    "    \n",
    "    return sum([tok in curr_tokens for tok in title_tokens])\n",
    "\n",
    "\n",
    "def begins_with_discourse_connective(curr):\n",
    "    discource_connective = ['because', 'since', 'when', 'thus', \n",
    "                            'however', 'although', 'for example', \n",
    "                            'and', 'for instance', 'how', 'in other words',\n",
    "                            'therefore', 'up to this point']\n",
    "    curr = curr.lower()\n",
    "    return any(curr.startswith(x) for x in discource_connective)\n",
    "\n",
    "\n",
    "def nouns_number(curr):\n",
    "    return sum(x.pos_ == 'NOUN' for x in nlp(curr))\n",
    "\n",
    "\n",
    "def pronouns_number(curr):\n",
    "    return sum(x.pos_ == 'PRON' for x in nlp(curr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights = {\n",
    "    +4:lambda s, indx, title, first_s, doc_length:  is_first_sentence(s, first_s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_superlatives(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_abbreviation(s),\n",
    "    +.5:lambda s, indx, title, first_s, doc_length:  relative_number_of_words(s),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  common_tokens_count(s, title),\n",
    "    -2:lambda s, indx, title, first_s, doc_length:  begins_with_discourse_connective(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  nouns_number(s),\n",
    "    -2.5:lambda s, indx, title, first_s, doc_length:  pronouns_number(s),\n",
    "    +0.01:lambda s, indx, title, first_s, doc_length:  relative_index(indx, doc_length),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  has_correct_ending(s)\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def get_sentence_score(sentence, index, title, first_sentence, doc_length, weights):\n",
    "    return sum(key * weights[key](sentence, index, title, first_sentence, doc_length) \n",
    "               for key in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(sum(len(chapter[x]) for x in chapter))\n",
    "global_indx = 0\n",
    "for key in chapter:\n",
    "    document = chapter[key]\n",
    "    title = key\n",
    "    doc_length = len(document)\n",
    "    first_sentence = document[0]\n",
    "    \n",
    "    for i, sentence in enumerate(document):\n",
    "        scores[global_indx] = get_sentence_score(\n",
    "                              sentence, i, title, first_sentence, \n",
    "                              doc_length, feature_weights)\n",
    "        global_indx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = OrderedDict((key, len(chapter[key])) for key in chapter)\n",
    "\n",
    "def get_sentence_index_in_document(doc_sent_indx, docs):\n",
    "    indexes =list(docs.values())\n",
    "    i = -1\n",
    "    prev = 0\n",
    "    while doc_sent_indx >= 0:\n",
    "        i += 1\n",
    "        prev = doc_sent_indx\n",
    "        doc_sent_indx -= indexes[i]\n",
    "    doc_name = list(docs.keys())[i]\n",
    "    return doc_name, prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sentences with best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving Ax = 0 and Ax = b \n",
      "Chapter 1 concentrated on square invertible matrices.\n",
      "13.955\n",
      "\n",
      "Solving Ax = 0 and Ax = b \n",
      "This is the equation for the plane (in the first description of the column space).\n",
      "12.965\n",
      "\n",
      "Solving Ax = 0 and Ax = b \n",
      "That makes Ax = b solvable, so b is in the column space.\n",
      "12.945\n",
      "\n",
      "The Four Fundamental Subspaces \n",
      "The previous section dealt with definitions rather than constructions.\n",
      "12.655000000000001\n",
      "\n",
      "Solving Ax = 0 and Ax = b \n",
      "The column space of the 1 by 1 zero matrix contains only b = 0.\n",
      "12.645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordered_scores = np.flip(np.argsort(scores))\n",
    "top_scores = ordered_scores[:5]\n",
    "\n",
    "top_sentences = []\n",
    "for s in top_scores:\n",
    "    doc_name, index = get_sentence_index_in_document(s, docs)\n",
    "    top_sentences.append((doc_name, index))\n",
    "    \n",
    "    print(doc_name)\n",
    "    print(chapter[doc_name][index])\n",
    "    print(scores[s])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: ['Chapter', 'square invertible matrices'],\n",
       "             1: ['the equation',\n",
       "              'the plane',\n",
       "              'the first description',\n",
       "              'the column space'],\n",
       "             2: ['Ax', 'b', 'the column space'],\n",
       "             3: ['The previous section', 'definitions', 'constructions'],\n",
       "             4: ['The column space', 'only b']})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "key_list = defaultdict(lambda: list())\n",
    "\n",
    "for i, (doc_name, sent_i) in enumerate(top_sentences):\n",
    "    sent = chapter[doc_name][sent_i]\n",
    "    for chunk in nlp(sent).noun_chunks:\n",
    "        if check_wordset(chunk.text):\n",
    "            key_list[i].append(chunk.text)\n",
    "\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_word(chunk):\n",
    "    importance_order = ['ADJ','NOUN', 'NUM']\n",
    "    for pos in importance_order:\n",
    "        for i in nlp(chunk):\n",
    "            if str(i.pos_) == pos:\n",
    "                return i.text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADV', 'NOUN', 'ADJ', 'SCONJ', 'NUM', 'CCONJ', 'NUM']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.pos_ for x in nlp('Even numbers such as 2 and 4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'such'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_word('Even numbers such as 2 and 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_occurance(key, title):\n",
    "    return common_tokens_count(key, title)\n",
    "\n",
    "\n",
    "def document_occurance(key, doc):\n",
    "    total = 0\n",
    "    for s in doc:\n",
    "        total += common_tokens_count(key, s)\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_depth_in_syntactic_tree(token, depth=0):\n",
    "    d = [get_depth_in_syntactic_tree(child, depth+1) for child in token.children]\n",
    "    d.append(0)\n",
    "    return max(d)\n",
    "    \n",
    "\n",
    "def depth_in_sentence(key, s):\n",
    "    most_imp = get_most_important_word(key)\n",
    "    for tok in nlp(s):\n",
    "        if str(tok.text) == most_imp:\n",
    "            return get_depth_in_syntactic_tree(tok)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_key(key_list, sentence, doc, title):\n",
    "    scores = [title_occurance((key), title)  + \\\n",
    "              document_occurance((key), doc) +\n",
    "              depth_in_sentence((key), sentence)\n",
    "                  for key in key_list]\n",
    "    return key_list[scores.index(max(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', 'square invertible matrices']\n",
      "['the equation', 'the plane', 'the first description', 'the column space']\n",
      "['Ax', 'b', 'the column space']\n",
      "['The previous section', 'definitions', 'constructions']\n",
      "['The column space', 'only b']\n"
     ]
    }
   ],
   "source": [
    "for i in key_list:\n",
    "    doc = chapter[top_sentences[i][0]]\n",
    "    doc = [i.lower() for i in doc]\n",
    "    s = doc[top_sentences[i][1]]\n",
    "    print(key_list[i])\n",
    "    key_list[i] = get_best_key(key_list[i], s, doc, top_sentences[i][0])\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distractors_generation import get_distractors\n",
    "import spacy\n",
    "nlp_model = spacy.load(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in key_list:\n",
    "    doc = chapter[top_sentences[i][0]]\n",
    "    doc = [i.lower() for i in doc]\n",
    "    s = doc[top_sentences[i][1]]\n",
    "    print(s)\n",
    "    distractors = get_distractors(doc, key_list[i].lower(), s, nlp_model, 4)\n",
    "    print(distractors[0], distractors[1][1])\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_generation import create_quiz   \n",
    "\n",
    "n = 10\n",
    "\n",
    "questions = []\n",
    "for i, (doc_name, sentence_index) in enumerate(top_sentences[:n]):\n",
    "    s = chapter[doc_name][sentence_index]\n",
    "    if key_list[i]:\n",
    "        distractors = get_distractors(chapter[doc_name], key_list[i].lower(), s.lower(), nlp_model, 3)\n",
    "        questions.append([s, distractors[1][1], distractors[0]])\n",
    "print(\"\\n\")\n",
    "print(create_quiz(questions, correct_answer=False, save=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
