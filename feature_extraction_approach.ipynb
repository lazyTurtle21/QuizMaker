{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from common import *\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, encoding='utf-8', mode='r', errors='ignore') as file:\n",
    "        return file.read().strip() \n",
    "\n",
    "    \n",
    "def split_chapter(chapter):\n",
    "    return re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', chapter.replace('\\n', ' ').replace('  ', ' ').strip())\n",
    "\n",
    "    \n",
    "def get_one_chapter_strang(chapter_number, book, subsections=False):\n",
    "    s_chapter_number = str(chapter_number)\n",
    "    chapter_starts = re.search('Chapter\\s' + s_chapter_number +'\\s\\n.*\\s\\n' + s_chapter_number + '\\.1.*\\n', book)\n",
    "    chapter_ends = re.search('Chapter\\s' + str(chapter_number + 1) + '\\s\\n.*\\s\\n' + str(chapter_number + 1) + '\\.1', \n",
    "                             book[chapter_starts.start():])\n",
    "    chapter = book[chapter_starts.start():chapter_starts.start()+chapter_ends.start()]\n",
    "\n",
    "    # TODO: check if this gavnocode works\n",
    "    \n",
    "    subs_start = re.search(s_chapter_number + '\\.[0-9]+\\s.*', chapter)\n",
    "    i = 1\n",
    "    \n",
    "    while True:\n",
    "        subs_end = re.search('\\nProblem\\sSet\\s' + s_chapter_number + '\\.' + str(i) + '+\\s', chapter)\n",
    "        \n",
    "        if not subs_end:\n",
    "#             print(chapter.count('Problem Set'), chapter)\n",
    "            break\n",
    "    \n",
    "        print(subs_end.start(), chapter[subs_end.start():subs_end.end()])\n",
    "        print(subs_start.start(), chapter[subs_start.start():subs_start.end()])\n",
    "\n",
    "        subs_name = chapter[subs_start.start():subs_start.end() - 1] # exclude \\n\n",
    "\n",
    "        subs = chapter[subs_start.end():subs_end.start()]\n",
    "\n",
    "        if subsections:\n",
    "            yield subs_name[subs_name.find(' ') + 1:], split_chapter(subs)\n",
    "        \n",
    "        chapter = chapter[subs_end.end():]\n",
    "\n",
    "        subs_start = re.search('('+s_chapter_number + '\\.' + str(i) + '\\s[^0-9]* )', chapter)\n",
    "\n",
    "        print(subs_start.start(), chapter[subs_start.start():subs_start.end()])\n",
    "        print('\\n\\n\\n')\n",
    "        break\n",
    "\n",
    "#         if chapter.count('Problem Set ') == 1:  # very clever ifka, only sverhrazums top 5 understand\n",
    "#             break\n",
    "        i+= 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16707 \n",
      "Problem Set 3.1 \n",
      "26 3.1 Orthogonal Vectors and Subspaces \n",
      "3500 3.1 Orthogonal Vectors and Subspaces \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = read_file('data/Strang-Linear Algebra.txt')\n",
    "chapter = dict(get_one_chapter_strang(3, book, subsections=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Orthogonal Vectors and Subspaces'])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Orthogonal Vectors and Subspaces': ['A basis is a set of independent vectors that span a space.',\n",
       "  'Geometrically, it is a set of coordinate axes.',\n",
       "  'A vector space is defined without those axes, but every time I think of the x-y plane or three-dimensional space or Rn, the axes are there.',\n",
       "  'They are usually perpendicular! The coordinate axes that the imagination constructs are practically always orthogonal.',\n",
       "  'In choosing a basis, we tend to choose an orthogonal basis.',\n",
       "  'The idea of an orthogonal basis is one of the foundations of linear algebra.',\n",
       "  'We need a basis to convert geometric constructions into algebraic calculations, and we need an orthogonal basis to make those calculations simple.',\n",
       "  'A further specialization makes the basis just about optimal: The vectors should have length 1.',\n",
       "  'For an orthonormal basis (orthogonal unit vectors), we will find 1. the length kxk of a vector; 2. the test xTy = 0 for perpendicular vectors; and 3. how to create perpendicular vectors from linearly independent vectors.',\n",
       "  'More than just vectors, subspaces can also be perpendicular.',\n",
       "  'We will discover, so beautifully and simply that it will be a delight to see, that the fundamental subspaces meet at right angles.',\n",
       "  'Those four subspaces are perpendicular in pairs, two in Rm and two in Rn. That will complete the fundamental theorem of linear algebra.',\n",
       "  'The first step is to find the length of a vector.',\n",
       "  'It is denoted by kxk, and in two dimensions it comes from the hypotenuse of a right triangle (Figure 3.1a).',\n",
       "  'The square of the length was given a long time ago by Pythagoras: kxk2 = x2 1+x2 2.',\n",
       "  'In three-dimensional space, x = (x1;x2;x3) is the diagonal of a box (Figure 3.1b).',\n",
       "  'Its length comes from two applications of the Pythagorean formula.',\n",
       "  'The two-dimensional case takes care of (x1;x2;0) = (1;2;0) across the base.',\n",
       "  'This forms a right angle with the vertical side (0;0;x3)= (0;0;3).',\n",
       "  'The hypotenuse of the bold triangle (Pythagoras again)\\x0c 160 Chapter 3 Orthogonality b 1 2 p5 (1, 0) (0, 2) (1, 2) kxk 2 = x2 1 + x2 2 + x2 3 5 = 12 + 22 14 = 12 + 22 + 32 (a) (b) x (0, 0, 3) (1, 2, 3) has length p 14 (1, 0, 0) (0, 2, 0) (1, 2, 0) has length p 5 Figure 3.1: The length of vectors (x1;x2) and (x1;x2;x3). is the length kxk we want: Length in 3D kxk2 = 12+22+32 and kxk = q x2 1+x2 2+x2 3: The extension to x = (x1; : : : ;xn) in n dimensions is immediate.',\n",
       "  'By Pythagoras n.1 times, the length kxk in Rn is the positive square root of xTx: Length squared kxk2 = x2 1+x2 2+. . .+x2 n = xTx: (1) The sum of squares matches xTxand the length of x = (1;2;.3) is p 14: xTx = h 1 2 .3 i 2 64 1 2 .3 3 75 = 12+22+(.3)2 = 14: Orthogonal Vectors How can we decide whether two vectors x and y are perpendicular?',\n",
       "  'What is the test for orthogonality in Figure 3.2?',\n",
       "  'In the plane spanned by x and y, those vectors are orthogonal provided they form a right triangle.',\n",
       "  'We go back to a2+b2 = c2: Sides of a right triangle kxk2+kyk2 = kx.yk2: (2) Applying the length formula (1), this test for orthogonality in Rn becomes . x2 1+. . .+x2 n . + . y21 +. . .+y2n . = (x1.y1)2+. . .+(xn.yn)2: The right-hand side has an extra .2xiyi from each (xi.yi)2: right-hand side = . x2 1+. . .+x2 n . .2(x1y1+. . .+xnyn)+ . y21 +. . .+y2n . :\\x0c 3.1 Orthogonal Vectors and Subspaces 161 y = \\x14.1 2 \\x15 x = \\x144 2 \\x15 p 25 p 20 p 5 xTy = 0 b Right angle xTy = 0 greater than 90 xTy < 0 less than 90 xTy > 0 Figure 3.2: A right triangle with 5+20 = 25.',\n",
       "  'Dotted angle 100, dashed angle 30.',\n",
       "  'We have a right triangle when that sum of cross-product terms xiyi is zero: Orthogonal vectors xTy = x1y1+. . .+xnyn = 0: (3) This sum is xTy = axiyi = yTx, the row vector xT times the column vector y: Inner product xTy = h x1 . . . xn i 2 64 y1 ... yn 3 75 = x1y1+. . .+xnyn: (4) This number is sometimes called the scalar product or dot product, and denoted by (x;y) or x . y.',\n",
       "  'We will use the name inner product and keep the notation xTy. 3A The inner product xTy is zero if and only if x and y are orthogonal vectors.',\n",
       "  'If xTy > 0, their angle is less than 90.',\n",
       "  'If xTy < 0, their angle is greater than 90.',\n",
       "  'The length squared is the inner product of x with itself: xTx = x2 1+. . .+x2 n = kxk2.',\n",
       "  'The only vector with length zerothe only vector orthogonal to itselfis the zero vector.',\n",
       "  'This vector x = 0 is orthogonal to every vector in Rn. Example 1. (2;2;.1) is orthogonal to (.1;2;2).',\n",
       "  'Both have length p 4+4+1 = 3.',\n",
       "  'Useful fact: If nonzero vectors v1; : : : ;vk are mutually orthogonal (every vector is perpendicular to every other), then those vectors are linearly independent.',\n",
       "  'Proof.',\n",
       "  'Suppose c1v1 +. . .+ckvk = 0.',\n",
       "  'To show that c1 must be zero, take the inner product of both sides with v1.',\n",
       "  'Orthogonality of the vs leaves only one term: vT1 (c1v1+. . .+ckvk) = c1vT1 v1 = 0: (5) The vectors are nonzero, so vT1 v1 6= 0 and therefore c1 = 0.',\n",
       "  'The same is true of every ci.',\n",
       "  'The only combination of the vs producing zero has all ci = 0: independence! The coordinate vectors e1; : : : ;en in Rn are the most important orthogonal vectors.',\n",
       "  'Those are the columns of the identity matrix.',\n",
       "  'They form the simplest basis for Rn, and\\x0c 162 Chapter 3 Orthogonality they are unit vectorseach has length keik=1.',\n",
       "  'They point along the coordinate axes.',\n",
       "  'If these axes are rotated, the result is a new orthonormal basis: a new system of mutually orthogonal unit vectors.',\n",
       "  'In R2 we have cos2q +sin2q = 1: Orthonormal vectors in R2 v1 = (cosq ; sinq ) and v2 = (.sinq ;cosq ): Orthogonal Subspaces We come to the orthogonality of two subspaces.',\n",
       "  'Every vector in one subspace must be orthogonal to every vector in the other subspace.',\n",
       "  'Subspaces of R3 can have dimension 0, 1, 2, or 3.',\n",
       "  'The subspaces are represented by lines or planes through the origin and in the extreme cases, by the origin alone or the whole space.',\n",
       "  'The subspace f0g is orthogonal to all subspaces.',\n",
       "  'A line can be orthogonal to another line, or it can be orthogonal to a plane, but a plane cannot be orthogonal to a plane.',\n",
       "  'I have to admit that the front wall and side wall of a room look like perpendicular planes in R3. But by our definition, that is not so! There are lines v and w in the front and side walls that do not meet at a right angle.',\n",
       "  'The line along the corner is in both walls, and it is certainly not orthogonal to itself. 3B Two subspaces V and W of the same space Rn are orthogonal if every vector v in V is orthogonal to every vector w in W: vTw = 0 for all v and w.',\n",
       "  'Example 2.',\n",
       "  'Suppose V is the plane spanned by v1 = (1;0;0;0) and v2 = (1;1;0;0).',\n",
       "  'If W is the line spanned by w = (0;0;4;5), then w is orthogonal to both vs.',\n",
       "  'The line W will be orthogonal to the whole plane V.',\n",
       "  'In this case, with subspaces of dimension 2 and 1 in R4, there is room for a third subspace.',\n",
       "  'The line L through z = (0;0;5;.4) is perpendicular to V and W.',\n",
       "  'Then the dimensions add to 2+1+1 = 4.',\n",
       "  'What space is perpendicular to all of V, W, and L?',\n",
       "  'The important orthogonal subspaces dont come by accident, and they come two at a time.',\n",
       "  'In fact orthogonal subspaces are unavoidable: They are the fundamental subspaces! The first pair is the nullspace and row space.',\n",
       "  'Those are subspaces of Rnthe rows have n components and so does the vector x in Ax = 0.',\n",
       "  'We have to show, using Ax = 0, that the rows of A are orthogonal to the nullspace vector x. 3C Fundamental theorem of orthogonality The row space is orthogonal to the nullspace (in Rn).',\n",
       "  'The column space is orthogonal to the left nullspace (in Rm).',\n",
       "  'First Proof.',\n",
       "  'Suppose x is a vector in the nullspace.',\n",
       "  'Then Ax = 0, and this system of m\\x0c 3.1 Orthogonal Vectors and Subspaces 163 equations can be written out as rows of A multiplying x: Every row is orthogonal to x Ax = 2 6664 . . . row 1 . . . . . . row 2 . . . ... ... ... . . . row m . . . 3 7775 2 6664 x1 x2 ... xn 3 7775 = 2 6664 0 0... 0 3 7775 : (6) The main point is already in the first equation: row 1 is orthogonal to x.',\n",
       "  'Their inner product is zero; that is equation 1.',\n",
       "  'Every right-hand side is zero, so x is orthogonal to every row.',\n",
       "  'Therefore x is orthogonal to every combination of the rows.',\n",
       "  'Each x in the nullspace is orthogonal to each vector in the row space, so N(A)?C(AT). The other pair of orthogonal subspaces comes from ATy = 0, or yTA = 0: yTA = h y1 . . . ym i 2 6666664 c c o o l l u . . . u m m n n 1 n 3 7777775 = h 0 . . . 0 i : (7) The vector y is orthogonal to every column.',\n",
       "  'The equation says so, from the zeros on the right-hand side.',\n",
       "  'Therefore y is orthogonal to every combination of the columns.',\n",
       "  'It is orthogonal to the column space, and it is a typical vector in the left nullspace: N(AT)?C(A). This is the same as the first half of the theorem, with A replaced by AT. Second Proof.',\n",
       "  'The contrast with this coordinate-free proof should be useful to the reader.',\n",
       "  'It shows a more abstract method of reasoning.',\n",
       "  'I wish I knew which proof is clearer, and more permanently understood.',\n",
       "  'If x is in the nullspace then Ax = 0.',\n",
       "  'If v is in the row space, it is a combination of the rows: v = ATz for some vector z.',\n",
       "  'Now, in one line: Nullspace ?',\n",
       "  'Row space vTx = (ATz)Tx = zTAx = zT0 = 0: (8) Example 3.',\n",
       "  'Suppose A has rank 1, so its row space and column space are lines: Rank-1 matrix A = 2 64 1 3 2 6 3 9 3 75 : The rows are multiples of (1;3).',\n",
       "  'The nullspace contains x=(.3;1), which is orthogonal to all the rows.',\n",
       "  'The nullspace and row space are perpendicular lines in R2: h 1 3 i\" 3 .1 # = 0 and h 2 6 i\" 3 .1 # = 0 and h 3 9 i\" 3 .1 # = 0:\\x0c 164 Chapter 3 Orthogonality In contrast, the other two subspaces are in R3. The column space is the line through (1;2;3).',\n",
       "  'The left nullspace must be the perpendicular plane y1 +2y2 +3y3 = 0.',\n",
       "  'That equation is exactly the content of yTA = 0.',\n",
       "  'The first two subspaces (the two lines) had dimensions 1+1 = 2 in the space R2. The second pair (line and plane) had dimensions 1+2 = 3 in the space R3. In general, the row space and nullspace have dimensions that add to r+(n.r) = n.',\n",
       "  'The other pair adds to r+(m.r) = m.',\n",
       "  'Something more than orthogonality is occurring, and I have to ask your patience about that one further point: the dimensions.',\n",
       "  'It is certainly true that the null space is perpendicular to the row spacebut it is not the whole truth.',\n",
       "  'N(A) contains every vector orthogonal to the row space.',\n",
       "  'The nullspace was formed from all solutions to Ax = 0.',\n",
       "  'Definition.',\n",
       "  'Given a subspace V of Rn, the space of all vectors orthogonal to V is called the orthogonal complement of V.',\n",
       "  'It is denoted by V? = V perp.',\n",
       "  'Using this terminology, the nullspace is the orthogonal complement of the row space: N(A) = (C(AT))?.',\n",
       "  'At the same time, the row space contains all vectors that are orthogonal to the nullspace.',\n",
       "  'A vector z cant be orthogonal to the nullspace but outside the row space.',\n",
       "  'Adding z as an extra row of A would enlarge the row space, but we know that there is a fixed formula r+(n.r) = n: Dimension formula dim(row space)+dim(nullspace) = number of columns.',\n",
       "  'Every vector orthogonal to the nullspace is in the row space: C(AT) = (N(A))?.',\n",
       "  'The same reasoning applied to AT produces the dual result: The left nullspace N(AT) and the column space C(A) are orthogonal complements.',\n",
       "  'Their dimensions add up to (m.r)+r = m, This completes the second half of the fundamental theorem of linear algebra.',\n",
       "  'The first half gave the dimensions of the four subspaces. including the fact that row rank = column rank.',\n",
       "  'Now we know that those subspaces are perpendicular.',\n",
       "  'More than that, the subspaces are orthogonal complements. 3D Fundamental Theorem of Linear Algebra, Part II The nullspace is the orthogonal complement of the row space in Rn. The left nullspace is the orthogonal complement of the column space in Rm. To repeat, the row space contains everything orthogonal to the nullspace.',\n",
       "  'The column space contains everything orthogonal to the left nullspace.',\n",
       "  'That is just a sentence, hidden in the middle of the book, but it decides exactly which equations can be solved! Looked at directly, Ax = b requires b to be in the column space.',\n",
       "  'Looked at indirectly.',\n",
       "  'Ax = b requires b to be perpendicular to the left nullspace. 3E Ax = b is solvable if and only if yTb = 0 whenever yTA = 0.\\x0c 3.1 Orthogonal Vectors and Subspaces 165 The direct approach was b must be a combination of the columns.',\n",
       "  'The indirect approach is b must be orthogonal to every vector that is orthogonal to the columns.',\n",
       "  'That doesnt sound like an improvement (to put it mildly).',\n",
       "  'But if only one or two vectors are orthogonal to the columns. it is much easier to check those one or two conditions yTb = 0.',\n",
       "  'A good example is Kirchhoffs Voltage Law in Section 2.5.',\n",
       "  'Testing for zero around loops is much easier than recognizing combinations of the columns.',\n",
       "  'When the left-hand sides of Ax = b add to zero, the right-hand sides must, too: x1.x2 = b1 x2.x3 = b2 x3.x1 = b3 is solvable if and only if b1+b2+b3 = 0: Here A = 2 64 1 .1 0 0 1 .1 .1 0 1 3 75 : This test b1 +b2 +b3 = 0 makes b orthogonal to y = (1;1;1) in the left nullspace.',\n",
       "  'By the Fundamental Theorem, b is a combination of the columns! The Matrix and the Subspaces We emphasize that V and W can be orthogonal without being complements.',\n",
       "  'Their dimensions can be too small.',\n",
       "  'The line V spanned by (0;1;0) is orthogonal to the line W spanned by (0;0;1), but V is not W?. The orthogonal complement of W is a twodimensional plane, and the line is only part of W?. When the dimensions are right, orthogonal subspaces are necessarily orthogonal complements: If W= V? then V =W? and dimV+dimW= n: In other words V?? = V.',\n",
       "  'The dimensions of V and W are right, and the whole space Rn is being decomposed into two perpendicular parts (Figure 3.3).',\n",
       "  'W V Two orthogonal axes in R3 Not orthogonal complements W V Line W perpendicular to plane V Orthogonal complements V =W?',\n",
       "  'Figure 3.3: Orthogonal complements in R3: a plane and a line (not two lines).',\n",
       "  'Splitting Rn into orthogonal parts will split every vector into x = v+w.',\n",
       "  'The vector v is the projection onto the subspace V.',\n",
       "  'The orthogonal component w is the projection of x onto W.',\n",
       "  'The next sections show how to find those projections of x.',\n",
       "  'They lead to what is probably the most important figure in the book (Figure 3.4).',\n",
       "  'Figure 3.4 summarizes the fundamental theorem of linear algebra.',\n",
       "  'It illustrates the true effect of a matrixwhat is happening inside the multiplication Ax. The nullspace\\x0c 166 Chapter 3 Orthogonality Figure 3.4: The true action Ax = A(xrow+xnull) of any m by n matrix. is carried to the zero vector.',\n",
       "  'Every Ax is in the column space.',\n",
       "  'Nothing is carried to the left nullspace.',\n",
       "  'The real action is between the row space and column space, and you see it by looking at a typical vector x.',\n",
       "  'It has a row space component and a nullspace component, with x = xr+xn.',\n",
       "  'When multiplied by A, this is Ax = Axr+Axn: The nullspace component goes to zero: Axn = 0.',\n",
       "  'The row space component goes to the column space: Axr = Ax. Of course everything goes to the column spacethe matrix cannot do anything else.',\n",
       "  'I tried to make the row and column spaces the same size, with equal dimension r. 3F From the row space to the column space, A is actually invertible.',\n",
       "  'Every vector b in the column space comes from exactly one vector xr in the row space.',\n",
       "  'Proof.',\n",
       "  'Every b in the column space is a combination Ax of the columns.',\n",
       "  'In fact, b is Axr, with xr in the row space, since the nullspace component gives Axn = 0, If another vector x0r in the row space gives Ax0r = b, then A(xr .x0r ) = b.b = 0.',\n",
       "  'This puts xr .x0r in the nullspace and the row space, which makes it orthogonal to itself.',\n",
       "  'Therefore it is zero, and xr.x0r .',\n",
       "  'Exactly one vector in the row space is carried to b.',\n",
       "  'Every matrix transforms its row space onto its column space.',\n",
       "  'On those r-dimensional spaces A is invertible.',\n",
       "  'On its nullspace A is zero.',\n",
       "  'When A is diagonal, you see the invertible submatrix holding the r nonzeros.',\n",
       "  'AT goes in the opposite direction, from Rm to Rn and from C(A) back to C(AT). Of course the transpose is not the inverse! AT moves the spaces correctly, but not the\\x0c 3.1 Orthogonal Vectors and Subspaces 167 individual vectors.',\n",
       "  'That honor belongs to A.1 if it existsand it only exists if r =m=n.',\n",
       "  'We cannot ask A.1 to bring back a whole nullspace out of the zero vector.',\n",
       "  'When A.1 fails to exist, the best substitute is the pseudoinverse A+. This inverts A where that is possible: A+Ax = x for x in the row space.',\n",
       "  'On the left nullspace, nothing can be done: A+y = 0.',\n",
       "  'Thus A+ inverts A where it is invertible, and has the same rank r.',\n",
       "  'One formula for A+ depends on the singular value decompositionfor which we first need to know about eigenvalues.']}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['191 3.',\n",
       " 'Solve Ax = b by least squares, and find p = Abx if A = 2 64 1 0 0 1 1 1 3 75 ; b = 2 64 1 1 0 3 75 : Verify that the error b. p is perpendicular to the columns of A. 4.',\n",
       " 'Write out E2 = kAx.bk2 and set to zero its derivatives with respect to u and v, if A = 2 64 1 0 0 1 1 1 3 75 ; x = \" u v # ; b = 2 64 1 3 4 3 75 : Compare the resulting equations with ATAbx = ATb, confirming that calculus as well as geometry gives the normal equations.',\n",
       " 'Find the solution bx and the projection p = Abx.',\n",
       " 'Why is p = b? 5.',\n",
       " 'The following system has no solution: Ax = 2 64 1 .1 1 0 1 1 3 75 \" C D # = 2 64 4 5 9 3 75 = b: Sketch and solve a straight-line fit that leads to the minimization of the quadratic (C.D.4)2+(C.5)2+(C+D.9)2?',\n",
       " 'What is the projection of b onto the column space of A? 6.',\n",
       " 'Find the projection of b onto the column space of A: A = 2 64 1 1 1 .1 .2 4 3 75 ; b = 2 64 1 2 7 3 75 : Split b into p+q, with p in the column space and q perpendicular to that space.',\n",
       " 'Which of the four subspaces contains q? 7.',\n",
       " 'Find the projection matrix P onto the space spanned by a1 = (1;0;1) and a2 = (1;1;.1). 8.',\n",
       " 'If P is the projection matrix onto a k-dimensional subspace S of the whole space Rn, what is the column space of P and what is its rank? 9. (a) If P = PTP, show that P is a projection matrix. (b) What subspace does the matrix P = 0 project onto? 10.',\n",
       " 'If the vectors a1, a2, and b are orthogonal, what are ATA and ATb? What is the projection of b onto the plane of a1 and a2?\\x0c 192 Chapter 3 Orthogonality 11.',\n",
       " 'Suppose P is the projection matrix onto the subspace S and Q is the projection onto the orthogonal complement S?. What are P+Q and PQ? Show that P.Q is its own inverse. 12.',\n",
       " 'If V is the subspace spanned by (1;1;0;1) and (0;0;1;0), find (a) a basis for the orthogonal complement V?. (b) the projection matrix P onto V. (c) the vector in V closest to the vector b = (0;1;0;.1) in V?. 13.',\n",
       " 'Find the best straight-line fit (least squares) to the measurements b = 4 at t = .2, b = 3 at t = .1, b = 1 at t = 0, b = 0 at t = 2.',\n",
       " 'Then find the projection of b = (4;3;1;0) onto the column space of A = 2 6664 1 .2 1 .1 1 0 1 2 3 7775 : 14.',\n",
       " 'The vectors a1 = (1;1;0) and a2 = (1;1;1) span a plane in R3. Find the projection matrix P onto the plane, and find a nonzero vector b that is projected to zero. 15.',\n",
       " 'If P is the projection matrix onto a line in the x-y plane, draw a figure to describe the effect of the reflection matrix H = I .2P.',\n",
       " 'Explain both geometrically and algebraically why H2 = I. 16.',\n",
       " 'Show that if u has unit length, then the rank-1 matrix P = uuT is a projection matrix: It has properties (i) and (ii) in 3N.',\n",
       " 'By choosing u = a=kak, P becomes the projection onto the line through a, and Pb is the point p = bxa.',\n",
       " 'Rank-1 projections correspond exactly to least-squares problems in one unknown. 17.',\n",
       " 'What 2 by 2 matrix projects the x-y plane onto the .45 line x+y = 0? 18.',\n",
       " 'We want to fit a plane y =C+Dt +Ez to the four points y = 3 at t = 1; z = 1 y = 6 at t = 0; z = 3 y = 5 at t = 2; z = 1 y = 0 at t = 0; z = 0. (a) Find 4 equations in 3 unknowns to pass a plane through the points (if there is such a plane). (b) Find 3 equations in 3 unknowns for the best least-squares solution. 19.',\n",
       " 'If PC = A(ATA).1AT is the projection onto the column space of A, what is the projection PR onto the row space? (It is not PT C !)\\x0c 3.3 Projections and Least Squares 193 20.',\n",
       " 'If P is the projection onto the column space of A, what is the projection onto the left nullspace? 21.',\n",
       " 'Suppose L1 is the line through the origin in the direction of a1 and L2 is the line through b in the direction of a2.',\n",
       " 'To find the closest points x1a1 and b+x2a2 on the two lines, write the two equations for the x1 and x2 that minimize kx1a1.x2a2.bk.',\n",
       " 'Solve for x if a1 = (1;1;0), a2 = (0;1;0), b = (2;1;4). 22.',\n",
       " 'Find the best line C+Dt to fit b = 4;2;.1;0;0 at times t = .2;.1;0;1;2. 23.',\n",
       " 'Show that the best least-squares fit to a set of measurements y1; : : : ;ym by a horizontal line (a constant function y =C) is their average C = y1+. . .+ym m : 24.',\n",
       " 'Find the best straight-line fit to the following measurements, and sketch your solution: y = 2 at t = .1, y = 0 at t = 0, y = .3 at t = 1, y = .5 at t = 2. 25.',\n",
       " 'Suppose that instead of a straight line, we fit the data in Problem 24 by a parabola: y =C+Dt +Et2.',\n",
       " 'In the inconsistent system Ax = b that comes from the four measurements, what are the coefficient matrix A, the unknown vector x, and the data vector b?',\n",
       " 'You need not compute bx. 26.',\n",
       " 'A Middle-Aged man was stretched on a rack to lengths L = 5, 6, and 7 feet under applied forces of F = 1, 2, and 4 tons.',\n",
       " 'Assuming Hookes law L = a+bF, find his normal length a by least squares.',\n",
       " 'Problems 2731 introduce basic ideas of statisticsthe foundation for least squares. 27. (Recommended) This problem projects b = (b1; : : : ;bm) onto the line through a = (1; : : : ;1).',\n",
       " 'We solve m equations ax = b in 1 unknown (by least squares). (a) Solve aTabx = aTb to show that is the mean (the average) of the bs, (b) Find e = b.abx, the variance kek2, and the standard deviation kek. (c) The horizontal linebb = 3 is closest to b = (1;2;6), Check that p = (3;3;3) is perpendicular to e and find the projection matrix P. 28.',\n",
       " 'First assumption behind least squares: Each measurement error has mean zero.',\n",
       " 'Multiply the 8 error vectors b.Ax = (1;1;1) by (ATA).1AT to show that the 8 vectors bx.x also average to zero.',\n",
       " 'The estimate bx is unbiased.\\x0c 194 Chapter 3 Orthogonality 29.',\n",
       " 'Second assumption behind least squares: The m errors ei are independent with variance s 2, so the average of (b.Ax)(b.Ax)T is s 2I.',\n",
       " 'Multiply on the left by (ATA).1AT and on the right by A(ATA).1 to show that the average of (bx.x)(bx.x)T is s 2(ATA).1.',\n",
       " 'This is the all-important covariance matrix for the error in bx. 30.',\n",
       " 'A doctor takes four readings of your heart rate.',\n",
       " 'The best solution to x=b1; : : : ;x=b4 is the average bx of b1; : : : ;b4.',\n",
       " 'The matrix A is a column of 1s.',\n",
       " 'Problem 29 gives the expected error (bx.x)2 as s 2(ATA).1 = .',\n",
       " 'By averaging, the variance drops from s 2 to s 2=4. 31.',\n",
       " 'If you know the average bx9 of 9 numbers b1; : : : ;b9, how can you quickly find the average bx10 with one more number b10?',\n",
       " 'The idea of recursive least squares is to avoid adding 10 numbers.',\n",
       " 'What coefficient of bx9 correctly gives bx10? bx10 = 1 10 bb 10+ bx9 = 1 10(b1+. . .+b10): Problems 3237 use four points b = (0;8;8;20) to bring out more ideas. 32.',\n",
       " 'With b=0;8;8;20 at t =0;1;3;4, set up and solve the normal equations ATAbx=ATb. For the best straight line as in Figure 3.9a, find its four heights pi and four errors ei.',\n",
       " 'What is the minimum value E2 = e21 +e22 +e23 +e24 ? 33. (Line C+Dt does go through ps) With b = 0;8;8;20 at times t = 0;1;3;4, write the four equations Ax = b (unsolvable).',\n",
       " 'Change the measurements to p = 1;5;13;17 and find an exact solution to Abx = p. 34.',\n",
       " 'Check that e = b. p = (.1;3;.5;3) is perpendicular to both columns of A.',\n",
       " 'What is the shortest distance kek from b to the column space of A? 35.',\n",
       " 'For the closest parabola b =C+Dt +Et2 to the same four points, write the unsolvable equations Ax = b in three unknowns x = (C;D;E). Set up the three normal equations ATAbx = ATb (solution not required).',\n",
       " 'You are now fitting a parabola to four pointswhat is happening in Figure 3.9b? 36.',\n",
       " 'For the closest cubic b =C+Dt +Et2+Ft3 to the same four points, write the four equations Ax = b.',\n",
       " 'Solve them by elimination, This cubic now goes exactly through the points.',\n",
       " 'What are p and e? 37.',\n",
       " 'The average of the four times isbt = 14 (0+1+3+4) = 2.',\n",
       " 'The average of the four bs isbb = 14 (0+8+8+20) = 9. (a) Verify that the best line goes through the center point (bt ;bb ) = (2;9). (b) Explain why C+Dbt =bb comes from the first equation in ATAbx = ATb. 38.',\n",
       " 'What happens to the weighted average bxW = (w21 b1 +w22 b2)=(w21 +w22 ) if the first weight w1 approaches zero?',\n",
       " 'The measurement b1 is totally unreliable.\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 195 39.',\n",
       " 'From m independent measurements b1; : : : ;bm of your pulse rate, weighted by w1; : : : ;wm, what is the weighted average that replaces equation (9)?',\n",
       " 'It is the best estimate when the statistical variances are s 2 i . 1=w2i . 40.',\n",
       " 'IfW = . 2 0 0 1  , find theW-inner product of x = (2;3) and y = (1;1), and theW-length of x.',\n",
       " 'What line of vectors isW-perpendicular to y? 41.',\n",
       " 'Find the weighted least-squares solution bxW to Ax = b: A = 2 64 1 0 1 1 1 2 3 75 b = 2 64 0 1 1 3 75 W = 2 64 2 0 0 0 1 0 0 0 1 3 75 : Check that the projection AbxW is still perpendicular (in the W-inner product!) to the error b.AbxW. 42. (a) Suppose you guess your professors age, making errors e = .2;.1;5 with probabilities 12 ;14 ;14 .',\n",
       " 'Check that the expected error E(e) is zero and find the variance E(e2). (b) If the professor guesses too (or tries to remember), making errors .1, 0, 1 with probabilities 18 ;68 ;18 , what weights w1 and w2 give the reliability of your guess and the professors guess? 3.4 Orthogonal Bases and Gram-Schmidt In an orthogonal basis, every vector is perpendicular to every other vector.',\n",
       " 'The coordinate axes are mutually orthogonal.',\n",
       " 'That is just about optimal, and the one possible improvement is easy: Divide each vector by its length, to make it a unit vector.',\n",
       " 'That changes an orthogonal basis into an orthonormal basis of qs: 3P The vectors q1; : : : ;qn are orthonormal if qTi qj = . 0 whenever i 6= j; giving the orthogonality; 1 whenever i = j; giving the normalization.',\n",
       " 'A matrix with orthonormal columns will be called Q.',\n",
       " 'The most important example is the standard basis.',\n",
       " 'For the x-y plane, the best-known axes e1 = (1;0) and e2 = (0;1) are not only perpendicular but horizontal and vertical.',\n",
       " 'Q is the 2 by 2 identity matrix.',\n",
       " 'In n dimensions the standard basis e1; : : : ;en again consists\\x0c 196 Chapter 3 Orthogonality of the columns of Q = I: Standard basis e1 = 2 6666664 1 0 0... 0 3 7777775 ; e2 = 2 6666664 0 1 0... 0 3 7777775 ; . . . ; en = 2 6666664 0 0 0... 1 3 7777775 : That is not the only orthonormal basis! We can rotate the axes without changing the right angles at which they meet.',\n",
       " 'These rotation matrices will be examples of Q.',\n",
       " 'If we have a subspace of Rn, the standard vectors ei might not lie in that subspace.',\n",
       " 'But the subspace always has an orthonormal basis, and it can be constructed in a simple way out of any basis whatsoever.',\n",
       " 'This construction, which converts a skewed set of axes into a perpendicular set, is known as Gram-Schmidt orthogonalization.',\n",
       " 'To summarize, the three topics basic to this section are: 1.',\n",
       " 'The definition and properties of orthogonal matrices Q. 2.',\n",
       " 'The solution of Qx = b, either n by n or rectangular (least squares). 3.',\n",
       " 'The Gram-Schmidt process and its interpretation as a new factorization A = QR. Orthogonal Matrices 3Q If Q (square or rectangular) has orthonormal columns, then QTQ = I: Orthonormal columns 2 6664  qT1  qT2  ...  qTn 3 7775 2 64 j j j q1 q2 . . . qn j j j 3 75 = 2 6664 1 0 . 0 0 1 . 0 . . . . 0 0 . 1 3 7775 = I: (1) An orthogonal matrix is a square matrix with orthonormal columns.2 Then QT is Q.1.',\n",
       " 'For square orthogonal matrices, the transpose is the inverse.',\n",
       " 'When row i of QT multiplies column j of Q, the result is qTj qj = 0.',\n",
       " 'On the diagonal where i = j, we have qTi qi = 1.',\n",
       " 'That is the normalization to unit vectors of length 1.',\n",
       " 'Note that QTQ = I even if Q is rectangular.',\n",
       " 'But then QT is only a left-inverse.',\n",
       " 'Example 1.',\n",
       " 'Q = \" cosq .sinq sinq cosq # ; QT = Q.1 = \" cosq sinq .sinq cosq # : 2Orthonormal matrix would have been a better name, but it is too late to change.',\n",
       " 'Also, there is no accepted word for a rectangular matrix with orthonormal columns.',\n",
       " 'We still write Q, but we wont call it an orthogonal matrix unless it is square.\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 197 Q rotates every vector through the angle q , and QT rotates it back through .q .',\n",
       " 'The columns are clearly orthogonal, and they are orthonormal because sin2q +cos2q = 1.',\n",
       " 'The matrix QT is just as much an orthogonal matrix as Q.',\n",
       " 'Example 2.',\n",
       " 'Any permutation matrix P is an orthogonal matrix.',\n",
       " 'The columns are certainly unit vectors and certainly orthogonalbecause the 1 appears in a different place in each column: The transpose is the inverse.',\n",
       " 'If P = 2 64 0 1 0 0 0 1 1 0 0 3 75 then P.1 = PT = 2 64 0 0 1 1 0 0 0 1 0 3 75 : An anti-diagonal P, with P13 =P22 =P31 =I, takes the x-y-z axes into the z-y-x axes a right-handed system into a left-handed system.',\n",
       " 'So we were wrong if we suggested that every orthogonal Q represents a rotation.',\n",
       " 'A reflection is also allowed.',\n",
       " 'P = . 0 1 1 0  reflects every point (x;y) into (y;x), its mirror image across the 45 line.',\n",
       " 'Geometrically, an orthogonal Q is the product of a rotation and a reflection.',\n",
       " 'There does remain one property that is shared by rotations and reflections, and in fact by every orthogonal matrix.',\n",
       " 'It is not shared by projections, which are not orthogonal or even invertible.',\n",
       " 'Projections reduce the length of a vector, whereas orthogonal matrices have a property that is the most important and most characteristic of all: 3R Multiplication by any Q preserves lengths: Lengths unchanged kQxk = kxk for every vector x: (2) It also preserves inner products and angles, since (Qx)T(Qy)=xTQTQy=xTy. The preservation of lengths comes directly from QTQ = I: kQxk2 = kxk2 because (Qx)T(Qx) = xTQTQx = xTx: (3) All inner products and lengths are preserved, when the space is rotated or reflected.',\n",
       " 'We come now to the calculation that uses the special property QT =Q.1.',\n",
       " 'If we have a basis, then any vector is a combination of the basis vectors.',\n",
       " 'This is exceptionally simple for an orthonormal basis, which will be a key idea behind Fourier series.',\n",
       " 'The problem is to find the coefficients of the basis vectors: Write b as a combination b = x1q1+x2q2+. . .+xnqn.',\n",
       " 'To compute x1 there is a neat trick.',\n",
       " 'Multiply both sides of the equation by qT1 .',\n",
       " 'On the left-hand side is qT1 b.',\n",
       " 'On the right-hand side all terms disappear (because qT1 qj = 0) except the first term.',\n",
       " 'We are left with qT1 b = x1qT1 q1:\\x0c 198 Chapter 3 Orthogonality Since qT1 q1 = 1, we have found x1 = qT1 b.',\n",
       " 'Similarly the second coefficient is x2 = qT2 b; that term survives when we multiply by qT2 .',\n",
       " 'The other terms die of orthogonality.',\n",
       " 'Each piece of b has a simple formula, and recombining the pieces gives back b: Every vector b is equal to (qT1 b)q1+(qT2 b)q2+. . .+(qTn b)qn: (4) I cant resist putting this orthonormal basis into a square matrix Q.',\n",
       " 'The vector equation x1q1+. . .+xnqn = b is identical to Qx = b. (The columns of Q multiply the components of x.) Its solution is x = Q.1b.',\n",
       " 'But since Q.1 = QTthis is where orthonormality entersthe solution is also x = QTb: x = QTb = 2 64 qT1  ...  qTn 3 75 2 6 4b 3 75 = 2 64 qT1 b ... qTn b 3 75 (5) The components of x are the inner products qTi b, as in equation (4).',\n",
       " 'The matrix form also shows what happens when the columns are not orthonormal.',\n",
       " 'Expressing b as a combination x1a1+. . .+xnan is the same as solving Ax = b.',\n",
       " 'The basis vectors go into the columns of A.',\n",
       " 'In that case we need A.1, which takes work.',\n",
       " 'In the orthonormal case we only need QT. Remark 1.',\n",
       " 'The ratio aTb=aTa appeared earlier, when we projected b onto a line.',\n",
       " 'Here a is q1, the denominator is 1, and the projection is (qT1 b)q1.',\n",
       " 'Thus we have a new interpretation for formula (4): Every vector b is the sum of its one-dimensional projections onto the lines through the qs.',\n",
       " 'Since those projections are orthogonal, Pythagoras should still be correct.',\n",
       " 'The square of the hypotenuse should still be the sum of squares of the components: kbk2 = (qT1 b)2+(qT2 b)2+. . .+(qTnb)2 which is kQTbk2: (6) Remark 2.',\n",
       " 'Since QT = Q.1, we also have QQT = I.',\n",
       " 'When Q comes before QT, multiplication takes the inner products of the rows of Q. (For QTQ it was the columns.) Since the result is again the identity matrix, we come to a surprising conclusion: The rows of a square matrix are orthonormal whenever the columns are.',\n",
       " 'The rows point in completely different directions from the columns, and I dont see geometrically why they are forced to be orthonormalbut they are.',\n",
       " 'Orthonormal columns Orthonormal rows Q = 2 64 1= p 3 1= p 2 1= p 6 1= p 3 0 .2= p 6 1= p 3 .1= p 2 1= p 6 3 75 : Rectangular Matrices with Orthogonal Columns This chapter is about Ax = b, when A is not necessarily square.',\n",
       " 'For Qx = b we now admit the same possibilitythere may be more rows than columns.',\n",
       " 'The n orthonormal\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 199 vectors qi in the columns of Q have m > n components.',\n",
       " 'Then Q is an m by n matrix and we cannot expect to solve Qx = b exactly.',\n",
       " 'We solve it by least squares.',\n",
       " 'If there is any justice, orthonormal columns should make the problem simple.',\n",
       " 'It worked for square matrices, and now it will work for rectangular matrices.',\n",
       " 'The key is to notice that we still have QTQ = I.',\n",
       " 'So QT is still the left-inverse of Q.',\n",
       " 'For least squares that is all we need.',\n",
       " 'The normal equations came from multiplying Ax = b by the transpose matrix, to give ATAbx = ATb. Now the normal equations are QTQ = QTb. But QTQ is the identity matrix! Therefore bx = QTb, whether Q is square and bx is an exact solution, or Q is rectangular and we need least squares. 3S If Q has orthonormal columns, the least-squares problem becomes easy: rectangular system with no solution for most b.',\n",
       " 'Qx = b QTQbx = QTb bx = QTb p = Qbx p = QQTb rectangular system with no solution for most b. normal equation for the best bxin which QTQ = I. bxi is qTi b. the projection of b is (qT1 b)q1+. . .+(qTn b)qn. the projection matrix is P = QQT. The last formulas are like p=Abx and P=A(ATA).1AT. When the columns are orthonormal, the cross-product matrix ATA becomes QTQ = I.',\n",
       " 'The hard part of least squares disappears when vectors are orthonormal.',\n",
       " 'The projections onto the axes are uncoupled, and p is the sum p = (qT1 b)q1+. . .+(qTn b)qn.',\n",
       " 'We emphasize that those projections do not reconstruct b.',\n",
       " 'In the square case m = n, they did.',\n",
       " 'In the rectangular case m > n, they dont.',\n",
       " 'They give the projection p and not the original vector bwhich is all we can expect when there are more equations than unknowns, and the qs are no longer a basis.',\n",
       " 'The projection matrix is usually A(ATA).1AT, and here it simplifies to P = Q(QTQ).1QT or P = QQT: (7) Notice that QTQ is the n by n identity matrix, whereas QQT is an m by m projection P.',\n",
       " 'It is the identity matrix on the columns of Q (P leaves them alone), But QQT is the zero matrix on the orthogonal complement (the nullspace of QT). Example 3.',\n",
       " 'The following case is simple but typical.',\n",
       " 'Suppose we project a point b = (x;y; z) onto the x-y plane.',\n",
       " 'Its projection is p = (x;y;0), and this is the sum of the separate projections onto the x- and y-axes: q1 = 2 64 1 0 0 3 75 and (qT1 b)q1 = 2 64 x 0 0 3 75 ; q2 = 2 64 0 1 0 3 75 and (qT2 b)q2 = 2 64 0 y 0 3 75 :\\x0c 200 Chapter 3 Orthogonality The overall projection matrix is P = q1qT1 +q2qT2 = 2 64 1 0 0 0 1 0 0 0 0 3 75 ; and P 2 64 x y z 3 75 = 2 64 x y 0 3 75 : Projection onto a plane = sum of projections onto orthonormal q1 and q2.',\n",
       " 'Example 4.',\n",
       " 'When the measurement times average to zero, fitting a straight line leads to orthogonal columns.',\n",
       " 'Take t1 = .3, t2 = 0, and t3 = 3.',\n",
       " 'Then the attempt to fit y =C+Dt leads to three equations in two unknowns: C + Dt1 = y1 C + Dt2 = y2 C + Dt3 = y3 ; or 2 64 1 .3 1 0 1 3 3 75 \" C D # = 2 64 y1 y2 y3 3 75 : The columns (1;1;1) and (.3;0;3) are orthogonal.',\n",
       " 'We can project y separately onto each column, and the best coefficients b C and bD can be found separately: b C = h 1 1 1 ih y1 y2 y3 iT 12+12+12 ; bD = h .3 0 3 ih y1 y2 y3 iT (.3)2+02+32 : Notice that b C = (y1 +y2 +y3)=3 is the mean of the data. b C gives the best fit by a horizontal line, whereas bDt is the best fit by a straight line through the origin.',\n",
       " 'The columns are orthogonal, so the sum of these two separate pieces is the best fit by any straight line whatsoever.',\n",
       " 'The columns are not unit vectors, so b C and bD have the length squared in the denominator.',\n",
       " 'Orthogonal columns are so much better that it is worth changing to that case. if the average of the observation times is not zeroit is .t = (t1+. . .+tm)=mthen the time origin can be shifted by .t.',\n",
       " 'Instead of y =C+Dt we work with y = c+d(t..t).',\n",
       " 'The best line is the same! As in the example, we find bc = h 1 . . . 1 ih y1 . . . ym iT 12+12+. . .+12 = y1+. . .+ym m b d = h (t1..t) . . . (tm..t) ih y1 . . . ym iT (t1..t)2+. . .+(tm..t)2 = a(ti..t)yi a(ti..t)2 : (8) The best bc is the mean, and we also get a convenient formula for b d.',\n",
       " 'The earlier ATA had the off-diagonal entries ati, and shifting the time by .t made these entries zero.',\n",
       " 'This shift is an example of the Gram-Schmidt process, which orthogonalizes the situation in advance.',\n",
       " 'Orthogonal matrices are crucial to numerical linear algebra, because they introduce no instability.',\n",
       " 'While lengths stay the same, roundoff is under control.',\n",
       " 'Orthogonalizing vectors has become an essential technique.',\n",
       " 'Probably it comes second only to elimination.',\n",
       " 'And it leads to a factorization A = QR that is nearly as famous as A = LU.\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 201 The Gram-Schmidt Process Suppose you are given three independent vectors a, b, c.',\n",
       " 'If they are orthonormal, life is easy.',\n",
       " 'To project a vector v onto the first one, you compute (aTv)a.',\n",
       " 'To project the same vector v onto the plane of the first two, you just add (aTv)a+(bTv)b.',\n",
       " 'To project onto the span of a, b, c, you add three projections.',\n",
       " 'All calculations require only the inner products aTv, bTv, and cTv. But to make this true, we are forced to say, If they are orthonormal.',\n",
       " 'Now we propose to find a way to make them orthonormal.',\n",
       " 'The method is simple.',\n",
       " 'We are given a, b, c and we want q1, q2, q3.',\n",
       " 'There is no problem with q1: it can go in the direction of a.',\n",
       " 'We divide by the length, so that q1 = a=kak is a unit vector.',\n",
       " 'The real problem begins with q2which has to be orthogonal to q1.',\n",
       " 'If the second vector b has any component in the direction of q1 (which is the direction of a), that component has to be subtracted: Second vector B = b.(qT1 b)q1 and q2 = B=kBk: (9) B is orthogonal to q1.',\n",
       " 'It is the part of b that goes in a new direction, and not in the a.',\n",
       " 'In Figure 3.10, B is perpendicular to q1.',\n",
       " 'It sets the direction for q2. b q2 B b q1 a Figure 3.10: The qi component of b is removed; a and B normalized to q1 and q2.',\n",
       " 'At this point q1 and q2 are set.',\n",
       " 'The third orthogonal direction starts with c.',\n",
       " 'It will not be in the plane of q1 and q2, which is the plane of a and b.',\n",
       " 'However, it may have a component in that plane, and that has to be subtracted. (If the result is C = 0, this signals that a, b, c were not independent in the first place) What is left is the component C we want, the part that is in a new direction perpendicular to the plane: Third vector C = c.(qT1 c)q1.(qT2 c)q2 and q3 =C=kCk: (10) This is the one idea of the whole Gram-Schmidt process, to subtract from every new vector its components in the directions that are already settled.',\n",
       " 'That idea is used over and over again.3 When there is a fourth vector, we subtract away its components in the directions of q1, q2, q3. 3If Gram thought of it first, what was left for Schmidt?\\x0c 202 Chapter 3 Orthogonality Example 5.',\n",
       " 'Gram-Schmidt Suppose the independent vectors are a, b, c: a = 2 64 1 0 1 3 75 ; b = 2 64 1 0 0 3 75 ; c = 2 64 2 1 0 3 75 : To find q1, make the first vector into a unit vector: q1 = a= p 2.',\n",
       " 'To find q2, subtract from the second vector its component in the first direction: B = b.(qT1 b)q1 = 2 64 1 0 0 3 75 . 1 p 2 2 64 1= p 2 0 1= p 2 3 75 = 1 2 2 64 1 0 .1 3 75 : The normalized q2 is B divided by its length, to produce a unit vector: q2 = 2 64 1= p 2 0 .1= p 2 3 75 : To find q3, subtract from c its components along q1 and q2: C = c.(qT1 c)q1.(qT2 c)q2 = 2 64 2 1 0 3 75 . p 2 2 64 1= p 2 0 1= p 2 3 75 . p 2 2 64 1= p 2 0 .1= p 2 3 75 = 2 640 1 0 3 75 : This is already a unit vector, so it is q3.',\n",
       " 'I went to desperate lengths to cut down the number of square roots (the painful part of Gram-Schmidt).',\n",
       " 'The result is a set of orthonormal vectors q1, q2, q3, which go into the columns of an orthogonal matrix Q: Orthonormal basis Q = 2 6 4q1 q2 q3 3 75 = 2 64 1= p 2 1= p 2 0 0 0 1 1= p 2 .1= p 2 0 3 75 : 3T The Gram-Schmidt process starts with independent vectors a1; : : : ;an and ends with orthonormal vectors q1; : : : ;qn.',\n",
       " 'At step j it subtracts from aj its components in the directions q1; : : : ;qj.1 that are already settled: Aj = aj .(qT1 aj)q1.. . ..(qTj .1aj)qj.1: (11) Then qj is the unit vector Aj=kAjk.',\n",
       " 'Remark on the calculations I think it is easier to compute the orthogonal a, B, C, without forcing their lengths to equal one.',\n",
       " 'Then square roots enter only at the end, when\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 203 dividing by those lengths.',\n",
       " 'The example above would have the same B and C, without using square roots.',\n",
       " 'Notice the 12 from aTb=aTa instead of p1 2 from qTb: B = 2 64 1 0 0 3 75 . 1 2 2 64 1 0 1 3 75 and then C = 2 64 2 1 0 3 75 . 2 64 1 0 1 3 75 .2 2 64 120 .12 3 75 : The Factorization A = QR We started with a matrix A, whose columns were a, b, c.',\n",
       " 'We ended with a matrix Q, whose columns are q1, q2, q3.',\n",
       " 'What is the relation between those matrices?',\n",
       " 'The matrices A and Q are m by n when the n vectors are in m-dimensional space, and there has to be a third matrix that connects them.',\n",
       " 'The idea is to write the as as combinations of the qs.',\n",
       " 'The vector b in Figure 3.10 is a combination of the orthonormal q1 and q2, and we know what combination it is: b = (qT1 b)q1+(qT2 b)q2: Every vector in the plane is the sum of its q1 and q2 components.',\n",
       " 'Similarly c is the sum of its q1, q2, q3 components: c = (qT1 c)q1 +(qT2 c)q2 +(qT3 c)q3.',\n",
       " 'If we express that in matrix form we have the new factorization A = QR: QR factors A = 2 6 4a b c 3 75 = 2 6 4q1 q2 q3 3 75 2 64 qT1 a qT1 b qT1 c qT2b qT2 c qT3 c 3 75 = QR (12) Notice the zeros in the last matrix! R is upper triangular because of the way Gram- Schmidt was done.',\n",
       " 'The first vectors a and q1 fell on the same line.',\n",
       " 'Then q1, q2 were in the same plane as a, b.',\n",
       " 'The third vectors c and q3 were not involved until step 3.',\n",
       " 'The QR factorization is like A = LU, except that the first factor Q has orthonormal columns.',\n",
       " 'The second factor is called R, because the nonzeros are to the right of the diagonal (and the letter U is already taken).',\n",
       " 'The off-diagonal entries of R are the numbers qT1 b = 1= p 2 and qT1 c = qT2 c = p 2, found above.',\n",
       " 'The whole factorization is A = 2 64 1 1 2 0 0 1 1 0 0 3 75= 2 641= p 2 1= p 2 0 0 0 1 1= p 2 .1= p 2 0 3 75 2 64 p 2 1= p 2 p 2 1= p 2 p 2 1 3 75 = QR: You see the lengths of a, B, C on the diagonal of R.',\n",
       " 'The orthonormal vectors q1, q2, q3, which are the whole object of orthogonalization, are in the first factor Q.',\n",
       " 'Maybe QR is not as beautiful as LU (because of the square roots).',\n",
       " 'Both factorizations are vitally important to the theory of linear algebra, and absolutely central to the calculations.',\n",
       " 'If LU is Hertz, then QR is Avis.\\x0c 204 Chapter 3 Orthogonality The entries ri j = qTi aj appear in formula (11), when kAjkqj is substituted for Aj: aj = (qT1 aj)q1+. . .+(qTj .1aj)qj.1+kAjkqj = Q times column j of R: (13) 3U Every m by n matrix with independent columns can be factored into A = QR. The columns of Q are orthonormal, and R is upper triangular and invertible.',\n",
       " 'When m = n and all matrices are square, Q becomes an orthogonal matrix.',\n",
       " 'I must not forget the main point of orthogonalization.',\n",
       " 'It simplifies the least-squares problem Ax = b.',\n",
       " 'The normal equations are still correct, but ATA becomes easier: ATA = RTQTQR = RTR: (14) The fundamental equation ATAbx = ATb simplifies to a triangular system: RTRbx = RTQTb or Rbx = QTb: (15) Instead of solving QRx = b, which cant be done, we solve Rbx = QTb which is just back-substitution because R is triangular.',\n",
       " 'The real cost is the mn2 operations of Gram- Schmidt, which are needed to find Q and R in the first place.',\n",
       " 'The same idea of orthogonality applies to functions, The sines and cosines are orthogonal; the powers 1, x, x2 are not.',\n",
       " 'When f (x) is written as a combination of sines and cosines, that is a Fourier series.',\n",
       " 'Each term is a projection onto a linethe line in function space containing multiples of cosnx or sinnx.',\n",
       " 'It is completely parallel to the vector case, and very important.',\n",
       " 'And finally we have a job for Schmidt: To orthogonalize the powers of x and produce the Legendre polynomials.',\n",
       " 'Function Spaces and Fourier Series This is a brief and optional section, but it has a number of good intentions: 1. to introduce the most famous infinite-dimensional vector space (Hilbert space); 2. to extend the ideas of length and inner product from vectors v to functions f (x): 3. to recognize the Fourier series as a sum of one-dimensional projections (the orthogonal columns are the sines and cosines); 4. to apply Gram-Schmidt orthogonalization to the polynomials 1;x;x2; : : :; and 5. to find the best approximation to f (x) by a straight line.',\n",
       " 'We will try to follow this outline, which opens up a range of new applications for linear algebra, in a systematic way. 1.',\n",
       " 'Hilbert Space.',\n",
       " 'After studying Rn, it is natural to think of the space R.. It contains all vectors v = (v1;v2;v3; : : :) with an infinite sequence of components.',\n",
       " 'This space\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 205 is actually too big when there is no control on the size of components v j.',\n",
       " 'A much better idea is to keep the familiar definition of length, using a sum of squares, and to include only those vectors that have a finite length: Length squared kvk2 = v21 +v22 +v23 +. . . (16) The infinite series must converge to a finite sum.',\n",
       " 'This leaves (1;12 ;13 ; : : :) but not (1;1;1; : : :).',\n",
       " 'Vectors with finite length can be added (kv+wkkvk+kwk) and multiplied by scalars, so they form a vector space.',\n",
       " 'It is the celebrated Hilbert space.',\n",
       " 'Hilbert space is the natural way to let the number of dimensions become infinite, and at the same time to keep the geometry of ordinary Euclidean space.',\n",
       " 'Ellipses become infinite-dimensional ellipsoids, and perpendicular lines are recognized exactly as before.',\n",
       " 'The vectors v and w are orthogonal when their inner product is zero: Orthogonality vTw = v1w1+v2w2+v3w3+. . . = 0: This sum is guaranteed to converge, and for any two vectors it still obeys the Schwarz inequality jvTwj kvkkwk.',\n",
       " 'The cosine, even in Hilbert space, is never larger than 1.',\n",
       " 'There is another remarkable thing about this space: It is found under a great many different disguises.',\n",
       " 'Its vectors can turn into functions, which is the second point. 2.',\n",
       " 'Lengths and Inner Products.',\n",
       " 'Suppose f (x) = sin x on the interval 0 x 2p.',\n",
       " 'This f is like a vector with a whole continuum of components, the values of sin x along the whole interval.',\n",
       " 'To find the length of such a vector, the usual rule of adding the squares of the components becomes impossible.',\n",
       " 'This summation is replaced, in a natural and inevitable way, by integration: Length k f k of function k f k2 = Z 2p 0 ( f (x))2dx = Z 2p 0 (sin x)2dx =p (17) Our Hilbert space has become a function space.',\n",
       " 'The vectors are functions, we have a way to measure their length, and the space contains all those functions that have a finite lengthjust as in equation (16).',\n",
       " 'It does not contain the function F(x) = 1=x, because the integral of 1=x2 is infinite.',\n",
       " 'The same idea of replacing summation by integration produces the inner product of two functions: If f (x) = sin x and g(x) = cos x, then their inner product is ( f ;g) = Z 2p 0 f (x)g(x)dx = Z 2p 0 sin x cos xdx = 0: (18) This is exactly like the vector inner product f Tg. It is still related to the length by ( f ; f ) = k f k2.',\n",
       " 'The Schwarz inequality is still satisfied: j( f ;g)j k f kkgk.',\n",
       " 'Of course, two functions like sin x and cos xwhose inner product is zerowill be called orthogonal.',\n",
       " 'They are even orthonormal after division by their length p p.\\x0c 206 Chapter 3 Orthogonality 3.',\n",
       " 'The Fourier series of a function is an expansion into sines and cosines: f (x) = a0+a1 cos x+b1 sin x+a2 cos2x+b2 sin2x+. . . : To compute a coefficient like b1, multiply both sides by the corresponding function sin x and integrate from 0 to 2p. (The function f (x) is given on that interval.) In other words, take the inner product of both sides with sin x: Z 2p 0 f (x) sin xdx = a0 Z 2p 0 sin xdx+a1 Z 2p 0 cos x sin xdx+b1 Z 2p 0 (sin x)2dx+. . . : On the right-hand side, every integral is zero except onethe one in which sin x multiplies itself.',\n",
       " 'The sines and cosines are mutually orthogonal as in equation (18) Therefore b1 is the left-hand side divided by that one nonzero integral: b1 = R 2p 0 f (x) sin xdx R 2p 0 (sin x)2dx = ( f ; sin x) (sin x; sin x): The Fourier coefficient a1 would have cos x in place of sin x, and a2 would use cos2x.',\n",
       " 'The whole point is to see the analogy with projections.',\n",
       " 'The component of the vector b along the line spanned by a is bTa=aTa. A Fourier series is projecting f (x) onto sin x.',\n",
       " 'Its component p in this direction is exactly b1 sin x.',\n",
       " 'The coefficient b1 is the least squares solution of the inconsistent equation b1 sin x = f (x).',\n",
       " 'This brings b1 sin x as close as possible to f (x).',\n",
       " 'All the terms in the series are projections onto a sine or cosine.',\n",
       " 'Since the sines and cosines are orthogonal, the Fourier series gives the coordinates of the vector f (x) with respect to a set of (infinitely many) perpendicular axes. 4.',\n",
       " 'Gram-Schmidt for Functions.',\n",
       " 'There are plenty of useful functions other than sines and cosines, and they are not always orthogonal.',\n",
       " 'The simplest are the powers of x, and unfortunately there is no interval on which even 1 and x2 are perpendicular. (Their inner product is always positive, because it is the integral of x2.) Therefore the closest parabola to f (x) is not the sum of its projections onto 1, x, and x2.',\n",
       " 'There will be a matrix like (ATA).1, and this coupling is given by the ill-conditioned Hilbert matrix.',\n",
       " 'On the interval 0 x 1, ATA = 2 64 (1;1) (1;x) (1;x2) (x;1) (x;x) (x;x2) (x2;1) (x2;x) (x2;x2) 3 75 = 2 64 R 1 R x R x2 R x R x2 R x3 R x2 R x3 R x4 3 75 = 2 64 1 12 13 12 13 14 13 14 15 3 75 : This matrix has a large inverse, because the axes 1, x, x2 are far from perpendicular.',\n",
       " 'The situation becomes impossible if we add a few more axes.',\n",
       " 'It is virtually hopeless to solve ATAbx = ATb for the closest polynomial of degree ten.',\n",
       " 'More precisely, it is hopeless to solve this by Gaussian elimination; every roundoff error would be amplified by more than 1013.',\n",
       " 'On the other hand, we cannot just give\\x0c 3.4 Orthogonal Bases and Gram-Schmidt 207 up; approximation by polynomials has to be possible.',\n",
       " 'The right idea is to switch to orthogonal axes (by Gram-Schmidt).',\n",
       " 'We look for combinations of 1, x, and x2 that are orthogonal.',\n",
       " 'It is convenient to work with a symmetrically placed interval like .1x1, because this makes all the odd powers of x orthogonal to all the even powers: (1;x) = Z 1 .1 xdx = 0; (x;x2) = Z 1 .1 x3dx = 0: Therefore the Gram-Schmidt process can begin by accepting v1 = 1 and v2 = x as the first two perpendicular axes.',\n",
       " 'Since (x;x2) = 0, it only has to correct the angle between 1 and x2.',\n",
       " 'By formula (10), the third orthogonal polynomial is Orthogonalize v3 = x2. (1;x2) (1;1) 1. (x;x2) (x;x) x = x2.',\n",
       " 'R1 .1 x2dx R1 .1 1dx = x2. 1 3 : The polynomials constructed in this way are called the Legendre polynomials and they are orthogonal to each other over the interval .1 x 1.',\n",
       " 'Check  1;x2. 1 3  = Z 1 .1 x2. 1 3 dx =  x3 3 . x 3 .1 .1 = 0: The closest polynomial of degree ten is now computable, without disaster, by projecting onto each of the first 10 (or 11) Legendre polynomials. 5.',\n",
       " 'Best Straight Line.',\n",
       " 'Suppose we want to approximate y = x5 by a straight line C+Dx between x = 0 and x = 1.',\n",
       " 'There are at least three ways of finding that line, and if you compare them the whole chapter might become clear! 1.',\n",
       " 'Solve [1 x] .',\n",
       " 'C D  = x5 by least squares.',\n",
       " 'The equation ATAbx = ATb is \" (1;1) (1;x) (x;1) (x;x) #\" C D # = \" (1;x5) (x;x5) # or \" 1 12 12 13 #\" C D # = \" 161 17 # : 2.',\n",
       " 'Minimize E2 = R 1 0 (x5.C.Dx)2dx = 1 11 .26 C.27 D+C2+CD+13 D2. The derivatives with respect to C and D, after dividing by 2, bring back the normal equations of method 1 (and the solution is b C = 16 . 5 14, bD = 5 17): . 1 6 +C+ 1 2 D = 0 and . 1 7 + 1 2 C+ 1 3 D = 0: 3.',\n",
       " 'Apply Gram-Schmidt to replace x by x.(1;x)=(1;1).',\n",
       " 'That is x. 12 , which is orthogonal to 1.',\n",
       " 'Now the one-dimensional projections add to the best line: C+Dx = (x5;1) (1;1) 1+ (x5;x. 12 ) (x. 12 ;x. 12 ) (x. 12 ) = 1 6 + 5 7  x. 1 2 :\\x0c 208 Chapter 3 Orthogonality']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter['Projections and Least Squares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' Introduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-45ce797bae1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchapter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' Introduction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: ' Introduction'"
     ]
    }
   ],
   "source": [
    "chapter[' Introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 3.1 Orthogonal Vectors and Subspaces \n",
      "16707 \n",
      "Problem Set 3.1 \n",
      "0 3.1 Orthogonal Vectors and Subspaces \n",
      "16968 \n",
      "Problem Set 3.2 \n",
      "0 3.2 Cosines and Projections onto Lines \n",
      "24073 \n",
      "Problem Set 3.3 \n",
      "0 3.3 Projections and Least Squares \n",
      "37141 \n",
      "Problem Set 3.4 \n",
      "0 3.4 Orthogonal Bases and Gram-Schmidt \n",
      "22087 \n",
      "Problem Set 3.5 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_one_chapter_strang(3, book, subsections=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = {'Orthogonal Vectors and Subspaces': ['A basis is a set of independent vectors that span a space.',\n",
    "  'Geometrically, it is a set of coordinate axes.',\n",
    "  'A vector space is defined without those axes, but every time I think of the x-y plane or three-dimensional space or Rn, the axes are there.',\n",
    "  'They are usually perpendicular! The coordinate axes that the imagination constructs are practically always orthogonal.',\n",
    "  'In choosing a basis, we tend to choose an orthogonal basis.',\n",
    "  'The idea of an orthogonal basis is one of the foundations of linear algebra.',\n",
    "  'We need a basis to convert geometric constructions into algebraic calculations, and we need an orthogonal basis to make those calculations simple.',\n",
    "  'A further specialization makes the basis just about optimal: The vectors should have length 1.',\n",
    "  'For an orthonormal basis (orthogonal unit vectors), we will find 1. the length kxk of a vector; 2. the test xTy = 0 for perpendicular vectors; and 3. how to create perpendicular vectors from linearly independent vectors.',\n",
    "  'More than just vectors, subspaces can also be perpendicular.',\n",
    "  'We will discover, so beautifully and simply that it will be a delight to see, that the fundamental subspaces meet at right angles.',\n",
    "  'Those four subspaces are perpendicular in pairs, two in Rm and two in Rn. That will complete the fundamental theorem of linear algebra.',\n",
    "  'The first step is to find the length of a vector.',\n",
    "  'It is denoted by kxk, and in two dimensions it comes from the hypotenuse of a right triangle (Figure 3.1a).',\n",
    "  'The square of the length was given a long time ago by Pythagoras: kxk2 = x2 1+x2 2.',\n",
    "  'In three-dimensional space, x = (x1;x2;x3) is the diagonal of a box (Figure 3.1b).',\n",
    "  'Its length comes from two applications of the Pythagorean formula.',\n",
    "  'The two-dimensional case takes care of (x1;x2;0) = (1;2;0) across the base.',\n",
    "  'This forms a right angle with the vertical side (0;0;x3)= (0;0;3).',\n",
    "  'The hypotenuse of the bold triangle (Pythagoras again)\\x0c 160 Chapter 3 Orthogonality b 1 2 p5 (1, 0) (0, 2) (1, 2) kxk 2 = x2 1 + x2 2 + x2 3 5 = 12 + 22 14 = 12 + 22 + 32 (a) (b) x (0, 0, 3) (1, 2, 3) has length p 14 (1, 0, 0) (0, 2, 0) (1, 2, 0) has length p 5 Figure 3.1: The length of vectors (x1;x2) and (x1;x2;x3). is the length kxk we want: Length in 3D kxk2 = 12+22+32 and kxk = q x2 1+x2 2+x2 3: The extension to x = (x1; : : : ;xn) in n dimensions is immediate.',\n",
    "  'By Pythagoras n.1 times, the length kxk in Rn is the positive square root of xTx: Length squared kxk2 = x2 1+x2 2+. . .+x2 n = xTx: (1) The sum of squares matches xTxand the length of x = (1;2;.3) is p 14: xTx = h 1 2 .3 i 2 64 1 2 .3 3 75 = 12+22+(.3)2 = 14: Orthogonal Vectors How can we decide whether two vectors x and y are perpendicular?',\n",
    "  'What is the test for orthogonality in Figure 3.2?',\n",
    "  'In the plane spanned by x and y, those vectors are orthogonal provided they form a right triangle.',\n",
    "  'We go back to a2+b2 = c2: Sides of a right triangle kxk2+kyk2 = kx.yk2: (2) Applying the length formula (1), this test for orthogonality in Rn becomes . x2 1+. . .+x2 n . + . y21 +. . .+y2n . = (x1.y1)2+. . .+(xn.yn)2: The right-hand side has an extra .2xiyi from each (xi.yi)2: right-hand side = . x2 1+. . .+x2 n . .2(x1y1+. . .+xnyn)+ . y21 +. . .+y2n . :\\x0c 3.1 Orthogonal Vectors and Subspaces 161 y = \\x14.1 2 \\x15 x = \\x144 2 \\x15 p 25 p 20 p 5 xTy = 0 b Right angle xTy = 0 greater than 90 xTy < 0 less than 90 xTy > 0 Figure 3.2: A right triangle with 5+20 = 25.',\n",
    "  'Dotted angle 100, dashed angle 30.',\n",
    "  'We have a right triangle when that sum of cross-product terms xiyi is zero: Orthogonal vectors xTy = x1y1+. . .+xnyn = 0: (3) This sum is xTy = axiyi = yTx, the row vector xT times the column vector y: Inner product xTy = h x1 . . . xn i 2 64 y1 ... yn 3 75 = x1y1+. . .+xnyn: (4) This number is sometimes called the scalar product or dot product, and denoted by (x;y) or x . y.',\n",
    "  'We will use the name inner product and keep the notation xTy. 3A The inner product xTy is zero if and only if x and y are orthogonal vectors.',\n",
    "  'If xTy > 0, their angle is less than 90.',\n",
    "  'If xTy < 0, their angle is greater than 90.',\n",
    "  'The length squared is the inner product of x with itself: xTx = x2 1+. . .+x2 n = kxk2.',\n",
    "  'The only vector with length zerothe only vector orthogonal to itselfis the zero vector.',\n",
    "  'This vector x = 0 is orthogonal to every vector in Rn. Example 1. (2;2;.1) is orthogonal to (.1;2;2).',\n",
    "  'Both have length p 4+4+1 = 3.',\n",
    "  'Useful fact: If nonzero vectors v1; : : : ;vk are mutually orthogonal (every vector is perpendicular to every other), then those vectors are linearly independent.',\n",
    "  'Proof.',\n",
    "  'Suppose c1v1 +. . .+ckvk = 0.',\n",
    "  'To show that c1 must be zero, take the inner product of both sides with v1.',\n",
    "  'Orthogonality of the vs leaves only one term: vT1 (c1v1+. . .+ckvk) = c1vT1 v1 = 0: (5) The vectors are nonzero, so vT1 v1 6= 0 and therefore c1 = 0.',\n",
    "  'The same is true of every ci.',\n",
    "  'The only combination of the vs producing zero has all ci = 0: independence! The coordinate vectors e1; : : : ;en in Rn are the most important orthogonal vectors.',\n",
    "  'Those are the columns of the identity matrix.',\n",
    "  'They form the simplest basis for Rn, and\\x0c 162 Chapter 3 Orthogonality they are unit vectorseach has length keik=1.',\n",
    "  'They point along the coordinate axes.',\n",
    "  'If these axes are rotated, the result is a new orthonormal basis: a new system of mutually orthogonal unit vectors.',\n",
    "  'In R2 we have cos2q +sin2q = 1: Orthonormal vectors in R2 v1 = (cosq ; sinq ) and v2 = (.sinq ;cosq ): Orthogonal Subspaces We come to the orthogonality of two subspaces.',\n",
    "  'Every vector in one subspace must be orthogonal to every vector in the other subspace.',\n",
    "  'Subspaces of R3 can have dimension 0, 1, 2, or 3.',\n",
    "  'The subspaces are represented by lines or planes through the origin and in the extreme cases, by the origin alone or the whole space.',\n",
    "  'The subspace f0g is orthogonal to all subspaces.',\n",
    "  'A line can be orthogonal to another line, or it can be orthogonal to a plane, but a plane cannot be orthogonal to a plane.',\n",
    "  'I have to admit that the front wall and side wall of a room look like perpendicular planes in R3. But by our definition, that is not so! There are lines v and w in the front and side walls that do not meet at a right angle.',\n",
    "  'The line along the corner is in both walls, and it is certainly not orthogonal to itself. 3B Two subspaces V and W of the same space Rn are orthogonal if every vector v in V is orthogonal to every vector w in W: vTw = 0 for all v and w.',\n",
    "  'Example 2.',\n",
    "  'Suppose V is the plane spanned by v1 = (1;0;0;0) and v2 = (1;1;0;0).',\n",
    "  'If W is the line spanned by w = (0;0;4;5), then w is orthogonal to both vs.',\n",
    "  'The line W will be orthogonal to the whole plane V.',\n",
    "  'In this case, with subspaces of dimension 2 and 1 in R4, there is room for a third subspace.',\n",
    "  'The line L through z = (0;0;5;.4) is perpendicular to V and W.',\n",
    "  'Then the dimensions add to 2+1+1 = 4.',\n",
    "  'What space is perpendicular to all of V, W, and L?',\n",
    "  'The important orthogonal subspaces dont come by accident, and they come two at a time.',\n",
    "  'In fact orthogonal subspaces are unavoidable: They are the fundamental subspaces! The first pair is the nullspace and row space.',\n",
    "  'Those are subspaces of Rnthe rows have n components and so does the vector x in Ax = 0.',\n",
    "  'We have to show, using Ax = 0, that the rows of A are orthogonal to the nullspace vector x. 3C Fundamental theorem of orthogonality The row space is orthogonal to the nullspace (in Rn).',\n",
    "  'The column space is orthogonal to the left nullspace (in Rm).',\n",
    "  'First Proof.',\n",
    "  'Suppose x is a vector in the nullspace.',\n",
    "  'Then Ax = 0, and this system of m\\x0c 3.1 Orthogonal Vectors and Subspaces 163 equations can be written out as rows of A multiplying x: Every row is orthogonal to x Ax = 2 6664 . . . row 1 . . . . . . row 2 . . . ... ... ... . . . row m . . . 3 7775 2 6664 x1 x2 ... xn 3 7775 = 2 6664 0 0... 0 3 7775 : (6) The main point is already in the first equation: row 1 is orthogonal to x.',\n",
    "  'Their inner product is zero; that is equation 1.',\n",
    "  'Every right-hand side is zero, so x is orthogonal to every row.',\n",
    "  'Therefore x is orthogonal to every combination of the rows.',\n",
    "  'Each x in the nullspace is orthogonal to each vector in the row space, so N(A)?C(AT). The other pair of orthogonal subspaces comes from ATy = 0, or yTA = 0: yTA = h y1 . . . ym i 2 6666664 c c o o l l u . . . u m m n n 1 n 3 7777775 = h 0 . . . 0 i : (7) The vector y is orthogonal to every column.',\n",
    "  'The equation says so, from the zeros on the right-hand side.',\n",
    "  'Therefore y is orthogonal to every combination of the columns.',\n",
    "  'It is orthogonal to the column space, and it is a typical vector in the left nullspace: N(AT)?C(A). This is the same as the first half of the theorem, with A replaced by AT. Second Proof.',\n",
    "  'The contrast with this coordinate-free proof should be useful to the reader.',\n",
    "  'It shows a more abstract method of reasoning.',\n",
    "  'I wish I knew which proof is clearer, and more permanently understood.',\n",
    "  'If x is in the nullspace then Ax = 0.',\n",
    "  'If v is in the row space, it is a combination of the rows: v = ATz for some vector z.',\n",
    "  'Now, in one line: Nullspace ?',\n",
    "  'Row space vTx = (ATz)Tx = zTAx = zT0 = 0: (8) Example 3.',\n",
    "  'Suppose A has rank 1, so its row space and column space are lines: Rank-1 matrix A = 2 64 1 3 2 6 3 9 3 75 : The rows are multiples of (1;3).',\n",
    "  'The nullspace contains x=(.3;1), which is orthogonal to all the rows.',\n",
    "  'The nullspace and row space are perpendicular lines in R2: h 1 3 i\" 3 .1 # = 0 and h 2 6 i\" 3 .1 # = 0 and h 3 9 i\" 3 .1 # = 0:\\x0c 164 Chapter 3 Orthogonality In contrast, the other two subspaces are in R3. The column space is the line through (1;2;3).',\n",
    "  'The left nullspace must be the perpendicular plane y1 +2y2 +3y3 = 0.',\n",
    "  'That equation is exactly the content of yTA = 0.',\n",
    "  'The first two subspaces (the two lines) had dimensions 1+1 = 2 in the space R2. The second pair (line and plane) had dimensions 1+2 = 3 in the space R3. In general, the row space and nullspace have dimensions that add to r+(n.r) = n.',\n",
    "  'The other pair adds to r+(m.r) = m.',\n",
    "  'Something more than orthogonality is occurring, and I have to ask your patience about that one further point: the dimensions.',\n",
    "  'It is certainly true that the null space is perpendicular to the row spacebut it is not the whole truth.',\n",
    "  'N(A) contains every vector orthogonal to the row space.',\n",
    "  'The nullspace was formed from all solutions to Ax = 0.',\n",
    "  'Definition.',\n",
    "  'Given a subspace V of Rn, the space of all vectors orthogonal to V is called the orthogonal complement of V.',\n",
    "  'It is denoted by V? = V perp.',\n",
    "  'Using this terminology, the nullspace is the orthogonal complement of the row space: N(A) = (C(AT))?.',\n",
    "  'At the same time, the row space contains all vectors that are orthogonal to the nullspace.',\n",
    "  'A vector z cant be orthogonal to the nullspace but outside the row space.',\n",
    "  'Adding z as an extra row of A would enlarge the row space, but we know that there is a fixed formula r+(n.r) = n: Dimension formula dim(row space)+dim(nullspace) = number of columns.',\n",
    "  'Every vector orthogonal to the nullspace is in the row space: C(AT) = (N(A))?.',\n",
    "  'The same reasoning applied to AT produces the dual result: The left nullspace N(AT) and the column space C(A) are orthogonal complements.',\n",
    "  'Their dimensions add up to (m.r)+r = m, This completes the second half of the fundamental theorem of linear algebra.',\n",
    "  'The first half gave the dimensions of the four subspaces. including the fact that row rank = column rank.',\n",
    "  'Now we know that those subspaces are perpendicular.',\n",
    "  'More than that, the subspaces are orthogonal complements. 3D Fundamental Theorem of Linear Algebra, Part II The nullspace is the orthogonal complement of the row space in Rn. The left nullspace is the orthogonal complement of the column space in Rm. To repeat, the row space contains everything orthogonal to the nullspace.',\n",
    "  'The column space contains everything orthogonal to the left nullspace.',\n",
    "  'That is just a sentence, hidden in the middle of the book, but it decides exactly which equations can be solved! Looked at directly, Ax = b requires b to be in the column space.',\n",
    "  'Looked at indirectly.',\n",
    "  'Ax = b requires b to be perpendicular to the left nullspace. 3E Ax = b is solvable if and only if yTb = 0 whenever yTA = 0.\\x0c 3.1 Orthogonal Vectors and Subspaces 165 The direct approach was b must be a combination of the columns.',\n",
    "  'The indirect approach is b must be orthogonal to every vector that is orthogonal to the columns.',\n",
    "  'That doesnt sound like an improvement (to put it mildly).',\n",
    "  'But if only one or two vectors are orthogonal to the columns. it is much easier to check those one or two conditions yTb = 0.',\n",
    "  'A good example is Kirchhoffs Voltage Law in Section 2.5.',\n",
    "  'Testing for zero around loops is much easier than recognizing combinations of the columns.',\n",
    "  'When the left-hand sides of Ax = b add to zero, the right-hand sides must, too: x1.x2 = b1 x2.x3 = b2 x3.x1 = b3 is solvable if and only if b1+b2+b3 = 0: Here A = 2 64 1 .1 0 0 1 .1 .1 0 1 3 75 : This test b1 +b2 +b3 = 0 makes b orthogonal to y = (1;1;1) in the left nullspace.',\n",
    "  'By the Fundamental Theorem, b is a combination of the columns! The Matrix and the Subspaces We emphasize that V and W can be orthogonal without being complements.',\n",
    "  'Their dimensions can be too small.',\n",
    "  'The line V spanned by (0;1;0) is orthogonal to the line W spanned by (0;0;1), but V is not W?. The orthogonal complement of W is a twodimensional plane, and the line is only part of W?. When the dimensions are right, orthogonal subspaces are necessarily orthogonal complements: If W= V? then V =W? and dimV+dimW= n: In other words V?? = V.',\n",
    "  'The dimensions of V and W are right, and the whole space Rn is being decomposed into two perpendicular parts (Figure 3.3).',\n",
    "  'W V Two orthogonal axes in R3 Not orthogonal complements W V Line W perpendicular to plane V Orthogonal complements V =W?',\n",
    "  'Figure 3.3: Orthogonal complements in R3: a plane and a line (not two lines).',\n",
    "  'Splitting Rn into orthogonal parts will split every vector into x = v+w.',\n",
    "  'The vector v is the projection onto the subspace V.',\n",
    "  'The orthogonal component w is the projection of x onto W.',\n",
    "  'The next sections show how to find those projections of x.',\n",
    "  'They lead to what is probably the most important figure in the book (Figure 3.4).',\n",
    "  'Figure 3.4 summarizes the fundamental theorem of linear algebra.',\n",
    "  'It illustrates the true effect of a matrixwhat is happening inside the multiplication Ax. The nullspace\\x0c 166 Chapter 3 Orthogonality Figure 3.4: The true action Ax = A(xrow+xnull) of any m by n matrix. is carried to the zero vector.',\n",
    "  'Every Ax is in the column space.',\n",
    "  'Nothing is carried to the left nullspace.',\n",
    "  'The real action is between the row space and column space, and you see it by looking at a typical vector x.',\n",
    "  'It has a row space component and a nullspace component, with x = xr+xn.',\n",
    "  'When multiplied by A, this is Ax = Axr+Axn: The nullspace component goes to zero: Axn = 0.',\n",
    "  'The row space component goes to the column space: Axr = Ax. Of course everything goes to the column spacethe matrix cannot do anything else.',\n",
    "  'I tried to make the row and column spaces the same size, with equal dimension r. 3F From the row space to the column space, A is actually invertible.',\n",
    "  'Every vector b in the column space comes from exactly one vector xr in the row space.',\n",
    "  'Proof.',\n",
    "  'Every b in the column space is a combination Ax of the columns.',\n",
    "  'In fact, b is Axr, with xr in the row space, since the nullspace component gives Axn = 0, If another vector x0r in the row space gives Ax0r = b, then A(xr .x0r ) = b.b = 0.',\n",
    "  'This puts xr .x0r in the nullspace and the row space, which makes it orthogonal to itself.',\n",
    "  'Therefore it is zero, and xr.x0r .',\n",
    "  'Exactly one vector in the row space is carried to b.',\n",
    "  'Every matrix transforms its row space onto its column space.',\n",
    "  'On those r-dimensional spaces A is invertible.',\n",
    "  'On its nullspace A is zero.',\n",
    "  'When A is diagonal, you see the invertible submatrix holding the r nonzeros.',\n",
    "  'AT goes in the opposite direction, from Rm to Rn and from C(A) back to C(AT). Of course the transpose is not the inverse! AT moves the spaces correctly, but not the\\x0c 3.1 Orthogonal Vectors and Subspaces 167 individual vectors.',\n",
    "  'That honor belongs to A.1 if it existsand it only exists if r =m=n.',\n",
    "  'We cannot ask A.1 to bring back a whole nullspace out of the zero vector.',\n",
    "  'When A.1 fails to exist, the best substitute is the pseudoinverse A+. This inverts A where that is possible: A+Ax = x for x in the row space.',\n",
    "  'On the left nullspace, nothing can be done: A+y = 0.',\n",
    "  'Thus A+ inverts A where it is invertible, and has the same rank r.',\n",
    "  'One formula for A+ depends on the singular value decompositionfor which we first need to know about eigenvalues.']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
