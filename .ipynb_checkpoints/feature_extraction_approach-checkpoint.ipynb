{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.16.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.45.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (7.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.16.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.45.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (7.2.0)\n",
      "symbolic link created for C:\\Users\\asus\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\data\\en <<===>> C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[+] Linking successful\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = read_file('data/Strang-Linear Algebra.txt')\n",
    "chapter = OrderedDict(get_one_chapter_strang(3, book, subsections=True, \n",
    "                                      split=True, sentence_spliter=lambda ss: nlp(ss).sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Orthogonal Vectors and Subspaces', 'Cosines and Projections onto Lines ', 'Projections and Least Squares ', 'Orthogonal Bases and Gram-Schmidt ', 'The Fast Fourier Transform '])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for gap-fill question-generatable and informative sentences selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### FEATURES\n",
    "\n",
    "def get_tags(sentence):\n",
    "    return [token.tag_ for token in nlp(sentence)]\n",
    "\n",
    "def get_noun_adj_tokens(words):\n",
    "    return [token.lemma_ for token in nlp(words) \n",
    "            if token.pos_ == 'ADJ' or token.pos_ == 'NOUN']\n",
    "\n",
    "    \n",
    "# Informative \n",
    "def is_first_sentence(f, c):\n",
    "    return f == c\n",
    "\n",
    "\n",
    "def has_superlatives(curr):\n",
    "    pos_tags = get_tags(curr)\n",
    "    return 'JJR' in pos_tags or 'JJS' in pos_tags\n",
    "\n",
    "\n",
    "def has_abbreviation(curr):\n",
    "    is_abbr = lambda word: word.upper() == word and len(word) > 1\n",
    "    return any(is_abbr(x) for x in curr.split())\n",
    "\n",
    "\n",
    "def has_correct_ending(curr):\n",
    "    return curr[-1] in ['?', '.', '!']\n",
    "\n",
    "# Generative\n",
    "\n",
    "\n",
    "def relative_number_of_words(curr):\n",
    "    abs_n = abs(len(curr.split()) - 10)\n",
    "    return -abs_n if abs_n > 5 else abs_n\n",
    "\n",
    "\n",
    "def relative_index(i, doc_length):\n",
    "    abs_i = abs(i - doc_length/2)\n",
    "    return abs_i if abs_i > doc_length/4 else -abs_i\n",
    "\n",
    "\n",
    "def common_tokens_count(curr, title):\n",
    "    curr_tokens = get_noun_adj_tokens(curr)\n",
    "    title_tokens = get_noun_adj_tokens(title.lower())\n",
    "    \n",
    "    return sum([tok in curr_tokens for tok in title_tokens])\n",
    "\n",
    "\n",
    "def begins_with_discourse_connective(curr):\n",
    "    discource_connective = ['because', 'since', 'when', 'thus', \n",
    "                            'however', 'although', 'for example', \n",
    "                            'and', 'for instance', 'how', 'in other words',\n",
    "                            'therefore', 'up to this point']\n",
    "    curr = curr.lower()\n",
    "    return any(curr.startswith(x) for x in discource_connective)\n",
    "\n",
    "\n",
    "def nouns_number(curr):\n",
    "    return sum(x.pos_ == 'NOUN' for x in nlp(curr))\n",
    "\n",
    "\n",
    "def pronouns_number(curr):\n",
    "    return sum(x.pos_ == 'PRON' for x in nlp(curr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights = {\n",
    "    +4:lambda s, indx, title, first_s, doc_length:  is_first_sentence(s, first_s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_superlatives(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  has_abbreviation(s),\n",
    "    +.5:lambda s, indx, title, first_s, doc_length:  relative_number_of_words(s),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  common_tokens_count(s, title),\n",
    "    -2:lambda s, indx, title, first_s, doc_length:  begins_with_discourse_connective(s),\n",
    "    +1:lambda s, indx, title, first_s, doc_length:  nouns_number(s),\n",
    "    -2.5:lambda s, indx, title, first_s, doc_length:  pronouns_number(s),\n",
    "    +0.01:lambda s, indx, title, first_s, doc_length:  relative_index(indx, doc_length),\n",
    "    +2:lambda s, indx, title, first_s, doc_length:  has_correct_ending(s)\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def get_sentence_score(sentence, index, title, first_sentence, doc_length, weights):\n",
    "    return sum(key * weights[key](sentence, index, title, first_sentence, doc_length) \n",
    "               for key in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(sum(len(chapter[x]) for x in chapter))\n",
    "global_indx = 0\n",
    "for key in chapter:\n",
    "    document = chapter[key]\n",
    "    title = key\n",
    "    doc_length = len(document)\n",
    "    first_sentence = document[0]\n",
    "    \n",
    "    for i, sentence in enumerate(document):\n",
    "        scores[global_indx] = get_sentence_score(\n",
    "                              sentence, i, title, first_sentence, \n",
    "                              doc_length, feature_weights)\n",
    "        global_indx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = OrderedDict((key, len(chapter[key])) for key in chapter)\n",
    "\n",
    "def get_sentence_index_in_document(doc_sent_indx, docs):\n",
    "    indexes =list(docs.values())\n",
    "    i = -1\n",
    "    prev = 0\n",
    "    while doc_sent_indx >= 0:\n",
    "        i += 1\n",
    "        prev = doc_sent_indx\n",
    "        doc_sent_indx -= indexes[i]\n",
    "    doc_name = list(docs.keys())[i]\n",
    "    return doc_name, prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sentences with best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal Bases and Gram-Schmidt \n",
      "In an orthogonal basis, every vector is perpendicular to every other vector.\n",
      "16.225\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "The Gram-Schmidt process and its interpretation as a new factorization A = QR.\n",
      "14.085\n",
      "\n",
      "Orthogonal Vectors and Subspaces\n",
      "A basis is a set of independent vectors that span a space.\n",
      "13.71\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "¤ reflects every point (x;y) into (y;x), its mirror image across the 45° line.\n",
      "13.515\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "improvement is easy: Divide each vector by its length, to make it a unit vector.\n",
      "13.195\n",
      "\n",
      "The Fast Fourier Transform \n",
      "The Fourier series is linear algebra in infinite dimensions.\n",
      "13.015\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "All the terms in the series are projections onto a sine or cosine.\n",
      "12.865\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "Its component p in this direction is exactly b1 sin x.\n",
      "12.825\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "The solution of Qx = b, either n by n or rectangular (least squares).\n",
      "12.605\n",
      "\n",
      "The Fast Fourier Transform \n",
      "At the right is the permutation matrix that separates c into c0 and c00.\n",
      "12.115\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "Geometrically, an orthogonal Q is the product of a rotation and a reflection.\n",
      "12.004999999999999\n",
      "\n",
      "The Fast Fourier Transform \n",
      "Even numbers come before odd (numbers ending in 0 come before numbers ending in 1).\n",
      "12.004999999999999\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "Any permutation matrix P is an orthogonal matrix.\n",
      "11.865\n",
      "\n",
      "Projections and Least Squares \n",
      "In spite of their unsolvability, inconsistent equations arise all the time in practice.\n",
      "11.86\n",
      "\n",
      "Orthogonal Vectors and Subspaces\n",
      "(2) Applying the length formula (1), this test for orthogonality in Rn becomes .\n",
      "11.76\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "These rotation matrices will be examples of Q.\n",
      "11.665\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "Each piece of b has a simple formula, and recombining the pieces gives back b:\n",
      "11.565000000000001\n",
      "\n",
      "Projections and Least Squares \n",
      "The projection of b onto the column space is the nearest point Abx: Projection p\n",
      "11.54\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "All inner products and lengths are preserved, when the space is rotated or reflected.\n",
      "11.325\n",
      "\n",
      "Orthogonal Bases and Gram-Schmidt \n",
      "The component of the vector b along the line spanned by a is bTa=aTa.\n",
      "11.305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordered_scores = np.flip(np.argsort(scores))\n",
    "top_scores = ordered_scores[:5]\n",
    "\n",
    "top_sentences = []\n",
    "for s in top_scores:\n",
    "    doc_name, index = get_sentence_index_in_document(s, docs)\n",
    "    top_sentences.append((doc_name, index))\n",
    "    \n",
    "    print(doc_name)\n",
    "    print(chapter[doc_name][index])\n",
    "    print(scores[s])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: ['an orthogonal basis', 'every vector', 'every other vector'],\n",
       "             1: ['The Gram-Schmidt process',\n",
       "              'its interpretation',\n",
       "              'a new factorization',\n",
       "              'A = QR'],\n",
       "             2: ['A basis', 'a set', 'independent vectors', 'a space'],\n",
       "             3: ['¤',\n",
       "              'every point',\n",
       "              'x;y',\n",
       "              '(y;x',\n",
       "              'its mirror image',\n",
       "              'the 45° line'],\n",
       "             4: ['improvement', 'each vector', 'its length', 'it'],\n",
       "             5: ['The Fourier series',\n",
       "              'linear algebra',\n",
       "              'infinite dimensions'],\n",
       "             6: ['All the terms',\n",
       "              'the series',\n",
       "              'projections',\n",
       "              'a sine',\n",
       "              'cosine'],\n",
       "             7: ['Its component p', 'this direction', 'exactly b1 sin x.'],\n",
       "             8: ['The solution', 'Qx = b', 'n'],\n",
       "             9: ['the right', 'the permutation matrix', 'c', 'c0', 'c00'],\n",
       "             10: ['an orthogonal Q',\n",
       "              'the product',\n",
       "              'a rotation',\n",
       "              'a reflection'],\n",
       "             11: ['Even numbers', 'numbers', 'numbers'],\n",
       "             12: ['Any permutation matrix P', 'an orthogonal matrix'],\n",
       "             13: ['spite',\n",
       "              'their unsolvability',\n",
       "              'inconsistent equations',\n",
       "              'all the time',\n",
       "              'practice'],\n",
       "             14: ['the length formula', 'this test', 'orthogonality', 'Rn'],\n",
       "             15: ['These rotation matrices', 'examples', 'Q.'],\n",
       "             16: ['Each piece', 'b', 'a simple formula', 'the pieces', 'b'],\n",
       "             17: ['The projection',\n",
       "              'b',\n",
       "              'the column space',\n",
       "              'the nearest point',\n",
       "              'Abx',\n",
       "              'Projection p'],\n",
       "             18: ['All inner products', 'lengths', 'the space'],\n",
       "             19: ['The component', 'the vector b', 'the line']})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "key_list = defaultdict(lambda: list())\n",
    "\n",
    "for i, (doc_name, sent_i) in enumerate(top_sentences):\n",
    "    sent = chapter[doc_name][sent_i]\n",
    "    for chunk in nlp(sent).noun_chunks:\n",
    "        key_list[i].append(chunk.text)\n",
    "\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_word(chunk):\n",
    "    importance_order = ['ADJ','NOUN', 'NUM']\n",
    "    for pos in importance_order:\n",
    "        for i in nlp(chunk):\n",
    "            if str(i.pos_) == pos:\n",
    "                return i.text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADV', 'NOUN', 'ADJ', 'SCONJ', 'NUM', 'CCONJ', 'NUM']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.pos_ for x in nlp('Even numbers such as 2 and 4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'such'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_important_word('Even numbers such as 2 and 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for key selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_occurance(key, title):\n",
    "    return common_tokens_count(key, title)\n",
    "\n",
    "\n",
    "def document_occurance(key, doc):\n",
    "    total = 0\n",
    "    for s in doc:\n",
    "        total += common_tokens_count(key, s)\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_depth_in_syntactic_tree(token, depth=0):\n",
    "    d = [get_depth_in_syntactic_tree(child, depth+1) for child in token.children]\n",
    "    d.append(0)\n",
    "    return max(d)\n",
    "    \n",
    "\n",
    "def depth_in_sentence(key, s):\n",
    "    most_imp = get_most_important_word(key)\n",
    "    for tok in nlp(s):\n",
    "        if str(tok.text) == most_imp:\n",
    "            return get_depth_in_syntactic_tree(tok)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_key(key_list, sentence, doc, title):\n",
    "    scores = [title_occurance((key), title)  + \\\n",
    "              document_occurance((key), doc) +\n",
    "              depth_in_sentence((key), sentence)\n",
    "                  for key in key_list]\n",
    "    return key_list[scores.index(max(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an orthogonal basis', 'every vector', 'every other vector']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'r.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-209414f0712b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(key_list[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistractors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Proposed distractor -\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-9432a5ca9cab>\u001b[0m in \u001b[0;36mdistractors\u001b[1;34m(chapter_sentences, key, sentence)\u001b[0m\n\u001b[0;32m     62\u001b[0m            \u001b[0mcand_tags\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mkey_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                                                     \u001b[1;31m#context_similarity()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m      \u001b[0mdiff_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mkey_score\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m   \u001b[1;31m#importance_difference()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m      \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent_simil\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcand_tags\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_tags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdiff_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m      \u001b[0mmax_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-9432a5ca9cab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m            \u001b[0mcand_tags\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mkey_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                                                     \u001b[1;31m#context_similarity()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m      \u001b[0mdiff_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mkey_score\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m   \u001b[1;31m#importance_difference()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m      \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent_simil\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcand_tags\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_tags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdiff_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m      \u001b[0mmax_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'r.'"
     ]
    }
   ],
   "source": [
    "for i in key_list:\n",
    "    doc = chapter[top_sentences[i][0]]\n",
    "    doc = [i.lower() for i in doc]\n",
    "    s = doc[top_sentences[i][1]]\n",
    "    print(key_list[i])\n",
    "#     key_list[i] = get_best_key(key_list[i], s, doc, top_sentences[i][0])\n",
    "#     print(key_list[i])\n",
    "    for j in key_list[i]:\n",
    "        dist = distractors(doc, j.lower(), s)\n",
    "    print(\"Proposed distractor -\", dist)\n",
    "\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (doc_name, sentence_index) in enumerate(top_sentences):\n",
    "    s = chapter[doc_name][sentence_index]\n",
    "    create_gap_filled_question(s, key_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## HELOOOOOOOOOOO\n",
    "##   top_sentences = [ (name of document, index in document),...]  see above s = chapter[top_sentences[i][0]][top_sentences[i][1]]\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "##get sentence = chapter[name of document][index]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def distractors(chapter_sentences, key, sentence):\n",
    "  candidates = []\n",
    "  cand_sents = []\n",
    "  nlp2 = spacy.load('.')\n",
    "  key_nlp = nlp2(key)\n",
    "\n",
    "  label = key_nlp.ents\n",
    "\n",
    "  tfidf_vectorizer=TfidfVectorizer()\n",
    "  tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(chapter_sentences)\n",
    "  tag = tfidf_vectorizer.get_feature_names()\n",
    "  n = tfidf_vectorizer_vectors.sum(axis=0).A1\n",
    "  candidates_scores = []\n",
    "  result = dict(zip(tag,n))\n",
    "\n",
    "  key_score = sum([result[key.split()[i]] for i in range(len(key.split()))])/len(key.split())\n",
    "#   print(key_score)\n",
    "  max_score = [- float(\"inf\"),\"\"]\n",
    "  key_tags = []\n",
    "\n",
    "  sent = sentence\n",
    "  context = sent[:sent.index(key)] + \".\" +sent[sent.index(key)+len(key):]\n",
    "#   print(context)\n",
    "  context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
    "#   print(context)\n",
    "  for word in context:\n",
    "      tag = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "      key_tags.append(tag)\n",
    "\n",
    "  if label:\n",
    "    key_label = label[0].label_\n",
    "  else:\n",
    "    key_label = 'obj'\n",
    "  for sentence in chapter_sentences:\n",
    "    doc = nlp2(sentence)\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_ == key_label:\n",
    "          candidates.append(ent.text)\n",
    "          cand_sents.append(sentence)\n",
    "#           print(ent.label_, ent.text)\n",
    "\n",
    "  for cand, sent in zip(candidates, cand_sents):\n",
    "    if cand not in key and key not in cand:\n",
    "      sent_simil = sum([2 for i in sent if i in sentence])/(len(sentence.split())+len(sent.split())) #sentence_similarity()\n",
    "      cand_tags = 0\n",
    "      context = sent[:sent.index(cand)] + \".\" +sent[sent.index(cand)+len(cand):]\n",
    "#       print(context)\n",
    "      context = context.split(\".\")[0].split()[-2:] + context.split(\".\")[1].split()[:2]\n",
    "#       print(context)\n",
    "      for word in [0,1,2,3]:\n",
    "        if word < len(key_tags) and word < len(context):\n",
    "            tag = nltk.pos_tag(nltk.word_tokenize(context[word]))\n",
    "#             print(tag)\n",
    "            tag = tag[0][1]\n",
    "            cand_tags -= int(tag != key_tags[word])                                                     #context_similarity()\n",
    "\n",
    "      diff_score = (sum([result[cand.split()[i]] for i in range(len(cand.split()))])/len(cand.split()) - key_score + 1)/2   #importance_difference()\n",
    "      score = [sent_simil + cand_tags/len(key_tags) - diff_score, cand]\n",
    "      max_score = max([max_score, score],key=lambda x: x[0])\n",
    "  return max_score[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plane'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors(['this is a symmetric matrix and also a plane is here',\n",
    "             'this can be an invertible matrix',\n",
    "             'use matrix multiplication for this problem'], \n",
    "            'matrix', 'this is a symmetric matrix and also a plane is here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
